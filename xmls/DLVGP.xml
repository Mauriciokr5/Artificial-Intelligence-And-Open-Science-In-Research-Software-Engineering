<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Video Game Playing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
				<availability status="unknown"><p>Copyright Institute of Electrical and Electronics Engineers (IEEE)</p>
				</availability>
				<date type="published" when="2020-03">2020-03</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,123.80,94.10,62.42,9.86"><forename type="first">Niels</forename><forename type="middle">Justesen</forename><surname>Justesen</surname></persName>
							<email>njustesen@gmail.com</email>
							<idno type="ORCID">0000-0001-5381-5498</idno>
						</author>
						<author>
							<persName coords="1,202.51,94.10,72.76,9.86"><forename type="first">Philip</forename><surname>Bontrager</surname></persName>
							<email>philipjb@nyu.edu</email>
							<idno type="ORCID">0000-0002-1011-2976</idno>
						</author>
						<author>
							<persName coords="1,291.57,94.10,66.40,9.86"><forename type="first">Julian</forename><surname>Togelius</surname></persName>
							<email>julian@togelius.com</email>
							<idno type="ORCID">0000-0003-3128-4598</idno>
						</author>
						<author>
							<persName coords="1,392.84,94.10,62.42,9.86"><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
							<email>sebastian.risi@gmail.com</email>
							<idno type="ORCID">0000-0003-3607-8400</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">IT University of Copenhagen, 2300 Copenhagen, Denmark</note>
								<orgName type="institution">IT University of Copenhagen</orgName>
								<address>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">New York University, New York, NY 11201 USA</note>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>11201</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">Univ Politecnica de Madrid.</note>
								<orgName type="institution">Univ Politecnica de Madrid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Video Game Playing</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE Transactions on Games</title>
						<title level="j" type="abbrev">IEEE Trans. Games</title>
						<idno type="ISSN">2475-1502</idno>
						<idno type="eISSN">2475-1510</idno>
						<imprint>
							<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
							<biblScope unit="volume">12</biblScope>
							<biblScope unit="issue">1</biblScope>
							<biblScope unit="page" from="1" to="20"/>
							<date type="published" when="2020-03" />
						</imprint>
					</monogr>
					<idno type="MD5">7B198803CEB1A016D1897723D0DA6777</idno>
					<idno type="DOI">10.1109/tg.2019.2896986</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3-SNAPSHOT" ident="GROBID" when="2023-02-13T23:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Algorithms</term>
					<term>learning</term>
					<term>machine learning algorithms</term>
					<term>multilayer neural network</term>
					<term>artificial intelligence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,87.42,137.40,201.51,8.07;1,37.91,147.37,251.08,8.07;1,37.91,157.33,251.06,8.07;1,37.91,167.30,95.04,8.07">In this paper, we review recent deep learning advances in the context of how they have been applied to play different types of video games such as first-person shooters, arcade games, and real-time strategy games.</s><s coords="1,134.61,167.30,154.36,8.07;1,37.91,177.26,251.07,8.07;1,37.91,187.22,251.07,8.07;1,37.91,197.19,251.08,8.07;1,37.91,207.15,247.40,8.07">We analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games, such as general game playing, dealing with extremely large decision spaces and sparse rewards.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,300.92,137.22,251.04,8.97;1,300.92,149.18,130.80,8.97">plenty of research on playing games in a believable, entertaining, or human-like manner <ref type="bibr" coords="1,412.64,149.18,15.26,8.97" target="#b58">[59]</ref>.</s><s coords="1,434.98,149.18,116.98,8.97;1,300.92,161.13,251.05,8.97;1,300.92,173.09,225.68,8.97">AI is also used for modeling players' behavior, experience or preferences <ref type="bibr" coords="1,481.26,161.13,20.05,8.97" target="#b168">[169]</ref>, or generating game content such as levels, textures, or rules <ref type="bibr" coords="1,502.54,173.09,20.06,8.97" target="#b129">[130]</ref>.</s><s coords="1,529.34,173.09,22.66,8.97;1,300.92,185.05,180.81,8.97">DL is far from the only AI method used in games.</s><s coords="1,485.04,185.05,66.92,8.97;1,300.92,197.00,251.01,8.97;1,300.92,208.96,99.02,8.97">Other prominent methods include Monte Carlo tree search <ref type="bibr" coords="1,466.44,197.00,16.59,8.97" target="#b17">[18]</ref> and evolutionary computation <ref type="bibr" coords="1,353.76,208.96,15.27,8.97" target="#b89">[90]</ref>, <ref type="bibr" coords="1,375.88,208.96,20.06,8.97" target="#b114">[115]</ref>.</s><s coords="1,402.98,208.96,149.02,8.97;1,300.92,220.91,198.95,8.97">In what follows, it is important to be aware of the limitations of the scope of this paper.</s></p><p><s coords="1,310.89,232.86,195.98,8.97">The rest of this paper is structured as follows.</s><s coords="1,511.17,232.86,40.83,8.97;1,300.92,244.82,251.04,8.97;1,300.92,256.78,251.05,8.97;1,300.92,268.73,26.61,8.97">Section II gives an overview of different DL methods applied to games, followed by the different research platforms that are currently in use.</s><s coords="1,330.63,268.73,221.36,8.97;1,300.92,280.69,72.71,8.97">Section IV reviews the use of DL methods in different video game types.</s><s coords="1,376.41,280.69,175.55,8.97;1,300.92,292.64,20.19,8.97">Section V gives a historical overview of the field.</s><s coords="1,324.60,292.64,227.35,8.97;1,300.92,304.59,228.55,8.97">We conclude this paper by pointing out important open challenges in Section VI and a conclusion in Section VII.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,368.45,333.31,116.00,8.97">II. DL IN GAMES OVERVIEW</head><p><s coords="1,310.89,350.25,241.10,8.97;1,300.92,362.20,174.22,8.97">This section gives a brief overview of neural networks and machine learning in the context of games.</s><s coords="1,478.92,362.20,73.06,8.97;1,300.92,374.16,251.05,8.97;1,300.92,386.11,251.04,8.97;1,300.92,398.08,189.37,8.97">First, we describe common neural network architectures followed by an overview of the three main categories of machine learning tasks: supervised learning, unsupervised learning, and RL.</s><s coords="1,493.47,398.08,58.52,8.97;1,300.92,410.03,251.08,8.97;1,300.92,421.98,37.34,8.97">Approaches in these categories are typically based on gradient-descent optimization.</s><s coords="1,341.99,421.98,209.98,8.97;1,300.92,433.94,251.05,8.97;1,300.92,445.89,97.93,8.97">We also highlight evolutionary approaches, as well as a few examples of hybrid approaches that combine several optimization techniques.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,300.92,474.61,109.64,8.97">A. Neural Network Models</head><p><s coords="1,310.89,491.55,241.10,8.97;1,300.92,503.50,251.03,8.97;1,300.92,515.46,76.16,8.97">Artificial neural networks (ANNs) are general-purpose functions that are defined by their network structure and the weight of each graph edge.</s><s coords="1,378.97,515.46,173.02,8.97;1,300.92,527.42,251.04,8.97;1,300.92,539.37,251.06,8.97;1,300.92,551.32,115.11,8.97">Because of their generality and ability to approximate any continuous real-valued function (given enough parameters), they have been applied to a variety of tasks, including video game playing.</s><s coords="1,419.12,551.32,132.85,8.97;1,300.92,563.28,251.06,8.97;1,300.92,575.23,197.25,8.97">The architectures of these ANNs can roughly be divided into two major categories: feedforward networks and recurrent neural networks (RNNs).</s><s coords="1,501.20,575.23,50.79,8.97;1,300.92,587.20,251.05,8.97;1,300.92,599.15,251.04,8.97;1,300.92,611.10,48.51,8.97">Feedforward networks take a single input, for example, a representation of the game state, and outputs probabilities or values for each possible action.</s><s coords="1,352.32,611.10,199.63,8.97;1,300.92,623.06,251.06,8.97;1,300.92,635.01,142.27,8.97">Convolutional neural networks (CNNs) consist of trainable filters and are suitable for processing image data such as pixels from a video game screen.</s></p><p><s coords="1,310.89,646.96,241.09,8.97;1,300.92,658.93,251.05,8.97;1,300.92,670.88,166.36,8.97">RNNs are typically applied to time-series data, in which the output of the network can depend on the network's activation from previous time steps <ref type="bibr" coords="1,421.36,670.88,15.27,8.97" target="#b81">[82]</ref>, <ref type="bibr" coords="1,443.22,670.88,20.06,8.97" target="#b164">[165]</ref>.</s><s coords="1,470.07,670.88,81.90,8.97;1,300.92,682.83,251.04,8.97;1,300.92,694.79,251.08,8.97;1,300.92,706.74,55.70,8.97">The training process is similar to feedforward networks, except that the network's previous hidden state is fed back into the network together with the next input.</s><s coords="1,358.40,706.74,193.57,8.97;2,42.12,66.09,251.04,8.97;2,42.12,78.04,251.03,8.97;2,42.12,90.01,44.44,8.97">This allows the network to become context-aware by memorizing the previous activations, which is useful when a single observation from a game does not represent the complete game state.</s><s coords="2,88.74,90.01,204.43,8.97;2,42.12,101.96,251.05,8.97;2,42.12,113.91,120.24,8.97">For video game playing, it is common to use a stack of convolutional layers followed by recurrent layers and fully connected feedforward layers.</s></p><p><s coords="2,52.08,125.87,241.09,8.97;2,42.12,137.82,251.04,8.97;2,42.12,149.78,224.26,8.97">The following sections will give a brief overview of different optimization methods, which are commonly used for learning game-playing behaviors with deep neural networks.</s><s coords="2,269.38,149.78,23.78,8.97;2,42.12,161.74,251.05,8.97;2,42.12,173.69,39.56,8.97">These methods search for the optimal set of parameters to solve some problems.</s><s coords="2,84.55,173.69,208.62,8.97;2,42.12,185.65,251.07,8.97;2,42.12,197.60,144.44,8.97">Optimization can also be used to find hyperparameters, such as network architecture and learning parameters, and is well studied within DL <ref type="bibr" coords="2,145.89,197.60,15.27,8.97" target="#b11">[12]</ref>, <ref type="bibr" coords="2,167.47,197.60,15.27,8.97" target="#b12">[13]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,42.12,228.53,128.46,8.97">B. Optimizing Neural Networks</head><p><s coords="2,52.08,245.47,241.11,8.97;2,42.12,257.42,93.26,8.97">1) Supervised Learning: In supervised learning, a model is trained from examples.</s><s coords="2,138.55,257.42,154.63,8.97;2,42.12,269.38,220.49,8.97">During training, the model is asked to make a decision, for which the correct answer is known.</s><s coords="2,264.73,269.38,28.46,8.97;2,42.12,281.34,251.04,8.97;2,42.12,293.29,186.86,8.97">The error, i.e., difference between the provided answer and the ground truth, is used as a loss to update the model.</s><s coords="2,233.27,293.29,59.90,8.97;2,42.12,305.24,251.03,8.97;2,42.12,317.20,225.14,8.97">The goal is to achieve a model that can generalize beyond the training data and thus perform well on examples it has never seen before.</s><s coords="2,270.13,317.20,23.05,8.97;2,42.12,329.16,231.01,8.97">Large datasets usually improve the model's ability to generalize.</s></p><p><s coords="2,52.08,341.12,241.10,8.97;2,42.12,353.07,251.05,8.97;2,42.12,365.02,251.04,8.97;2,42.12,376.98,251.07,8.97;2,42.12,388.93,44.29,8.97">In games, these data can come from play traces <ref type="bibr" coords="2,254.80,341.12,16.59,8.97" target="#b15">[16]</ref> (i.e., humans playing through the game while being recorded), allowing the agent to learn the mapping from the input state to output actions based on what actions the human performed in a given state.</s><s coords="2,88.21,388.93,204.96,8.97;2,42.12,400.89,251.05,8.97;2,42.12,412.85,159.63,8.97">If the game is already solved by another algorithm, it can be used to generate training data, which is useful if the first algorithm is too slow to run in real time.</s><s coords="2,204.13,412.85,89.03,8.97;2,42.12,424.80,251.04,8.97;2,42.12,436.75,251.05,8.97;2,42.12,448.71,251.06,8.97;2,42.12,460.66,39.75,8.97">While learning to play from existing data allows agents to quickly learn best practices, it is often brittle; the data available can be expensive to produce and may be missing key scenarios the agent should be able to deal with.</s><s coords="2,84.81,460.66,208.37,8.97;2,42.12,472.62,251.07,8.97">For gameplay, the algorithm is limited to the strategies available in the data and cannot explore new ones itself.</s><s coords="2,42.12,484.58,251.04,8.97;2,42.12,496.53,213.92,8.97">Therefore, in games, supervised algorithms are often combined with additional training through RL algorithms <ref type="bibr" coords="2,231.97,496.53,20.05,8.97" target="#b132">[133]</ref>.</s></p><p><s coords="2,52.08,508.48,241.09,8.97;2,42.12,520.44,149.35,8.97">Another application of supervised learning in games is to learn the state transitions of a game.</s><s coords="2,194.96,520.44,98.23,8.97;2,42.12,532.40,251.03,8.97;2,42.12,544.35,159.76,8.97">Instead of providing the action for a given state, the neural network can learn to predict the next state for an action-state pair.</s><s coords="2,206.26,544.35,86.92,8.97;2,42.12,556.31,251.05,8.97;2,42.12,568.26,226.57,8.97">Thus, the network is essentially learning a model of the game, which can then be used to play the game better or to perform planning <ref type="bibr" coords="2,249.60,568.26,15.27,8.97" target="#b44">[45]</ref>.</s></p><p><s coords="2,52.08,580.22,241.07,8.97;2,42.12,592.17,251.04,8.97;2,42.12,604.13,143.69,8.97">2) Unsupervised Learning: Instead of learning a mapping between data and its labels, the objective in unsupervised learning is to discover patterns in the data.</s><s coords="2,187.62,604.13,105.54,8.97;2,42.12,616.09,251.06,8.97;2,42.12,628.04,251.05,8.97;2,42.12,639.99,251.06,8.97;2,42.12,651.95,33.90,8.97">These algorithms can learn the distribution of features for a dataset, which can be used to cluster similar data, compress data into its essential features, or create new synthetic data that are characteristic of the original data.</s><s coords="2,78.67,651.95,214.49,8.97;2,42.12,663.90,251.06,8.97;2,42.12,675.86,220.25,8.97">For games with sparse rewards (such as Montezuma's revenge), learning from data in an unsupervised fashion is a potential solution and an important open DL challenge.</s></p><p><s coords="2,52.08,687.82,241.08,8.97;2,42.12,699.77,251.07,8.97;2,42.12,711.73,251.03,8.97;2,42.12,723.68,45.87,8.97">A prominent unsupervised learning technique in DL is the autoencoder, which is a neural network that attempts to learn the identity function such that the output is identical to the input <ref type="bibr" coords="2,42.12,723.68,15.27,8.97" target="#b79">[80]</ref>, <ref type="bibr" coords="2,63.92,723.68,20.06,8.97" target="#b116">[117]</ref>.</s><s coords="2,90.71,723.68,202.48,8.97;2,42.12,734.97,251.06,9.63;2,42.12,746.93,182.64,9.63">The network consists of two parts: an encoder that maps the input x to a low-dimensional hidden vector h, and a decoder that attempts to reconstruct x from h.</s><s coords="2,227.20,747.60,65.99,8.97;2,305.14,151.09,21.59,7.17">The main idea is Fig. <ref type="figure" coords="2,320.75,151.09,2.99,7.17">1</ref>.</s><s coords="2,336.19,150.56,220.00,7.71;2,305.14,160.06,50.02,7.17">RL framework where an agent's policy π is determined by a deep neural network.</s><s coords="2,357.24,160.06,198.95,7.17;2,305.14,169.03,164.22,7.17">The state of the environment, or an observation such as screen pixels, is fed as input to the agent's policy network.</s><s coords="2,471.41,169.03,84.81,7.17;2,305.14,177.46,251.07,7.71;2,305.14,186.96,35.91,7.17">An action is sampled from the policy network's output π, whereafter it receives a reward and the subsequent game state.</s><s coords="2,343.12,186.96,153.34,7.17">The goal is to maximize the cumulated rewards.</s><s coords="2,498.53,186.96,57.68,7.17;2,305.14,195.93,195.85,7.17">The RL algorithm updates the policy (network parameters) based on the reward.</s></p><p><s coords="2,305.14,222.16,251.04,9.63;2,305.14,234.79,200.94,8.97">that by keeping h small, the network has to learn to compress the data and, therefore, learn a good representation.</s><s coords="2,508.05,234.79,48.11,8.97;2,305.14,246.74,251.05,8.97;2,305.14,258.69,251.05,8.97;2,305.14,270.65,251.05,8.97;2,305.14,282.61,47.40,8.97">Researchers are beginning to apply such unsupervised algorithms to games to help distill high-dimensional data to more meaningful lower dimensional data, but this research direction is still in its early stages <ref type="bibr" coords="2,333.45,282.61,15.27,8.97" target="#b44">[45]</ref>.</s><s coords="2,356.50,282.61,199.70,8.97;2,305.14,294.56,167.93,8.97">For a more detailed overview of supervised and unsupervised learning, see <ref type="bibr" coords="2,413.03,294.56,16.60,8.97" target="#b38">[39]</ref> and <ref type="bibr" coords="2,449.00,294.56,20.05,8.97" target="#b125">[126]</ref>.</s></p><p><s coords="2,315.10,306.52,241.08,8.97;2,305.14,318.47,251.04,8.97;2,305.14,330.43,72.52,8.97">3) RL Approaches: In RL, an agent learns a behavior by interacting with an environment that provides a reward signal back to the agent.</s><s coords="2,380.91,330.43,175.29,8.97;2,305.14,342.38,251.04,8.97;2,305.14,354.34,251.05,8.97;2,305.14,366.30,251.03,8.97">A video game can easily be modeled as an environment in an RL setting, wherein players are modeled as agents with a finite set of actions that can be taken at each step, and the reward signal can be determined by the game score.</s><s coords="2,305.14,378.25,132.18,8.97">Fig. <ref type="figure" coords="2,323.41,378.25,4.98,8.97">1</ref> depicts the RL framework.</s></p><p><s coords="2,315.10,390.20,182.62,8.97">In RL, the agent relies on the reward signal.</s><s coords="2,501.22,390.20,54.98,8.97;2,305.14,402.16,251.04,8.97;2,305.14,414.12,251.05,8.97;2,305.14,426.07,62.95,8.97">These signals can occur frequently, such as the change in score within a game, or it can occur infrequently, such as whether an agent has won or lost a game.</s><s coords="2,372.06,426.07,184.14,8.97;2,305.14,438.03,200.47,8.97">Video games and RL go well together since most games give rewards for successful strategies.</s><s coords="2,508.15,438.03,48.03,8.97;2,305.14,449.98,251.05,8.97;2,305.14,461.94,123.41,8.97">Open-world games do not always have a clear reward model and are thus challenging for RL algorithms.</s></p><p><s coords="2,315.10,473.89,241.08,8.97;2,305.14,485.85,251.05,8.97;2,305.14,497.81,135.58,8.97">A key challenge in applying RL to games with sparse rewards is to determine how to assign credit to the many previous actions when a reward signal is obtained.</s><s coords="2,443.75,497.14,112.44,9.63;2,305.14,509.76,251.06,8.97;2,305.14,521.71,22.30,8.97">The reward R(s) for state s needs to be propagated back to the actions that lead to the reward.</s><s coords="2,329.68,521.71,226.52,8.97;2,305.14,533.67,212.82,8.97">Historically, there are several different ways this problem is approached, which are described in the following.</s><s coords="2,521.13,533.67,35.08,8.97;2,305.14,545.62,251.04,8.97;2,305.14,557.58,251.06,8.97;2,305.14,569.54,33.66,8.97">If an environment can be described as a Markov decision process, then the agent can build a probability tree of future states and their rewards.</s><s coords="2,341.72,569.54,214.48,8.97;2,305.14,581.49,105.90,8.97">The probability tree can then be used to calculate the utility of the current state.</s><s coords="2,414.21,581.49,141.99,8.97;2,305.14,592.77,251.06,9.97;2,305.14,604.73,116.96,9.63">For an RL agent, this means learning the model P (s |s, a), where P is the probability of state s given state s and action a.</s><s coords="2,425.21,604.73,130.97,9.63;2,305.14,617.35,52.83,8.97">With a model P , utilities can be calculated by</s></p><formula xml:id="formula_0" coords="2,341.71,636.47,171.92,20.84">U (s) = R(s) + γ max a s P (s |s, a)U (s )</formula><p><s coords="2,305.14,663.23,231.39,9.63">where γ is the discount factor for the utility of future states.</s><s coords="2,538.47,663.90,17.71,8.97;2,305.14,675.85,251.06,8.97;2,305.14,687.81,251.03,8.97;2,305.14,699.76,59.51,8.97">This algorithm, known as adaptive dynamic programming, can converge rather quickly as it directly handles the credit assignment problem <ref type="bibr" coords="2,340.59,699.76,20.05,8.97" target="#b141">[142]</ref>.</s><s coords="2,366.89,699.76,189.29,8.97;2,305.14,711.73,251.03,8.97;2,305.14,723.68,62.08,8.97">The issue is that it has to build a probability tree over the whole problem space and is, therefore, intractable for large problems.</s><s coords="2,370.01,723.68,186.18,8.97;2,305.14,735.63,251.07,8.97;2,305.14,747.59,41.23,8.97">As the games covered in this work are considered "large problems," we will not go into further detail on this algorithm.</s></p><p><s coords="3,47.87,66.09,241.09,8.97;3,37.91,78.04,35.13,8.97">Another approach to this problem is temporal difference (TD) learning.</s><s coords="3,75.25,78.04,213.73,8.97;3,37.91,90.01,251.07,8.97;3,37.91,101.96,251.05,8.97">In TD learning, the agent learns the utilities U directly based off of the observation that the current utility is equal to the current reward plus the utility value of the next state <ref type="bibr" coords="3,264.89,101.96,20.06,8.97" target="#b141">[142]</ref>.</s><s coords="3,37.91,113.24,251.06,9.63;3,37.91,125.20,135.69,9.63">Instead of learning the state transition model P , it learns to model the utility U for every state.</s><s coords="3,175.74,125.20,113.23,9.63">The update equation for U is</s></p><formula xml:id="formula_1" coords="3,73.76,142.85,173.38,9.96">U (s) = U (s) + α(R(s) + γU (s ) − U (s))</formula><p><s coords="3,37.91,160.49,181.43,9.63">where α is the learning rate of the algorithm.</s><s coords="3,222.18,161.16,66.77,8.97;3,37.91,172.45,197.72,9.63">The above equation does not take into account how s was chosen.</s><s coords="3,237.73,173.12,51.24,8.97;3,37.91,184.41,140.92,10.42">If a reward is found at s t , it will only affect U (s t ).</s><s coords="3,180.59,185.07,108.36,8.97;3,37.91,196.36,213.31,10.42">The next time the agent is at s t−1 , then U (s t−1 ) will be aware of the future reward.</s><s coords="3,253.49,197.02,35.48,8.97;3,37.91,208.99,124.75,8.97">This will propagate backward over time.</s><s coords="3,165.75,208.99,123.20,8.97;3,37.91,220.94,205.81,8.97">Likewise, less common transitions will have less of an impact on utility values.</s><s coords="3,247.20,220.94,41.75,8.97;3,37.91,232.22,251.04,9.63;3,37.91,244.85,52.88,8.97">Therefore, U will converge to the same values as are obtained from ADP, albeit slower.</s></p><p><s coords="3,47.87,256.80,241.08,8.97;3,37.91,268.76,116.90,8.97">There are alternative implementations of TD that learn rewards for state-action pairs.</s><s coords="3,158.85,268.76,130.12,8.97;3,37.91,280.72,251.04,8.97;3,37.91,292.67,62.67,8.97">This allows an agent to choose an action, given the state, with no model of how to transition to future states.</s><s coords="3,103.84,292.67,185.13,8.97;3,37.91,304.63,107.64,8.97">For this reason, these approaches are referred to as model-free methods.</s><s coords="3,149.45,304.63,139.48,8.97;3,37.91,316.58,251.06,8.97;3,37.91,328.53,144.46,8.97">A popular model-free RL method is Q-learning <ref type="bibr" coords="3,96.78,316.58,20.05,8.97" target="#b161">[162]</ref>, where the utility of a state is equal to the maximum Q-value for a state.</s><s coords="3,187.01,328.53,101.95,8.97;3,37.91,340.50,52.29,8.97">The update equation for Q-learning is</s></p><formula xml:id="formula_2" coords="3,40.87,357.47,239.14,15.37">Q(s, a) = Q(s, a) + α(R(s) + γ max a Q(s , a ) − Q(s, a)).</formula><p><s coords="3,37.91,378.97,251.07,8.97;3,37.91,390.92,159.57,8.97">In Q-learning, the future reward is accounted for by selecting the best-known future state-action pair.</s><s coords="3,200.57,390.92,88.38,8.97;3,37.91,402.21,251.05,9.63;3,37.91,414.17,251.06,9.63;3,37.91,426.79,61.55,8.97">In a similar algorithm called State-Action-Reward-State-Action (SARSA), Q(s, a) is updated only when the next a has been selected and the next s is known <ref type="bibr" coords="3,75.41,426.79,20.05,8.97" target="#b117">[118]</ref>.</s><s coords="3,101.47,426.79,187.48,8.97;3,37.91,438.75,34.31,8.97">This action pair is used instead of the maximum Q-value.</s><s coords="3,75.41,438.75,213.56,8.97;3,37.91,450.70,251.05,8.97;3,37.91,462.66,142.83,8.97">This makes SARSA an on-policy method in contrast to Q-learning, which is off-policy, because SARSA's Q-value accounts for the agent's own policy.</s></p><p><s coords="3,47.87,474.61,241.02,8.97;3,37.91,486.56,131.70,8.97">Q-learning and SARSA can use a neural network as a function approximator for the Q-function.</s><s coords="3,172.31,486.56,116.65,8.97;3,37.91,498.53,251.06,8.97;3,37.91,510.48,43.30,8.97">The given Q-update equation can be used to provide the new "expected" Q-value for a stateaction pair.</s><s coords="3,82.76,510.48,206.11,8.97;3,37.91,522.43,35.12,8.97">The network can then be updated as it is in supervised learning.</s></p><p><s coords="3,47.87,533.72,241.08,9.63;3,37.91,545.68,33.22,9.63">An agent's policy π(s) determines which action to take given a state s.</s><s coords="3,72.81,546.34,216.14,8.97;3,37.91,558.30,140.44,8.97">For Q-learning, a simple policy would be to always take the action with the highest Q-value.</s><s coords="3,180.55,558.30,108.43,8.97;3,37.91,570.26,251.03,8.97;3,37.91,582.21,107.48,8.97">Yet, early on in training, Qvalues are not very accurate, and an agent could get stuck always exploiting a small reward.</s><s coords="3,149.22,582.21,139.75,8.97;3,37.91,594.16,251.03,8.97;3,37.91,606.12,57.11,8.97">A learning agent should prioritize exploration of new actions, as well as the exploitation of what it has learned.</s><s coords="3,98.28,606.12,190.68,8.97;3,37.91,618.07,146.30,8.97">This problem is known as a multiarmed bandit problem and has been well explored.</s><s coords="3,186.70,618.07,102.26,8.97;3,37.91,630.03,251.07,8.97;3,44.45,641.99,200.06,8.97">The -greedy strategy is a simple approach that selects the (estimated) optimal action with probability and otherwise selects a random action.</s></p><p><s coords="3,47.88,653.94,241.12,8.97;3,37.91,665.90,103.19,8.97">One approach to RL is to perform gradient descent in the policy's parameter space.</s><s coords="3,144.78,665.23,144.18,10.42;3,37.91,677.18,182.95,9.63">Let π θ (s, a) be the probability that action a is taken at state s given parameters θ.</s><s coords="3,223.32,677.85,65.64,8.97;3,37.91,689.80,251.06,8.97;3,37.91,701.10,166.64,11.76;3,207.34,701.10,81.62,10.42;3,37.91,713.05,251.05,9.63;3,37.91,725.67,33.92,8.97">The basic policy gradient algorithm from the REINFORCE family of algorithms <ref type="bibr" coords="3,37.91,701.76,21.57,8.97" target="#b163">[164]</ref> updates θ using the gradient ∇ θ a π θ (s, a)R(s), where R(s) is the discounted cumulative reward obtained from s and forward.</s><s coords="3,73.50,725.67,215.46,8.97;3,300.93,66.09,251.07,8.97;3,300.93,78.05,205.41,8.97">In practice, a sample of possible actions from the policy is taken, and it is updated to increase the likelihood that the more successful actions are returned in the future.</s><s coords="3,509.79,78.05,42.20,8.97;3,300.93,89.34,251.07,9.63;3,300.93,101.29,90.63,9.63">This lends itself well to neural networks, as π can be a neural network and θ the network weights.</s></p><p><s coords="3,310.89,113.91,241.07,8.97;3,300.93,125.20,251.05,10.42;3,300.93,137.82,251.05,8.97;3,300.93,149.11,129.79,9.63">Actor-critic methods combine the policy gradient approach with TD learning, where an actor learns a policy π θ (s, a) using the policy gradient algorithm, and the critic learns to approximate R using TD learning <ref type="bibr" coords="3,406.65,149.77,20.05,8.97" target="#b142">[143]</ref>.</s><s coords="3,432.96,149.77,119.00,8.97;3,300.93,161.74,165.47,8.97">Together, they are an effective approach to iteratively learning a policy.</s><s coords="3,469.92,161.74,82.05,8.97;3,300.93,173.02,251.05,9.63;3,300.93,184.98,113.60,9.63">In actor-critic methods, there can either be a single network to predict both π and R or two separate networks.</s><s coords="3,417.35,185.65,134.64,8.97;3,300.93,197.60,251.03,8.97;3,300.93,209.55,36.52,8.97">For an overview of RL applied to deep neural networks, we suggest the article by Arulkumaran et al. <ref type="bibr" coords="3,323.34,209.55,10.58,8.97" target="#b1">[2]</ref>.</s></p><p><s coords="3,310.89,221.51,241.08,8.97;3,300.93,233.47,251.04,8.97;3,300.93,245.42,95.69,8.97">4) Evolutionary Approaches: The optimization techniques discussed so far rely on gradient descent, based on differentiation of a defined error.</s><s coords="3,399.72,245.42,152.25,8.97;3,300.93,257.38,251.04,8.97;3,300.93,269.33,251.06,8.97;3,300.93,281.28,39.05,8.97">However, derivative-free optimization methods such as evolutionary algorithms have also been widely used to train neural networks, including, but not limited to, RL tasks.</s><s coords="3,343.89,281.28,208.08,8.97;3,300.93,293.25,251.07,8.97;3,300.93,305.20,67.20,8.97">This approach, often referred to as neuroevolution (NE), can optimize a network's weights, as well as their topology/architecture.</s><s coords="3,372.20,305.20,179.76,8.97;3,300.93,317.15,251.04,8.97">Because of their generality, NE approaches have been applied extensively to different types of video games.</s><s coords="3,300.93,329.11,251.06,8.97;3,300.93,341.06,146.59,8.97">For a complete overview of this field, we refer the interested reader to our NE survey paper <ref type="bibr" coords="3,423.46,341.06,20.05,8.97" target="#b114">[115]</ref>.</s></p><p><s coords="3,310.89,353.02,241.12,8.97;3,300.93,364.98,251.07,8.97;3,300.93,376.93,251.05,8.97;3,300.93,388.89,148.31,8.97">Compared to gradient-descent-based training methods, NE approaches have the benefit of not requiring the network to be differentiable and can be applied to supervised learning, unsupervised learning, and RL problems.</s><s coords="3,452.43,388.89,99.56,8.97;3,300.93,400.84,251.04,8.97;3,300.93,412.79,251.05,8.97;3,300.93,424.75,200.35,8.97">The ability to evolve the topology, as well as the weights, potentially offers a way of automating the development of neural network architecture, which currently requires considerable domain knowledge.</s><s coords="3,502.82,424.75,49.11,8.97;3,300.93,436.71,251.06,8.97;3,300.93,448.66,251.06,8.97;3,300.93,460.62,123.39,8.97">The promise of these techniques is that evolution could find a neural network topology that is better at playing a certain game than existing human-designed architectures.</s><s coords="3,428.50,460.62,123.46,8.97;3,300.93,472.57,251.06,8.97;3,300.93,484.52,251.04,8.97;3,300.93,496.49,251.05,8.97;3,300.93,508.44,251.05,8.97;3,300.93,520.39,251.03,8.97;3,300.93,532.35,251.07,8.97;3,300.93,544.30,32.36,8.97">While NE has been traditionally applied to problems with lower input dimensionality than typical DL approaches, recently, Salimans et al. <ref type="bibr" coords="3,497.25,484.52,21.57,8.97" target="#b120">[121]</ref> showed that evolution strategies (ESs), which rely on parameter exploration through stochastic noise instead of calculating gradients, can achieve results competitive to current deep RL approaches for Atari video game playing, given enough computational resources.</s></p><p><s coords="3,310.89,556.26,241.09,8.97;3,300.93,568.22,251.05,8.97;3,300.93,580.17,251.05,8.97;3,300.93,592.13,84.02,8.97">5) Hybrid Learning Approaches: More recently, researchers have started to investigate hybrid approaches for video game playing, which combine DL methods with other machine learning approaches.</s><s coords="3,389.08,592.13,162.89,8.97;3,300.93,604.08,250.94,8.97;3,300.93,616.03,251.05,8.97;3,300.93,627.99,224.67,8.97">Alvernaz and Togelius <ref type="bibr" coords="3,486.22,592.13,11.62,8.97" target="#b0">[1]</ref> and Poulsen et al. <ref type="bibr" coords="3,321.51,604.08,21.57,8.97" target="#b112">[113]</ref> experimented with combining a deep network trained through gradient descent feeding a condensed feature representation into a network trained through artificial evolution.</s><s coords="3,528.19,627.99,23.79,8.97;3,300.93,639.95,251.07,8.97;3,300.93,651.90,251.06,8.97;3,300.93,663.86,251.07,8.97;3,300.93,675.81,224.36,8.97">These hybrids aim to combine the best of both approaches as DL methods are able to learn directly from high-dimensional input, while evolutionary methods do not rely on differentiable architectures and work well in games with sparse rewards.</s><s coords="3,529.30,675.81,22.69,8.97;3,300.93,687.77,251.07,8.97;3,300.93,699.73,251.03,8.97;3,300.93,711.68,251.05,8.97;3,300.93,723.64,129.07,8.97">Some results suggest that gradient-free methods seem to be better in the early stages of training to avoid premature convergence, while gradient-based methods may be better in the end when less exploration is needed <ref type="bibr" coords="3,405.93,723.64,20.05,8.97" target="#b138">[139]</ref>.</s></p><p><s coords="4,52.08,66.09,241.09,8.97;4,42.12,78.04,251.05,8.97;4,42.12,90.01,251.04,8.97;4,42.12,101.96,152.05,8.97">Another hybrid method for board game playing was AlphaGo <ref type="bibr" coords="4,42.12,78.04,21.57,8.97" target="#b132">[133]</ref> that relied on deep neural networks and tree search methods to defeat the world champion in Go, and <ref type="bibr" coords="4,227.24,90.01,16.59,8.97" target="#b35">[36]</ref> that applies planning on top of a predictive model.</s></p><p><s coords="4,52.08,113.91,241.09,8.97;4,42.12,125.87,251.05,8.97;4,42.12,137.82,251.06,8.97;4,42.12,149.78,197.28,8.97">In general, the hybridization of ontogenetic RL (such as Qlearning) with phylogenetic methods (such as evolutionary algorithms) has the potential to be very impactful, as it could enable concurrent learning on different timescales <ref type="bibr" coords="4,215.34,149.78,20.05,8.97" target="#b152">[153]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,71.34,174.71,192.60,8.97">III. GAME GENRES AND RESEARCH PLATFORMS</head><p><s coords="4,52.08,191.64,241.09,8.97;4,42.12,203.60,251.07,8.97">The fast progression of DL methods is undoubtedly due to the convention of comparing results on publicly available datasets.</s><s coords="4,42.12,215.56,251.02,8.97;4,42.12,227.52,251.05,8.97;4,42.12,239.47,251.04,8.97">A similar convention in game AI is to use game environments to compare game playing algorithms, in which methods are ranked based on their ability to score points or win in games.</s><s coords="4,42.12,251.42,251.03,8.97;4,42.12,263.38,251.04,8.97;4,42.12,275.33,91.12,8.97">Conferences like the IEEE Conference on Computational Intelligence and Games run popular competitions in a variety of game environments.</s></p><p><s coords="4,52.08,287.29,241.08,8.97;4,42.12,299.25,251.04,8.97;4,42.12,311.20,251.05,8.97;4,42.12,323.15,251.03,8.97;4,42.12,335.11,165.00,8.97">This section describes popular game genres and research platforms, used in the literature, that are relevant to DL; some examples are shown in Fig. <ref type="figure" coords="4,146.37,311.20,3.74,8.97" target="#fig_0">2</ref>. For each genre, we briefly outline what characterizes that genre and describe the challenges faced by algorithms playing games of the genre.</s><s coords="4,209.15,335.11,84.01,8.97;4,42.12,347.07,251.05,8.97;4,42.12,359.02,251.04,8.97;4,42.12,370.98,251.06,8.97;4,42.12,382.93,85.53,8.97">The video games that are discussed in this paper have to a large extent supplanted an earlier generation of simpler control problems that long served as the main RL benchmarks but are generally too simple for modern RL methods.</s><s coords="4,130.71,382.93,162.49,8.97;4,42.12,394.88,251.05,8.97;4,42.12,406.84,44.89,8.97">In such classic control problems, the input is a simple feature vector, describing the position, velocity, and angles.</s><s coords="4,89.59,406.84,203.60,8.97;4,42.12,418.80,251.06,8.97;4,42.12,430.75,251.05,8.97;4,42.12,442.71,251.05,8.97;4,42.12,454.66,119.45,8.97">Popular platforms for such problems are rllab <ref type="bibr" coords="4,274.10,406.84,15.27,8.97" target="#b28">[29]</ref>, which includes classic problems such as pole balancing and the mountain car problem, and MuJoCo (multijoint dynamics with contact), a physics engine for complex control tasks such as the humanoid walking task <ref type="bibr" coords="4,137.51,454.66,20.05,8.97" target="#b151">[152]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,42.12,479.59,72.61,8.97">A. Arcade Games</head><p><s coords="4,52.08,496.53,241.09,8.97;4,42.12,508.49,251.08,8.97;4,42.12,520.44,251.03,8.97;4,42.12,532.39,117.00,8.97">Classic arcade games, of the type found in the late seventies' and early eighties' arcade cabinets, home video game consoles, and home computers, have been commonly used as AI benchmarks within the last decade.</s><s coords="4,161.91,532.39,131.29,8.97;4,42.12,544.36,251.03,8.97;4,42.12,556.31,73.10,8.97">Representative platforms for this game type are the Atari 2600, Nintendo NES, Commodore 64, and ZX Spectrum.</s><s coords="4,117.61,556.31,175.57,8.97;4,42.12,568.26,251.06,8.97;4,42.12,580.22,251.04,8.97;4,42.12,592.17,251.05,8.97;4,42.12,604.13,251.05,8.97;4,42.12,616.09,210.66,8.97">Most classic arcade games are characterized by movement in a 2-D space (sometimes represented isometrically to provide the illusion of 3-D movement), heavy use of graphical logics (where game rules are triggered by the intersection of sprites or images), continuous-time progression, and either continuous-space or discrete-space movement.</s><s coords="4,255.29,616.09,37.87,8.97;4,42.12,628.04,171.49,8.97">The challenges of playing such games vary by game.</s><s coords="4,215.45,628.04,77.73,8.97;4,42.12,639.99,251.06,8.97;4,42.12,651.95,251.05,8.97;4,42.12,663.90,194.99,8.97">Most games require fast reactions and precise timing, and a few games, in particular, early sports games such as Track &amp; Field <ref type="bibr" coords="4,229.25,651.95,63.92,8.97">(Konami, 1983)</ref> rely almost exclusively on speed and reactions.</s><s coords="4,240.90,663.90,52.26,8.97;4,42.12,675.86,251.06,8.97;4,42.12,687.82,251.07,8.97;4,42.12,699.77,82.04,8.97">Many games require prioritization of several co-occurring events, which requires some ability to predict the behavior or trajectory of other entities in the game.</s><s coords="4,127.28,699.77,165.88,8.97;4,42.12,711.73,251.06,8.97;4,42.12,723.68,251.05,8.97;4,305.14,66.10,215.68,8.97">This challenge is explicit in, e.g., Tapper (Bally Midway, 1983) but also in different ways part of platform games such as Super Mario Bros. <ref type="bibr" coords="4,205.95,723.68,69.47,8.97">(Nintendo, 1985)</ref> and shooters such as Missile Command <ref type="bibr" coords="4,448.19,66.10,68.34,8.97">(Atari Inc., 1980)</ref>.</s><s coords="4,523.55,66.10,32.64,8.97;4,305.14,78.05,251.03,8.97;4,305.14,90.01,251.04,8.97;4,305.14,101.97,251.04,8.97">Another common requirement is navigating mazes or other complex environments, as exemplified clearly by games such as Pac-Man (Namco, 1980) and Boulder Dash <ref type="bibr" coords="4,445.46,101.97,106.43,8.97">(First Star Software, 1984)</ref>.</s><s coords="4,305.13,113.92,251.06,8.97;4,305.13,125.88,251.06,8.97;4,305.13,137.83,166.10,8.97">Some games, such as Montezuma's Revenge <ref type="bibr" coords="4,487.28,113.92,68.92,8.97;4,305.13,125.88,21.44,8.97">(Parker Brothers, 1984)</ref>, require long-term planning involving the memorization of temporarily unobservable game states.</s><s coords="4,474.31,137.83,81.90,8.97;4,305.13,149.78,251.05,8.97;4,305.13,161.74,138.07,8.97">Some games feature incomplete information and stochasticity; others are completely deterministic and fully observable.</s></p><p><s coords="4,315.10,173.70,241.11,8.97;4,305.13,185.66,171.59,8.97">The most notable game platform used for DL methods is the Arcade Learning Environment (ALE) <ref type="bibr" coords="4,457.64,185.66,15.26,8.97" target="#b9">[10]</ref>.</s><s coords="4,479.26,185.66,76.91,8.97;4,305.13,197.61,251.05,8.97;4,305.13,209.56,109.92,8.97">ALE is built on top of the Atari 2600 emulator Stella and contains more than 50 original Atari 2600 games.</s><s coords="4,418.63,209.56,137.56,8.97;4,305.13,220.85,251.05,9.96;4,305.13,233.47,170.82,8.97">The framework extracts the game score, 160 × 210 screen pixels, and the RAM content that can be used as input for game playing agents.</s><s coords="4,479.31,233.47,76.88,8.97;4,305.13,245.43,251.05,8.97;4,305.13,257.39,81.44,8.97">ALE was the main environment explored in the first deep RL papers that used raw pixels as input.</s><s coords="4,390.49,257.39,165.71,8.97;4,305.13,269.34,251.06,8.97;4,305.13,281.29,236.88,8.97">By enabling agents to learn from visual input, ALE thus differs from classic control problems in the RL literature, such as the Cart Pole and Mountain Car problems.</s><s coords="4,544.00,281.29,12.17,8.97;4,305.13,293.25,251.04,8.97;4,305.13,305.21,29.33,8.97">An overview and discussion of the ALE environment can be found in <ref type="bibr" coords="4,315.37,305.21,15.27,8.97" target="#b90">[91]</ref>.</s></p><p><s coords="4,315.10,317.16,241.09,8.97;4,305.13,329.12,251.02,8.97;4,305.13,341.07,251.04,8.97;4,305.13,353.02,19.08,8.97">Another platform for classic arcade games is the Retro Learning Environment (RLE) that currently contains seven games released for the Super Nintendo Entertainment System (SNES) <ref type="bibr" coords="4,305.13,353.02,15.26,8.97" target="#b14">[15]</ref>.</s><s coords="4,326.13,353.02,230.03,8.97;4,305.13,364.98,162.59,8.97">Many of these games have 3-D graphics, and the controller allows for over 720 action combinations.</s><s coords="4,470.22,364.98,85.96,8.97;4,305.13,376.94,251.07,8.97;4,305.13,388.89,112.87,8.97">SNES games are thus more complex and realistic than Atari 2600 games, but RLE has not been as popular as ALE.</s></p><p><s coords="4,315.10,400.85,241.07,8.97;4,305.13,412.80,251.07,8.97;4,305.13,424.76,198.78,8.97">The General Video Game AI (GVG-AI) framework <ref type="bibr" coords="4,534.59,400.85,21.58,8.97" target="#b115">[116]</ref> allows for easy creation and modification of games and levels using the video game description language <ref type="bibr" coords="4,479.86,424.76,20.05,8.97" target="#b121">[122]</ref>.</s><s coords="4,506.77,424.76,49.40,8.97;4,305.13,436.71,251.05,8.97">This is ideal for testing the generality of agents on multiple games or levels.</s><s coords="4,305.13,448.67,251.04,8.97;4,305.13,460.63,61.87,8.97">GVG-AI includes over 100 classic arcade games each with five different levels.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,305.13,491.56,72.42,8.97">B. Racing Games</head><p><s coords="4,315.10,508.50,241.08,8.97;4,305.13,520.45,251.06,8.97;4,305.13,532.40,251.08,8.97;4,305.13,544.36,132.38,8.97">Racing games are games where the player is tasked with controlling some kind of vehicle or character so as to reach a goal in the shortest possible time or as to traverse as far as possible along a track in a given time.</s><s coords="4,440.33,544.36,115.85,8.97;4,305.13,556.32,251.06,8.97;4,305.13,568.27,103.48,8.97">Usually, the game employs a first-person perspective or a vantage point from just behind the player-controlled vehicle.</s><s coords="4,412.61,568.27,143.54,8.97;4,305.13,580.23,251.04,8.97;4,305.13,592.18,60.23,8.97">The vast majority of racing games take a continuous input signal as steering input, similar to a steering wheel.</s><s coords="4,367.82,592.18,188.37,8.97;4,305.13,604.13,251.06,8.97;4,305.13,616.10,251.06,8.97;4,305.13,628.05,251.06,8.97;4,305.13,640.00,251.05,8.97;4,305.13,651.96,251.04,8.97;4,305.13,663.91,128.31,8.97">Some games, such as those in the Forza Motorsport <ref type="bibr" coords="4,327.76,604.13,98.51,8.97">(Microsoft Studios, 2005</ref><ref type="bibr" coords="4,426.27,604.13,28.88,8.97">-2016)</ref> or Real Racing (Firemint and EA Games, 2009-2013) series, allow for complex input including gear stick, clutch, and handbrake, whereas more arcadefocused games such as those in the Need for Speed <ref type="bibr" coords="4,511.93,640.00,44.25,8.97;4,305.13,651.96,42.12,8.97">(Electronic Arts, 1994</ref><ref type="bibr" coords="4,347.26,651.96,28.88,8.97">-2015)</ref> series typically have a simpler set of inputs and thus lower branching factor.</s></p><p><s coords="4,315.10,675.86,241.09,8.97;4,305.13,687.83,251.06,8.97;4,305.13,699.78,251.06,8.97;4,305.13,711.74,165.85,8.97">A challenge that is common in all racing games is that the agent needs to control the position of the vehicle and adjust the acceleration or braking, using fine-tuned continuous input, so as to traverse the track as fast as possible.</s><s coords="4,473.55,711.74,82.63,8.97;4,305.13,723.69,251.06,8.97">Doing this optimally requires at least short-term planning, one or two turns forward.</s><s coords="5,37.91,189.82,251.07,8.97;5,37.91,201.77,251.04,8.97">If there are resources to be managed in the game, such as fuel, damage, or speed boosts, this requires longer term planning.</s><s coords="5,37.91,213.73,251.05,8.97;5,37.91,225.68,251.04,8.97;5,37.91,237.63,251.03,8.97;5,37.91,249.59,251.03,8.97;5,37.91,261.55,109.58,8.97">When other vehicles are present on the track, there is an adversarial planning aspect added, in trying to manage or block overtaking; this planning is often done in the presence of hidden information (position and resources of other vehicles on different parts of the track).</s></p><p><s coords="5,47.87,273.50,241.02,8.97;5,37.91,285.46,199.87,8.97">A popular environment for visual RL with realistic 3-D graphics is the open racing car simulator TORCS <ref type="bibr" coords="5,213.72,285.46,20.05,8.97" target="#b167">[168]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="5,37.91,320.26,131.50,8.97">C. First-Person Shooters (FPSs)</head><p><s coords="5,47.87,337.19,241.10,8.97;5,37.91,349.15,132.26,8.97">More advanced game environments have recently emerged for visual RL agents in an FPS.</s><s coords="5,173.92,349.15,115.04,8.97;5,37.91,361.10,251.06,8.97;5,37.91,373.06,251.08,8.97;5,37.91,385.02,110.76,8.97">In contrast to classic arcade games such as those in the ALE benchmark, FPSs have 3-D graphics with partially observable states and are thus a more realistic environment to study.</s><s coords="5,150.63,385.02,138.34,8.97;5,37.91,396.97,251.05,8.97;5,37.91,408.93,233.69,8.97">Usually, the viewpoint is that of the player-controlled character, though some games that are broadly in the FPS categories adopt an over-the-shoulder viewpoint.</s><s coords="5,273.48,408.93,15.49,8.97;5,37.91,420.88,251.06,8.97;5,37.91,432.84,251.04,8.97;5,37.91,444.80,93.23,8.97">The design of FPS games is such that part of the challenge is simply fast perception and reaction, in particular, spotting enemies and quickly aiming at them.</s><s coords="5,133.17,444.80,155.77,8.97;5,37.91,456.75,251.05,8.97;5,37.91,468.70,251.03,8.97;5,37.91,480.66,251.05,8.97">But there are other cognitive challenges as well, including orientation and movement in a complex 3-D environment, predicting actions and locations of multiple adversaries, and in some game modes also team-based collaboration.</s><s coords="5,37.91,492.61,251.06,8.97;5,37.91,504.57,131.61,8.97">If visual inputs are used, there is the challenge of extracting relevant information from pixels.</s></p><p><s coords="5,47.87,516.53,241.09,8.97;5,37.91,528.48,251.02,8.97;5,37.91,540.43,140.27,8.97">Among FPS platforms are ViZDoom, a framework that allows agents to play the classic FPS Doom (id Software, 1993-2017) using the screen buffer as input <ref type="bibr" coords="5,159.09,540.43,15.26,8.97" target="#b72">[73]</ref>.</s><s coords="5,179.74,540.43,109.13,8.97;5,37.91,552.39,251.04,8.97;5,37.91,564.34,162.22,8.97">DeepMind Lab is a platform for 3-D navigation and puzzle-solving tasks based on the Quake III Arena (id Software, 1999) engine <ref type="bibr" coords="5,186.03,564.34,10.58,8.97" target="#b5">[6]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="5,37.91,599.15,94.19,8.97">D. Open-World Games</head><p><s coords="5,47.87,616.09,241.09,8.97;5,37.91,628.04,251.06,8.97;5,37.91,639.99,251.05,8.97;5,37.91,651.95,251.06,8.97;5,37.91,663.90,232.76,8.97">Open-world games such as Minecraft (Mojang, 2011) or the Grand Theft Auto (Rockstar Games, 1997-2013) series are characterized by very nonlinear gameplay, with a large game world to explore, either no set goals or many goals with unclear internal ordering, and large freedom of action at any given time.</s><s coords="5,272.77,663.90,16.19,8.97;5,37.91,675.86,251.07,8.97;5,37.91,687.82,143.29,8.97">Key challenges for agents are exploring the world and setting goals, which are realistic and meaningful.</s><s coords="5,184.47,687.82,104.48,8.97;5,37.91,699.77,251.04,8.97;5,37.91,711.73,251.03,8.97;5,37.91,723.68,53.33,8.97">As this is a very complex challenge, most research uses these open environments to explore RL methods that can reuse and transfer learned knowledge to new tasks.</s><s coords="5,94.65,723.68,194.31,8.97;5,37.91,735.63,251.04,8.97;5,37.91,747.60,143.05,8.97">Project Malmo is a platform built on top of the open-world game Minecraft, which can be used to define many diverse and complex problems <ref type="bibr" coords="5,161.87,747.60,15.27,8.97" target="#b64">[65]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="5,300.92,189.82,146.04,8.97">E. Real-Time Strategy (RTS) Games</head><p><s coords="5,310.89,206.76,241.11,8.97;5,300.92,218.71,251.04,8.97;5,300.92,230.67,149.03,8.97">Strategy games are games where the player controls multiple characters or units, and the objective of the game is to prevail in some sort of conquest or conflict.</s><s coords="5,453.45,230.67,98.51,8.97;5,300.92,242.63,251.07,8.97;5,300.92,254.58,167.18,8.97">Usually, but not always, the narrative and graphics reflect a military conflict, where units may be, e.g., knights, tanks, or battleships.</s><s coords="5,470.19,254.58,81.82,8.97;5,300.92,266.54,251.04,8.97;5,300.92,278.49,57.78,8.97">The key challenge in strategy games is to lay out and execute complex plans involving multiple units.</s><s coords="5,361.41,278.49,190.58,8.97;5,300.92,290.44,251.04,8.97;5,300.92,302.40,251.07,8.97;5,300.92,314.36,231.98,8.97">This challenge is in general significantly harder than the planning challenge in classic board games such as Chess mainly because multiple units must be moved at any time and the effective branching factor is typically enormous.</s><s coords="5,536.48,314.36,15.49,8.97;5,300.92,326.31,251.06,8.97;5,300.92,338.27,240.19,8.97">The planning horizon can be extremely long, where actions that are taken at the beginning of a game impact the overall strategy.</s><s coords="5,543.67,338.27,8.30,8.97;5,300.92,350.22,251.06,8.97;5,300.92,362.17,229.98,8.97">In addition, there is the challenge of predicting the moves of one or several adversaries, who have multiple units themselves.</s><s coords="5,534.29,362.17,17.67,8.97;5,300.92,374.14,251.08,8.97;5,300.92,386.09,230.76,8.97">RTS games are strategy games, which do not progress in discrete turns, but where actions can be taken at any point in time.</s><s coords="5,534.29,386.09,17.67,8.97;5,300.92,398.04,251.02,8.97;5,300.92,410.00,194.59,8.97">RTS games add the challenge of time prioritization to the already substantial challenges of playing strategy games.</s></p><p><s coords="5,310.89,421.95,241.12,8.97;5,300.92,433.91,232.47,8.97">The StarCraft (Blizzard Entertainment, 1998-2017) series is without a doubt the most studied game in the RTS genre.</s><s coords="5,536.49,433.91,15.49,8.97;5,300.92,445.87,106.28,8.97;5,407.21,444.28,3.49,6.28;5,415.02,445.86,136.95,8.97;5,300.92,457.81,251.05,8.97;5,300.92,469.77,107.19,8.97">The Brood War API (BWAPI) <ref type="foot" coords="5,407.21,444.28,3.49,6.28" target="#foot_1">1</ref> enables software to communicate with StarCraft while the game runs, e.g., to extract state features and perform actions.</s><s coords="5,411.67,469.77,140.28,8.97;5,300.92,481.72,251.06,8.97;5,300.92,493.67,111.60,8.97">BWAPI has been used extensively in game AI research, but, currently, only a few examples exist where DL has been applied.</s><s coords="5,415.06,493.67,136.93,8.97;5,300.92,505.64,251.06,8.97;5,300.92,517.59,251.05,8.97;5,300.92,529.54,48.81,8.97">TorchCraft is a library built on top of BWAPI that connects the scientific computing framework Torch to StarCraft to enable machine learning research for this game <ref type="bibr" coords="5,325.67,529.54,20.05,8.97" target="#b144">[145]</ref>.</s><s coords="5,352.96,529.54,199.02,8.97;5,300.92,541.50,251.05,8.97;5,300.92,553.45,251.05,8.97;5,300.92,565.40,209.31,8.97">Additionally, DeepMind and Blizzard (the developers of StarCraft) have developed a machine learning API to support research in StarCraft II with features such as simplified visuals designed for convolutional networks <ref type="bibr" coords="5,486.17,565.40,20.05,8.97" target="#b156">[157]</ref>.</s><s coords="5,514.23,565.40,37.74,8.97;5,300.92,577.37,251.06,8.97;5,300.92,589.32,70.35,8.97">This API contains several mini-challenges, while it also supports the full 1v1 game setting.</s><s coords="5,373.69,588.65,178.29,9.63;5,300.93,601.27,251.04,8.97;5,300.93,613.23,104.20,8.97">μRTS <ref type="bibr" coords="5,398.71,589.32,21.56,8.97" target="#b103">[104]</ref> and ELF <ref type="bibr" coords="5,459.62,589.32,21.57,8.97" target="#b150">[151]</ref> are two minimalistic RTS game engines that implement some of the features that are present in RTS games.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="5,300.93,638.55,92.04,8.97">F. Team Sports Games</head><p><s coords="5,310.89,655.48,241.00,8.97;5,300.93,667.44,152.30,8.97">Popular sports games are typically based on team-based sports such as soccer, basketball, and football.</s><s coords="5,454.75,667.44,97.15,8.97;5,300.93,679.39,251.06,8.97">These games aim to be as realistic as possible with life-like animations and 3-D graphics.</s><s coords="5,300.93,691.35,251.02,8.97;5,300.93,703.30,251.06,8.97;5,300.93,715.26,251.05,8.97;6,42.12,66.09,63.31,8.97">Several soccer-like environments have been used extensively as research platforms, both with physical robots and 2-D/3-D simulations, in the annual Robot World Cup Soccer Games (RoboCup) <ref type="bibr" coords="6,91.33,66.09,10.58,8.97" target="#b2">[3]</ref>.</s><s coords="6,109.81,66.09,183.36,8.97;6,42.12,78.04,251.04,8.97;6,42.12,90.01,251.04,8.97">Keepaway Soccer is a simplistic soccer-like environment, where one team of agents try to maintain control of the ball while another team tries to gain control of it <ref type="bibr" coords="6,269.10,90.01,20.05,8.97" target="#b137">[138]</ref>.</s><s coords="6,42.12,101.96,251.06,8.97;6,42.12,113.91,251.05,8.97;6,42.12,125.87,251.04,8.97;6,42.12,137.82,39.29,8.97">A similar environment for multiagent learning is RoboCup 2-D Half-Field-Offense (HFO), where teams of two to three players either take the role as offense or defense on one half of a soccer field <ref type="bibr" coords="6,62.33,137.82,15.27,8.97" target="#b49">[50]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,42.12,161.89,104.42,8.97">G. Text Adventure Games</head><p><s coords="6,52.08,178.83,241.08,8.97;6,42.12,190.78,251.05,8.97;6,42.12,202.73,251.03,8.97;6,42.12,214.69,112.85,8.97">A classic text adventure game is a form of interactive fiction, where players are given descriptions and instructions in text, rather than graphics, and interact with the storyline through text-based commands <ref type="bibr" coords="6,130.91,214.69,20.05,8.97" target="#b143">[144]</ref>.</s><s coords="6,157.38,214.69,135.79,8.97;6,42.12,226.64,251.08,8.97;6,42.12,238.60,251.06,8.97;6,42.12,250.56,60.77,8.97">These commands are usually used to query the system about the state, interact with characters in the story, collect and use items, or navigate the space in the fictional world.</s></p><p><s coords="6,52.08,262.51,241.09,8.97;6,42.12,274.46,251.04,8.97">These games typically implement one of three text-based interfaces: parser-based, choice-based, and hyperlink-based <ref type="bibr" coords="6,274.09,274.46,15.26,8.97" target="#b53">[54]</ref>.</s><s coords="6,42.12,286.42,251.04,8.97;6,42.12,298.38,251.05,8.97;6,42.12,310.33,138.45,8.97">Choice-based and hyperlink-based interfaces provide the possible actions to the player at a given state as a list, out of context, or as links in the state description.</s><s coords="6,183.50,310.33,109.68,8.97;6,42.12,322.29,251.07,8.97;6,42.12,334.24,138.56,8.97">Parser-based interfaces are, on the other hand, open to any input, and the player has to learn what words the game understands.</s><s coords="6,183.49,334.24,109.67,8.97;6,42.12,346.19,251.06,8.97;6,42.12,358.15,251.06,8.97;6,42.12,370.11,162.03,8.97">This is interesting for computers as it is much more akin to natural language, where you have to know what actions should exist based on your understanding of language and the given state.</s></p><p><s coords="6,52.08,382.06,241.11,8.97;6,42.12,394.02,251.05,8.97;6,42.12,405.97,122.73,8.97">Unlike some other game genres, like arcade games, text adventure games have not had a standard benchmark of games that everyone can compare against.</s><s coords="6,167.48,405.97,125.68,8.97;6,42.12,417.93,82.40,8.97">This makes a lot of results hard to directly compare.</s><s coords="6,128.27,417.93,164.89,8.97;6,42.12,429.88,251.03,8.97;6,42.12,441.84,167.75,8.97">A lot of research has focused on games that run on Infocom's Z-Machine game engine, an engine that can play a lot of the early classic games.</s><s coords="6,213.26,441.84,79.92,8.97;6,42.12,453.80,251.03,8.97;6,42.12,465.75,167.24,8.97">Recently, Microsoft has introduced the environment TextWorld to help create a standardized text adventure environment <ref type="bibr" coords="6,190.28,465.75,15.26,8.97" target="#b24">[25]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,42.12,489.82,123.24,8.97">H. OpenAI Gym and Universe</head><p><s coords="6,52.08,506.75,241.09,8.97;6,42.12,518.71,251.04,8.97;6,42.12,530.66,251.06,8.97;6,42.12,542.61,19.08,8.97">OpenAI Gym is a large platform for comparing RL algorithms with a single interface to a suite of different environments including ALE, GVG-AI, MuJoCo, Malmo, ViZDoom, and more <ref type="bibr" coords="6,42.12,542.61,15.26,8.97" target="#b16">[17]</ref>.</s><s coords="6,65.15,542.61,228.01,8.97;6,42.12,554.57,251.08,8.97;6,42.12,566.52,209.53,8.97;6,251.66,564.94,3.49,6.28">OpenAI Universe is an extension to OpenAI Gym that currently interfaces with more than a thousand Flash games and aims to add many modern video games in the future. 2</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,89.46,590.59,156.37,8.97">IV. DL METHODS FOR GAME PLAYING</head><p><s coords="6,52.08,607.52,241.09,8.97;6,42.12,619.48,152.87,8.97">This section gives an overview of DL techniques used to play video games, divided by game genre.</s><s coords="6,198.42,619.48,94.75,8.97;6,42.12,631.43,251.05,8.97;6,42.12,643.39,240.37,8.97">Table II lists DL methods for each game genre and highlights which input features, network architecture, and training methods they rely upon.</s><s coords="6,285.97,643.39,7.19,8.97;6,42.12,655.34,251.05,8.97;6,42.12,667.29,25.74,8.97">A typical neural network architecture used in deep RL is shown in Fig. <ref type="figure" coords="6,60.39,667.29,3.73,8.97">3</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,42.12,691.36,72.61,8.97">A. Arcade Games</head><p><s coords="6,52.08,708.30,241.09,8.97;6,42.12,720.25,251.02,8.97;6,50.09,747.41,93.49,8.71">The ALE consists of more than 50 Atari games and has been the main testbed for deep RL algorithms that learn control 2 https://universe.openai.com/</s><s coords="6,305.14,293.29,142.08,8.97">policies directly from raw pixels.</s><s coords="6,452.32,293.29,103.88,8.97;6,305.14,305.25,235.49,8.97">This section reviews the main advancements that have been demonstrated in ALE.</s><s coords="6,544.04,305.25,12.17,8.97;6,305.14,317.20,209.27,8.97">An overview of these advancements is shown in Table <ref type="table" coords="6,508.60,317.20,2.91,8.97" target="#tab_0">I</ref>.</s><s coords="6,315.10,329.15,241.09,8.97;6,305.14,341.12,205.24,8.97">Deep Q-network (DQN) was the first learning algorithm that showed human expert-level control in ALE <ref type="bibr" coords="6,491.29,341.12,15.26,8.97" target="#b96">[97]</ref>.</s><s coords="6,514.80,341.12,41.38,8.97;6,305.14,353.07,251.05,8.97;6,305.14,365.02,251.05,8.97;6,305.14,376.98,227.25,8.97">DQN was tested in seven Atari 2600 games and outperformed previous approaches, such as SARSA with feature construction <ref type="bibr" coords="6,527.12,365.02,11.62,8.97" target="#b6">[7]</ref> and NE <ref type="bibr" coords="6,320.63,376.98,15.26,8.97" target="#b48">[49]</ref>, as well as a human expert on three of the games.</s><s coords="6,534.61,376.98,21.58,8.97;6,305.14,388.93,251.05,8.97;6,305.14,400.22,72.07,9.63;6,377.20,398.85,3.98,6.68;6,382.61,400.22,173.59,9.96;6,305.14,412.18,251.06,9.63">DQN is based on Q-learning, where a neural network model learns to approximate Q π (s, a) that estimates the expected return of taking action a in state s while following a behavior policy μ.</s><s coords="6,305.14,424.80,251.03,8.97;6,305.14,436.75,251.05,8.97;6,305.14,448.71,91.29,8.97">A simple network architecture consisting of two convolutional layers followed by a single fully-connected layer was used as a function approximator.</s></p><p><s coords="6,315.10,460.66,241.09,8.97;6,305.14,471.95,251.03,10.42;6,305.14,484.57,251.07,8.97;6,305.14,496.53,34.02,8.97">A key mechanism in DQN is experience replay <ref type="bibr" coords="6,509.77,460.66,15.27,8.97" target="#b88">[89]</ref>, where experiences in the form {s t , a t , r t+1 , s t+1 } are stored in a replay memory and randomly sampled in batches when the network is updated.</s><s coords="6,343.19,496.53,213.00,8.97;6,305.14,508.49,251.04,8.97;6,305.14,520.44,59.36,8.97">This enables the algorithm to reuse and learn from past and uncorrelated experiences, which reduces the variance of the updates.</s><s coords="6,367.50,520.44,188.69,8.97;6,305.14,532.39,251.04,8.97;6,305.14,544.35,251.04,8.97;6,305.14,556.31,188.51,8.97">DQN was later extended with a separate target Q-network, the parameters of which are held fixed between individual updates, and was shown to achieve above human expert scores in 29 out of 49 tested games <ref type="bibr" coords="6,474.56,556.31,15.27,8.97" target="#b97">[98]</ref>.</s></p><p><s coords="6,315.10,568.26,241.07,8.97;6,305.14,580.22,251.05,8.97;6,305.14,592.17,187.02,8.97">Deep recurrent Q-learning (DRQN) extends the DQN architecture with a recurrent layer before the output and works well for games with partially observable states <ref type="bibr" coords="6,473.07,592.17,15.27,8.97" target="#b50">[51]</ref>.</s></p><p><s coords="6,315.10,604.12,241.10,8.97;6,305.14,616.08,251.06,8.97;6,305.14,628.04,175.77,8.97">A distributed version of DQN was shown to outperform a nondistributed version in 41 of the 49 games using the Gorila architecture (general RL architecture) <ref type="bibr" coords="6,456.84,628.04,20.05,8.97" target="#b99">[100]</ref>.</s><s coords="6,483.15,628.04,73.04,8.97;6,305.14,639.99,251.06,8.97;6,305.14,651.95,251.07,8.97;6,305.14,663.90,85.39,8.97">Gorila parallelizes actors that collect experiences into a distributed replay memory, as well as parallelizing learners that train on samples from the same replay memory.</s></p><p><s coords="6,315.10,675.85,241.09,8.97;6,305.14,687.81,251.04,8.97;6,305.14,699.77,190.48,8.97">One problem with the Q-learning algorithm is that it often overestimates action values because it uses the same value function for action selection and action evaluation.</s><s coords="6,499.21,699.77,56.97,8.97;6,305.14,711.73,251.06,8.97;6,305.14,723.01,248.41,9.63;6,305.14,735.63,251.02,8.97;6,305.14,746.92,214.24,10.42;6,519.38,751.62,2.52,6.68;6,522.90,746.92,30.44,9.96">Double DQN, based on double Q-learning <ref type="bibr" coords="6,416.10,711.73,15.27,8.97" target="#b45">[46]</ref>, reduces the observed overestimation by learning two value networks with parameters θ and θ that both use the other network for value estimation, such that the target Y t = R t+1 + γQ(S t+1 , max a Q(S t+1 , a; θ t ); θ t ) <ref type="bibr" coords="6,529.27,747.59,20.05,8.97" target="#b154">[155]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="7,278.82,65.64,32.25,7.17;7,216.69,74.61,156.49,7.17">TABLE II OVERVIEW OF DL METHODS APPLIED TO GAMES</head><p><s coords="7,50.51,633.63,488.89,6.28;7,50.51,642.60,180.28,6.28">We refer to features as low-dimensional items and values that describe the state of the game such as health, ammunition, score, objects, etc. MLP refers to a traditional fully connected architecture without convolutional or recurrent layers.</s></p><p><s coords="7,47.87,664.20,241.10,8.97;7,37.91,676.15,250.93,8.97;7,37.91,688.11,251.06,8.97;7,37.91,700.06,118.69,8.97">Another improvement is prioritized experience replay from which important experiences are sampled more frequently based on the TD error, which was shown to significantly improve both DQN and double DQN <ref type="bibr" coords="7,132.54,700.06,20.05,8.97" target="#b122">[123]</ref>.</s></p><p><s coords="7,47.87,712.01,241.09,8.97;7,37.91,723.98,251.07,8.97;7,300.92,663.83,32.40,9.63;7,335.55,662.46,3.98,6.68;7,340.95,663.83,137.28,9.96;7,478.22,662.46,3.98,6.68;7,483.63,663.83,68.34,9.96;7,300.93,675.79,7.88,9.54;7,308.80,674.41,3.98,6.68;7,314.21,675.79,41.21,9.96;7,357.62,674.41,3.98,6.68;7,363.03,675.79,31.49,9.96;7,394.52,674.41,3.98,6.68;7,399.93,675.79,50.03,9.96">Dueling DQN uses a network that is split into two streams after the convolutional layers to separately estimate state value V π (s) and the action advantage A π (s, a), such that Q π (s, a) = V π (s) +A π (s, a) <ref type="bibr" coords="7,425.90,676.45,20.05,8.97" target="#b160">[161]</ref>.</s><s coords="7,453.82,676.45,98.14,8.97;7,300.92,688.41,251.03,8.97;7,300.92,700.36,47.46,8.97">Dueling DQN improves double DQN and can also be combined with prioritized experience replay.</s></p><p><s coords="7,310.89,712.32,241.09,8.97;7,300.92,724.27,251.06,8.97;8,42.12,173.14,342.20,7.17">Double DQN and dueling DQN were also tested in the five more complex games in the RLE and achieved a mean score Fig. <ref type="figure" coords="8,57.59,173.14,2.99,7.17">3</ref>. Example of a typical network architecture used in deep RL for game playing with pixel input.</s><s coords="8,387.16,173.14,169.06,7.17;8,42.12,182.10,514.09,7.17">The input usually consists of a preprocessed screen image, or several stacked or concatenated images, which is followed by a couple of convolutional layers (often without pooling), and a few fully connected layers.</s><s coords="8,42.12,191.08,293.68,7.17">Recurrent networks have a recurrent layer, usually an LSTM, after the fully connected layers.</s><s coords="8,337.49,191.08,218.73,7.17;8,42.12,199.51,283.16,7.97">The output typically consists of one unit for each unique combination of actions in the game, and actor-critic methods also have one for the state value V (s).</s><s coords="8,327.69,200.04,228.53,7.17;8,42.12,209.01,514.10,7.17;8,42.12,217.97,63.30,7.17">Examples of this architecture, without a recurrent layer and with some variations, are <ref type="bibr" coords="8,88.98,209.01,8.47,7.17" target="#b8">[9]</ref>, <ref type="bibr" coords="8,102.21,209.01,12.22,7.17" target="#b23">[24]</ref>, <ref type="bibr" coords="8,119.42,209.01,12.22,7.17" target="#b29">[30]</ref>, <ref type="bibr" coords="8,136.63,209.01,12.22,7.17" target="#b34">[35]</ref>, <ref type="bibr" coords="8,153.84,209.01,12.22,7.17" target="#b55">[56]</ref>, <ref type="bibr" coords="8,171.05,209.01,13.02,7.17" target="#b95">[96]</ref>- <ref type="bibr" coords="8,187.32,209.01,13.02,7.17" target="#b97">[98]</ref>, <ref type="bibr" coords="8,205.53,209.01,16.05,7.17" target="#b99">[100]</ref>, <ref type="bibr" coords="8,226.72,209.01,16.05,7.17" target="#b105">[106]</ref>, <ref type="bibr" coords="8,247.92,209.01,16.05,7.17" target="#b119">[120]</ref>, <ref type="bibr" coords="8,269.11,209.01,16.05,7.17" target="#b120">[121]</ref>, <ref type="bibr" coords="8,290.32,209.01,16.05,7.17" target="#b122">[123]</ref>, <ref type="bibr" coords="8,311.51,209.01,16.05,7.17" target="#b138">[139]</ref>, <ref type="bibr" coords="8,332.71,209.01,16.05,7.17" target="#b154">[155]</ref>, <ref type="bibr" coords="8,353.90,209.01,16.05,7.17" target="#b159">[160]</ref>, <ref type="bibr" coords="8,375.10,209.01,16.05,7.17" target="#b160">[161]</ref>, and <ref type="bibr" coords="8,409.75,209.01,16.05,7.17" target="#b165">[166]</ref>, and examples with a recurrent layer are <ref type="bibr" coords="8,42.12,217.97,12.22,7.17" target="#b50">[51]</ref>, <ref type="bibr" coords="8,59.38,217.97,12.22,7.17" target="#b62">[63]</ref>, and <ref type="bibr" coords="8,90.14,217.97,12.22,7.17" target="#b95">[96]</ref>.</s></p><p><s coords="8,42.12,239.72,156.13,8.97">of around 50% of a human expert <ref type="bibr" coords="8,179.17,239.72,15.27,8.97" target="#b14">[15]</ref>.</s><s coords="8,200.86,239.72,92.30,8.97;8,42.12,251.68,251.04,8.97;8,42.12,263.63,112.41,8.97">The best result in these experiments was by dueling DQN in the game Mortal Kombat (Midway, 1992) with 128%.</s></p><p><s coords="8,52.08,275.59,241.09,8.97;8,42.12,287.55,49.41,8.97">Bootstrapped DQN improves exploration by training multiple Q-networks.</s><s coords="8,94.21,287.55,198.98,8.97;8,42.12,299.50,251.06,8.97;8,42.12,311.46,142.68,8.97">A randomly sampled network is used during each training episode, and bootstrap masks modulate the gradients to train the networks differently <ref type="bibr" coords="8,160.74,311.46,20.05,8.97" target="#b105">[106]</ref>.</s></p><p><s coords="8,52.08,323.41,241.11,8.97;8,42.12,335.36,251.04,8.97;8,42.12,347.33,251.05,8.97;8,42.12,359.28,24.06,8.97">Robust policies can be learned with DQN for competitive or cooperative multiplayer games by training one network for each player and play them against each other in the training process <ref type="bibr" coords="8,42.12,359.28,20.05,8.97" target="#b145">[146]</ref>.</s><s coords="8,70.37,359.28,222.81,8.97;8,42.12,371.23,251.07,8.97;8,42.12,383.19,251.03,8.97;8,42.12,395.14,47.14,8.97">Agents trained in the multiplayer mode perform very well against novel opponents, whereas agents trained against a stationary algorithm fail to generalize their strategies to novel adversaries.</s></p><p><s coords="8,52.08,407.10,241.09,8.97;8,42.12,419.06,251.07,8.97;8,42.12,431.01,251.04,8.97;8,42.12,442.96,98.66,8.97">Multithreaded asynchronous variants of DQN, SARSA, and actor-critic methods can utilize multiple CPU threads on a single machine, reducing training roughly linear to the number of parallel threads <ref type="bibr" coords="8,121.70,442.96,15.26,8.97" target="#b95">[96]</ref>.</s><s coords="8,144.99,442.96,148.18,8.97;8,42.12,454.92,251.05,8.97;8,42.12,466.87,251.05,8.97;8,42.12,478.83,62.44,8.97">These variants do not rely on a replay memory because the network is updated on uncorrelated experiences from parallel actors, which also helps stabilize onpolicy methods.</s><s coords="8,106.42,478.83,186.76,8.97;8,42.12,490.79,251.05,8.97;8,42.12,502.74,251.07,8.97;8,42.12,514.70,110.04,8.97">The asynchronous advantage actor-critic (A3C) algorithm is an actor-critic method that uses several parallel agents to collect experiences that all asynchronously update a global actor-critic network.</s><s coords="8,155.46,514.70,137.72,8.97;8,42.12,526.65,251.08,8.97;8,42.12,538.60,146.66,8.97">A3C outperformed prioritized dueling DQN, which was trained for 8 days on a GPU, with just half the training time on a CPU <ref type="bibr" coords="8,169.70,538.60,15.27,8.97" target="#b95">[96]</ref>.</s></p><p><s coords="8,52.08,550.57,241.02,8.97;8,42.12,562.52,251.03,8.97;8,42.12,574.47,251.04,8.97">An actor-critic method with experience replay (ACER) implements an efficient trust region policy method that forces updates to not deviate far from a running average of past policies <ref type="bibr" coords="8,269.10,574.47,20.05,8.97" target="#b159">[160]</ref>.</s><s coords="8,42.12,586.43,251.06,8.97;8,42.12,598.38,251.02,8.97;8,42.12,610.34,172.58,8.97">The performance of the ACER in ALE matches dueling DQN with prioritized experience replay and A3C without experience replay, while it is much more data efficient.</s></p><p><s coords="8,52.08,622.30,241.07,8.97;8,42.12,634.25,170.27,8.97">A3C with progressive neural networks <ref type="bibr" coords="8,209.99,622.30,21.58,8.97" target="#b119">[120]</ref> can effectively transfer learning from one game to another.</s><s coords="8,214.47,634.25,78.67,8.97;8,42.12,646.20,251.03,8.97;8,42.12,658.16,141.91,8.97">The training is done by instantiating a network for every new task with connections to all the previous learned networks.</s><s coords="8,185.98,658.16,107.21,8.97;8,42.12,670.11,147.73,8.97">This gives the new network access to knowledge already learned.</s></p><p><s coords="8,52.08,682.07,241.09,8.97;8,42.12,694.03,251.05,8.97;8,42.12,705.98,251.05,8.97;8,42.12,717.94,91.15,8.97">The advantage actor-critic (A2C), a synchronous variant of A3C <ref type="bibr" coords="8,65.38,694.03,15.27,8.97" target="#b95">[96]</ref>, updates the parameters synchronously in batches and has comparable performance while only maintaining one neural network <ref type="bibr" coords="8,109.21,717.94,20.05,8.97" target="#b165">[166]</ref>.</s><s coords="8,138.10,717.94,155.07,8.97;8,42.12,729.89,251.05,8.97;8,305.13,239.73,251.06,8.97;8,305.13,251.69,48.59,8.97">Actor-critic using Kronecker-factored trust region (ACKTR) extends A2C by approximating the natural policy gradient updates for both the actor and the critic <ref type="bibr" coords="8,329.66,251.69,20.05,8.97" target="#b165">[166]</ref>.</s><s coords="8,357.80,251.69,198.35,8.97;8,305.13,263.64,251.05,8.97;8,305.13,275.60,199.63,8.97">In Atari, ACKTR has slower updates compared to A2C (at most 25% per time step) but is more sample efficient (e.g., by a factor of 10 in Atlantis) <ref type="bibr" coords="8,480.69,275.60,20.06,8.97" target="#b165">[166]</ref>.</s><s coords="8,507.49,275.60,48.70,8.97;8,305.13,287.55,251.06,8.97;8,305.13,299.51,251.02,8.97;8,305.13,311.47,251.07,8.97;8,305.13,323.42,51.31,8.97">Trust region policy optimization (TRPO) uses a surrogate objective with theoretical guarantees for monotonic policy improvement, while it practically implements an approximation called trust region <ref type="bibr" coords="8,332.39,323.42,20.05,8.97" target="#b127">[128]</ref>.</s><s coords="8,358.47,323.42,197.71,8.97;8,305.13,335.37,251.03,8.97;8,305.13,347.33,151.81,8.97">This is done by constraining network updates with a bound on the Kullback-Leibler (KL) divergence between the current and the updated policy.</s><s coords="8,462.31,347.33,93.88,8.97;8,305.13,359.28,251.07,8.97;8,305.13,371.24,184.58,8.97">TRPO has robust and data-efficient performance in Atari games, while it has high memory requirements and several restrictions.</s><s coords="8,492.31,371.24,63.87,8.97;8,305.13,383.20,251.06,8.97;8,305.13,395.15,251.06,8.97;8,305.13,407.10,251.07,8.97;8,305.13,419.06,102.52,8.97">Proximal policy optimization (PPO) is an improvement on TRPO that uses a similar surrogate objective <ref type="bibr" coords="8,430.79,395.15,20.05,8.97" target="#b128">[129]</ref>, but instead uses a soft constraint (originally suggested in <ref type="bibr" coords="8,451.14,407.10,20.75,8.97" target="#b127">[128]</ref>) by adding the KL divergence as a penalty.</s><s coords="8,412.84,419.06,143.35,8.97;8,305.13,431.02,251.03,8.97;8,305.13,442.97,184.65,8.97">Instead of having a fixed penalty coefficient, it uses a clipped surrogate objective that penalizes policy updates outside some specified interval.</s><s coords="8,492.11,442.97,64.06,8.97;8,305.13,454.93,251.06,8.97;8,305.13,466.88,195.46,8.97">PPO was shown to be more sample efficient than A2C and on par with ACER in Atari, while PPO does not rely on replay memory.</s><s coords="8,502.57,466.88,53.64,8.97;8,305.13,478.84,251.05,8.97;8,305.13,490.79,251.05,8.97;8,305.13,502.75,43.97,8.97">PPO was also shown to have comparable or better performance than TRPO in continuous control tasks while being simpler and easier to parallelize.</s></p><p><s coords="8,315.10,514.71,241.09,8.97;8,305.13,526.66,251.05,8.97;8,305.13,538.61,251.03,8.97;8,305.13,550.57,137.55,8.97">Importance weighted actor-learner architecture (IMPALA) is an actor-critic method, where multiple learners with GPU access share gradients between each other while being synchronously updated from a set of actors <ref type="bibr" coords="8,423.60,550.57,15.27,8.97" target="#b29">[30]</ref>.</s><s coords="8,446.02,550.57,110.17,8.97;8,305.13,562.52,196.22,8.97">This method can scale to a large number of machines and outperforms A3C.</s><s coords="8,504.00,562.52,52.19,8.97;8,305.13,574.48,251.05,8.97;8,305.13,586.44,251.07,8.97;8,305.13,598.39,132.03,8.97">Additionally, IMPALA was trained, with one set of parameters, to play all 57 Atari games in ALE with a mean human-normalized score of 176.9% (median of 59.7%) <ref type="bibr" coords="8,418.08,598.39,15.27,8.97" target="#b29">[30]</ref>.</s><s coords="8,440.23,598.39,115.97,8.97;8,305.13,610.34,251.06,8.97;8,305.13,622.30,149.93,8.97">Experiences collected by the actors in the IMPALA setup can lack behind the learners' policy and thus result in off-policy learning.</s><s coords="8,458.26,622.30,97.95,8.97;8,305.13,634.26,251.04,8.97;8,305.13,646.21,251.06,8.97;8,305.13,658.17,89.36,8.97">This discrepancy is mitigated through a V-trace algorithm that weighs the importance of experiences based on the difference between the actor's and learner's policies <ref type="bibr" coords="8,375.41,658.17,15.27,8.97" target="#b29">[30]</ref>.</s></p><p><s coords="8,315.10,670.12,241.07,8.97;8,305.13,682.08,250.97,8.97;8,305.13,694.03,251.04,8.97;8,305.13,705.99,72.29,8.97">The UNsupervised REinforcement and Auxiliary Learning (UNREAL) algorithm is based on A3C but uses a replay memory from which it learns auxiliary tasks and pseudoreward functions concurrently <ref type="bibr" coords="8,358.34,705.99,15.26,8.97" target="#b62">[63]</ref>.</s><s coords="8,380.28,705.99,175.88,8.97;8,305.13,717.95,251.04,8.97;8,305.13,729.90,113.00,8.97">UNREAL only shows a small improvement over vanilla A3C in ALE, but larger improvements in other domains (see Section IV-D).</s></p><p><s coords="9,47.87,66.09,241.09,8.97;9,37.91,77.38,251.05,9.63;9,37.91,90.01,251.06,8.97">Distributional DQN takes a distributional perspective on RL by treating Q(s, a) as an approximate distribution of returns instead of a single approximate expectation for each action <ref type="bibr" coords="9,274.86,90.01,10.58,8.97" target="#b8">[9]</ref>.</s><s coords="9,37.91,101.96,251.05,8.97;9,37.91,113.91,177.28,8.97">The distribution is divided into a so-called set of atoms, which determines the granularity of the distribution.</s><s coords="9,216.99,113.91,71.96,8.97;9,37.91,125.87,251.06,8.97;9,37.91,137.82,251.06,8.97;9,37.91,149.78,251.05,8.97">Their results show that the more fine-grained the distributions are, the better are the results, and with 51 atoms (this variant was called C51), it achieved mean scores in ALE almost comparable to UNREAL.</s></p><p><s coords="9,47.87,161.74,241.10,8.97;9,37.91,173.69,251.04,8.97;9,37.91,185.65,67.02,8.97">In NoisyNets, noise is added to the network parameters, and a unique noise level for each parameter is learned using gradient descent <ref type="bibr" coords="9,85.84,185.65,15.27,8.97" target="#b34">[35]</ref>.</s><s coords="9,107.86,185.65,181.09,8.97;9,37.91,197.60,251.07,8.97;9,37.91,209.55,251.04,8.97;9,37.91,221.52,251.06,8.97;9,37.91,233.47,179.84,8.97">In contrast to -greedy exploration, where an agent samples actions either from the policy or from a uniform random distribution, NoisyNets use a noisy version of the policy to ensure exploration, and this was shown to improve DQN (NoisyNet-DQN) and A3C (NoisyNet-A3C).</s></p><p><s coords="9,47.87,245.42,241.09,8.97;9,37.91,257.38,251.05,8.97;9,37.91,269.33,251.06,8.97;9,37.91,281.28,128.43,8.97">Rainbow combines several DQN enhancements: double DQN, prioritized replay, dueling DQN, distributional DQN, and NoisyNets, and achieved a mean score higher than any of the enhancements individually <ref type="bibr" coords="9,147.26,281.28,15.26,8.97" target="#b55">[56]</ref>.</s></p><p><s coords="9,47.87,293.25,241.00,8.97;9,37.91,305.20,251.06,8.97;9,37.91,317.15,251.07,8.97;9,37.91,329.11,251.04,8.97">ESs are black-box optimization algorithms that rely on parameter exploration through stochastic noise instead of calculating gradients and were found to be highly parallelizable with a linear speedup in training time when more CPUs are used <ref type="bibr" coords="9,264.88,329.11,20.05,8.97" target="#b120">[121]</ref>.</s><s coords="9,37.91,341.06,251.04,8.97;9,37.91,353.02,251.06,8.97;9,37.91,364.98,251.05,8.97;9,37.91,376.93,122.36,8.97">Seven hundred and twenty CPUs were used for 1 h, whereafter ESs managed to outperform A3C (which ran for four days) in 23 out of 51 games, while ES used three to ten times as much data due to its high parallelization.</s><s coords="9,163.70,376.93,125.25,8.97;9,37.91,388.88,187.36,8.97">ESs only ran a single day, and thus, their full potential is currently unknown.</s><s coords="9,228.52,388.88,60.46,8.97;9,37.91,400.84,251.05,8.97;9,37.91,412.79,251.07,8.97;9,37.91,424.76,84.65,8.97">Novelty search is a popular algorithm that can overcome environments with deceptive and/or sparse rewards by guiding the search toward novel behaviors <ref type="bibr" coords="9,103.48,424.76,15.27,8.97" target="#b83">[84]</ref>.</s><s coords="9,125.16,424.76,163.80,8.97;9,37.91,436.71,251.05,8.97;9,37.91,448.66,251.06,8.97;9,37.91,460.62,43.66,8.97">The ES has been extended to use novelty search (NS-ES), which outperforms the ES on several challenging Atari games by defining novel behaviors based on the RAM states <ref type="bibr" coords="9,62.48,460.62,15.27,8.97" target="#b23">[24]</ref>.</s><s coords="9,84.02,460.62,204.94,8.97;9,37.91,472.57,251.06,8.97;9,37.91,484.52,59.26,8.97">A quality-diversity variant called NSR-ES that uses both novelty and the reward signal reach an even higher performance <ref type="bibr" coords="9,78.08,484.52,15.27,8.97" target="#b23">[24]</ref>.</s><s coords="9,99.70,484.52,189.27,8.97;9,37.91,496.49,251.05,8.97;9,37.91,508.44,40.24,8.97">NS-ES and NSR-ES reached worse results on a few games, possibly where the reward function is not sparse or deceptive.</s></p><p><s coords="9,47.87,520.39,241.08,8.97;9,37.91,532.35,251.06,8.97;9,37.91,544.30,251.04,8.97;9,37.91,556.26,73.67,8.97">A simple genetic algorithm with a Gaussian noise mutation operator evolves the parameters of a deep neural network (deep GA) and can achieve surprisingly good scores across several Atari games <ref type="bibr" coords="9,87.51,556.26,20.05,8.97" target="#b138">[139]</ref>.</s><s coords="9,113.44,556.26,175.54,8.97;9,37.91,568.22,251.04,8.97;9,37.91,580.17,41.97,8.97">Deep GA shows comparable results to DQN, A3C, and ES on 13 Atari games using up to thousands of CPUs in parallel.</s><s coords="9,81.76,580.17,207.19,8.97;9,37.91,592.12,251.06,8.97;9,37.91,604.08,186.10,8.97">Additionally, random search, given roughly the same amount of computation, was shown to outperform DQN on four out of 13 games and A3C on five games <ref type="bibr" coords="9,199.95,604.08,20.05,8.97" target="#b138">[139]</ref>.</s><s coords="9,226.49,604.08,62.47,8.97;9,37.91,616.03,251.03,8.97;9,37.91,628.00,251.05,8.97;9,37.91,639.95,251.04,8.97;9,37.91,651.90,251.07,8.97;9,37.91,663.86,58.38,8.97">While there has been concern that evolutionary methods do not scale as well as gradient-descent-based methods, one possibility is separating the feature construction from the policy network; evolutionary algorithms can then create extremely small networks that still play well <ref type="bibr" coords="9,77.21,663.86,15.27,8.97" target="#b25">[26]</ref>.</s></p><p><s coords="9,47.87,675.81,241.11,8.97;9,37.91,687.76,57.28,8.97">A few supervised learning approaches have been applied to arcade games.</s><s coords="9,98.60,687.76,190.38,8.97;9,37.91,699.73,251.07,8.97;9,37.91,711.68,164.15,8.97">In <ref type="bibr" coords="9,110.30,687.76,15.27,8.97" target="#b41">[42]</ref>, a slow planning agent was applied offline, using Monte Carlo tree search, to generate data for training a CNN via multinomial classification.</s><s coords="9,204.19,711.68,84.75,8.97;9,37.91,723.63,221.83,8.97">This approach, called UCTtoClassification, was shown to outperform DQN.</s><s coords="9,263.66,723.63,25.30,8.97;9,37.91,735.59,251.03,8.97;9,300.92,66.10,251.07,8.97;9,300.92,78.05,46.27,8.97">Policy distillation <ref type="bibr" coords="9,83.34,735.59,21.58,8.97" target="#b118">[119]</ref> or actor-mimic <ref type="bibr" coords="9,170.20,735.59,21.57,8.97" target="#b107">[108]</ref> methods can be used to train one network to mimic a set of policies (e.g., for different games).</s><s coords="9,350.11,78.05,201.87,8.97;9,300.92,90.01,184.36,8.97">These methods can reduce the size of the network and sometimes also improve the performance.</s><s coords="9,487.99,90.01,63.99,8.97;9,300.92,101.97,251.06,8.97;9,300.92,113.92,251.07,8.97;9,300.92,125.88,251.03,8.97;9,300.92,137.83,111.60,8.97">A frame prediction model can be learned from a dataset generated by a DQN agent using the encoding-transformation-decoding network architecture; the model can then be used to improve exploration in a retraining phase <ref type="bibr" coords="9,388.46,137.83,20.06,8.97" target="#b102">[103]</ref>.</s><s coords="9,416.02,137.83,135.96,8.97;9,300.92,149.78,251.06,8.97;9,300.92,161.75,251.04,8.97;9,300.92,173.70,251.04,8.97;9,300.92,185.66,110.90,8.97">Self-supervised tasks, such as reward prediction, validation of state-successor pairs, and mapping states and successor states to actions, can define auxiliary losses used in pretraining of a policy network, which ultimately can improve learning <ref type="bibr" coords="9,387.76,185.66,20.05,8.97" target="#b131">[132]</ref>.</s></p><p><s coords="9,310.88,197.61,241.09,8.97;9,300.92,209.56,217.23,8.97">The training objective provides feedback to the agent, while the performance objective specifies the target behavior.</s><s coords="9,520.28,209.56,31.69,8.97;9,300.92,221.52,251.07,8.97;9,300.92,233.48,251.04,8.97">Often, a single reward function takes both roles, but, for some games, the performance objective does not guide the training sufficiently.</s><s coords="9,300.92,245.43,251.05,8.97;9,300.92,256.72,251.07,9.63;9,300.92,269.34,127.21,8.97">The hybrid reward architecture (HRA) splits the reward function into n different reward functions, where each of them is assigned a separate learning agent <ref type="bibr" coords="9,404.07,269.34,20.05,8.97" target="#b155">[156]</ref>.</s><s coords="9,431.11,269.34,120.88,8.97;9,300.92,280.63,251.05,9.63;9,300.92,293.25,161.40,8.97">The HRA does this by having n output streams in the network, and thus n Q-values, which are combined when actions are selected.</s><s coords="9,465.01,293.25,86.99,8.97;9,300.92,305.21,251.04,8.97">The HRA was able to achieve the maximum possible score in less than 3000 episodes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,300.92,336.14,103.28,8.97">B. Montezuma's Revenge</head><p><s coords="9,310.89,353.08,241.06,8.97;9,300.92,365.03,29.58,8.97">Environments with sparse feedback remain an open challenge for RL.</s><s coords="9,333.27,365.03,218.71,8.97;9,300.92,376.99,251.06,8.97;9,300.92,388.94,251.06,8.97;9,300.92,400.90,137.45,8.97">The game Montezuma's Revenge is a good example of such an environment in ALE and has thus been studied in more detail and used for benchmarking learning methods based on intrinsic motivation and curiosity.</s><s coords="9,442.06,400.90,109.91,8.97;9,300.92,412.86,251.04,8.97;9,300.92,424.81,251.01,8.97;9,300.92,436.76,195.89,8.97">The main idea of applying intrinsic motivation is to improve the exploration of the environment based on some self-rewarding system, which eventually will help the agent to obtain an extrinsic reward.</s><s coords="9,499.76,436.76,52.25,8.97;9,300.92,448.72,251.06,8.97;9,300.92,460.67,166.40,8.97">DQN fails to obtain any reward in this game (receiving a score of 0) and Gorila achieves an average score of just 4.2.</s><s coords="9,470.27,460.67,81.70,8.97;9,300.92,472.63,251.04,8.97;9,300.92,484.59,251.06,8.97;9,300.92,496.54,33.66,8.97">A human expert can achieve 4367 points, and it is clear that the methods presented so far are unable to deal with environments with such sparse rewards.</s><s coords="9,336.73,496.54,215.25,8.97;9,300.92,508.50,27.93,8.97">A few promising methods aim to overcome these challenges.</s></p><p><s coords="9,310.89,520.45,241.06,8.97;9,300.92,531.72,251.04,10.71;9,300.93,544.35,251.07,8.97;9,300.93,555.64,251.03,10.71;9,300.93,568.26,251.05,8.97;9,300.93,580.22,39.55,8.97">Hierarchical-DQN (h-DQN) <ref type="bibr" coords="9,430.71,520.45,16.59,8.97" target="#b76">[77]</ref> operates on two temporal scales, where one Q-value function Q 1 (s, a; g), the controller, learns a policy over actions that satisfy goals chosen by a higher level Q-value function Q 2 (s, g), the metacontroller, which learns a policy over intrinsic goals (i.e., which goals to select).</s><s coords="9,343.80,580.22,208.20,8.97;9,300.93,592.17,251.03,8.97;9,300.93,604.12,251.05,8.97;9,300.93,616.08,56.29,8.97">This method was able to reach an average score of around 400 in Montezuma's Revenge, where goals were defined as states in which the agent reaches (collides with) a certain type of object.</s><s coords="9,359.22,616.08,192.74,8.97;9,300.93,628.04,86.87,8.97">This method, therefore, must rely on some object detection mechanism.</s></p><p><s coords="9,310.89,639.99,241.10,8.97;9,300.93,651.95,251.03,8.97;9,300.93,663.90,251.06,8.97;9,300.93,675.85,251.04,8.97;9,300.93,687.81,107.12,8.97">Pseudocounts have been used to provide intrinsic motivation in the form of exploration bonuses when unexpected pixel configurations are observed and can be derived from context tree switching (CTS) density models <ref type="bibr" coords="9,502.32,675.85,11.62,8.97" target="#b7">[8]</ref> or neural density models <ref type="bibr" coords="9,383.97,687.81,20.06,8.97" target="#b106">[107]</ref>.</s><s coords="9,413.04,687.81,138.93,8.97;9,300.93,699.77,251.03,8.97;9,300.93,711.73,251.05,8.97;9,300.93,723.68,215.80,8.97">Density models assign probabilities to images, and a model's pseudocount of an observed image is the model's change in prediction compared to being trained one additional time on the same image.</s><s coords="9,520.99,723.68,30.97,8.97;9,300.93,735.63,251.05,8.97;10,42.12,66.09,251.06,8.97;10,42.12,78.04,251.04,8.97;10,42.12,90.01,64.10,8.97">Impressive results were achieved in Montezuma's Revenge and other hard Atari games by combining DQN with the CTS density model (DQN-CTS) or the PixelCNN density model (DQN-PixelCNN) <ref type="bibr" coords="10,92.11,90.01,10.58,8.97" target="#b7">[8]</ref>.</s><s coords="10,111.52,90.01,181.67,8.97;10,42.12,101.96,251.05,8.97;10,42.12,113.91,63.65,8.97">Interestingly, the results were less impressive when the CTS density model was combined with A3C (A3C-CTS) <ref type="bibr" coords="10,91.66,113.91,10.58,8.97" target="#b7">[8]</ref>.</s></p><p><s coords="10,52.08,125.87,241.08,8.97;10,42.12,137.82,178.51,8.97">Ape-X DQN is a distributed DQN architecture similar to Gorila, as actors are separated from the learner.</s><s coords="10,223.46,137.82,69.72,8.97;10,42.12,149.78,251.05,8.97;10,42.12,161.74,227.27,8.97">Ape-X DQN was able to reach state-of-the-art results across the 57 Atari games using 376 cores and one GPU, running at 50K FPS <ref type="bibr" coords="10,250.31,161.74,15.26,8.97" target="#b60">[61]</ref>.</s><s coords="10,272.15,161.74,21.01,8.97;10,42.12,173.69,251.00,8.97;10,42.12,185.65,251.05,8.97;10,42.12,197.60,251.05,8.97;10,42.12,209.55,172.60,8.97">Deep Q-learning from demonstrations (DQfD) draws samples from an experience replay buffer that is initialized with demonstration data from a human expert and is superior to previous methods on 11 Atari games with sparse rewards <ref type="bibr" coords="10,195.64,209.55,15.27,8.97" target="#b56">[57]</ref>.</s><s coords="10,216.72,209.55,76.46,8.97;10,42.12,221.52,251.05,8.97;10,42.12,233.47,251.06,8.97;10,42.12,245.42,251.04,8.97;10,42.12,257.38,133.53,8.97">Ape-X DQfD combines the distributed architecture from Ape-X, and the learning algorithm from DQfD using expert data and was shown to outperform all previous methods in ALE, as well as beating level 1 in Montezuma's Revenge <ref type="bibr" coords="10,151.59,257.38,20.05,8.97" target="#b111">[112]</ref>.</s></p><p><s coords="10,52.08,269.33,241.09,8.97;10,42.12,281.28,147.46,8.97">To improve the performance, Kaplan et al. augmented the agent training with text instructions.</s><s coords="10,192.93,281.28,100.24,8.97;10,42.12,293.25,251.04,8.97;10,42.12,305.20,251.07,8.97;10,42.12,317.15,60.83,8.97">An instruction-based RL approach that uses both a CNN for visual input and the RNN for text-based instruction inputs managed to achieve a score of 3500 points.</s><s coords="10,105.83,317.15,187.33,8.97;10,42.12,329.11,251.06,8.97;10,42.12,341.06,251.05,8.97;10,42.12,353.02,98.45,8.97">Instructions were linked to positions in rooms, and agents were rewarded when they reached those locations <ref type="bibr" coords="10,42.12,341.06,15.27,8.97" target="#b70">[71]</ref>, demonstrating a fruitful collaboration between a human and a learning algorithm.</s><s coords="10,142.51,353.02,150.64,8.97;10,42.12,364.98,251.06,8.97;10,42.12,376.93,214.23,8.97">Experiments in Montezuma's Revenge also showed that the network learned to generalize to unseen instructions that were similar to previous instructions.</s></p><p><s coords="10,52.08,388.88,241.09,8.97;10,42.12,400.84,251.05,8.97;10,42.12,412.79,251.06,8.97;10,42.12,424.76,169.19,8.97">Similar work demonstrates how an agent can execute textbased commands in a 2-D maze-like environment called XWORLD, such as walking to and picking up objects, after having learned a teacher's language <ref type="bibr" coords="10,187.25,424.76,20.05,8.97" target="#b171">[172]</ref>.</s><s coords="10,213.70,424.76,79.47,8.97;10,42.12,436.71,251.07,8.97">An RNN-based language module is connected to a CNN-based perception module.</s><s coords="10,42.12,448.66,251.04,8.97;10,42.12,460.62,251.06,8.97;10,42.12,472.57,155.50,8.97">These two modules were then connected to an action-selection module and a recognition module that learns the teacher's language in a question answering process.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,42.12,503.51,72.99,8.97">C. Racing Games</head><p><s coords="10,52.08,520.44,241.11,8.97;10,42.12,532.40,251.07,8.97;10,42.12,544.36,251.02,8.97;10,42.12,556.31,251.05,8.97;10,42.12,568.26,113.47,8.97">There are generally two paradigms for vision-based autonomous driving highlighted in <ref type="bibr" coords="10,178.94,532.40,15.49,8.97" target="#b20">[21]</ref>: 1) end-to-end systems that learn to map images to actions directly (behavior reflex); and 2) systems that parse the sensor data to make informed decisions (mediated perception).</s><s coords="10,158.25,568.26,134.90,8.97;10,42.12,580.22,251.06,8.97;10,42.12,592.17,251.04,8.97;10,42.12,604.13,251.05,8.97;10,42.12,616.09,158.83,8.97">An approach that falls in between these paradigms is direct perception, where a CNN learns to map from images to meaningful affordance indicators, such as the car angle and distance to lane markings, from which a simple controller can make decisions <ref type="bibr" coords="10,181.87,616.09,15.26,8.97" target="#b20">[21]</ref>.</s><s coords="10,204.24,616.09,88.91,8.97;10,42.12,628.04,251.06,8.97;10,42.12,639.99,251.03,8.97;10,42.12,651.95,26.29,8.97">Direct perception was trained on recordings of 12 h of human driving in TORCS, and the trained system was able to drive in very diverse environments.</s><s coords="10,71.68,651.95,221.51,8.97;10,42.12,663.91,48.13,8.97">Amazingly, the network was also able to generalize to real images.</s></p><p><s coords="10,52.08,675.86,241.09,8.97;10,42.12,687.82,251.03,8.97;10,42.12,699.77,251.07,8.97;10,42.12,711.73,61.04,8.97">End-to-end RL algorithms such as DQN cannot be directly applied to continuous environments such as racing games because the action space must be discrete and with relatively low dimensionality.</s><s coords="10,105.38,711.73,187.78,8.97;10,42.12,723.68,251.06,8.97;10,42.12,735.64,251.04,8.97">Instead, policy gradient methods, such as actorcritic <ref type="bibr" coords="10,66.10,723.68,16.59,8.97" target="#b26">[27]</ref> and deterministic policy gradient (DPG) <ref type="bibr" coords="10,254.23,723.68,21.58,8.97" target="#b133">[134]</ref> can learn policies in high-dimensional and continuous action spaces.</s></p><p><s coords="10,305.13,66.10,251.04,8.97;10,305.13,78.05,251.07,8.97;10,305.13,90.01,244.15,8.97">Deep DPG (DDPG) is a policy gradient method that implements both experience replay and a separate target network and was used to train a CNN end-to-end in TORCS from images <ref type="bibr" coords="10,530.20,90.01,15.27,8.97" target="#b87">[88]</ref>.</s></p><p><s coords="10,315.10,101.97,241.09,8.97;10,305.13,113.92,239.71,8.97">The aforementioned A3C methods have also been applied to the racing game TORCS using only pixels as input <ref type="bibr" coords="10,525.76,113.92,15.27,8.97" target="#b95">[96]</ref>.</s><s coords="10,547.91,113.92,8.30,8.97;10,305.13,125.88,251.06,8.97;10,305.13,137.83,251.06,8.97;10,305.13,149.78,251.06,8.97;10,305.13,161.75,160.54,8.97">In those experiments, rewards were shaped as the agent's velocity on the track, and after 12 h of training, A3C reached a score between roughly 75% and 90% of a human tester in tracks with and without opponent bots, respectively.</s></p><p><s coords="10,315.10,173.70,241.10,8.97;10,305.13,185.66,251.05,8.97;10,305.13,197.61,251.05,8.97;10,305.13,209.56,251.06,8.97;10,305.13,221.52,153.03,8.97">While most approaches to training deep networks from highdimensional input in video games are based on gradient descent, a notable exception is an approach by Koutník et al. <ref type="bibr" coords="10,510.67,197.61,15.27,8.97" target="#b75">[76]</ref>, where Fourier-type coefficients were evolved that encoded a recurrent network with over one million weights.</s><s coords="10,459.91,221.52,96.27,8.97;10,305.13,233.48,251.08,8.97;10,305.13,245.43,133.93,8.97">Here, evolution was able to find a high-performing controller for TORCS that only relied on high-dimensional visual input.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,305.13,272.57,101.88,8.97">D. First-Person Shooters</head><p><s coords="10,315.10,289.51,241.07,8.97;10,305.13,301.46,251.05,8.97;10,305.13,313.42,196.35,8.97">Kempka et al. <ref type="bibr" coords="10,379.80,289.51,16.59,8.97" target="#b72">[73]</ref> demonstrated that a CNN with maxpooling and fully connected layers trained with DQN can achieve human-like behaviors in basic scenarios.</s><s coords="10,504.44,313.42,51.76,8.97;10,305.13,325.38,114.89,8.97;10,420.02,323.79,3.49,6.28;10,426.44,325.36,129.75,8.97;10,305.14,337.32,251.05,8.97;10,305.14,349.28,126.86,8.97">In the Visual Doom AI Competition 2016, <ref type="foot" coords="10,420.02,323.79,3.49,6.28" target="#foot_3">3</ref> a number of participants submitted pretrained neural-network-based agents that competed in a multiplayer deathmatch setting.</s><s coords="10,434.97,349.28,121.23,8.97;10,305.14,361.23,251.05,8.97;10,305.14,373.18,203.97,8.97">Both a limited competition, in which bots competed in known levels, and a full competition that included bots competing in unseen levels were held.</s><s coords="10,511.07,373.18,45.11,8.97;10,305.14,385.14,251.03,8.97;10,305.14,397.10,154.27,8.97">The winner of the limited track used a CNN trained with A3C using reward shaping and curriculum learning <ref type="bibr" coords="10,435.35,397.10,20.05,8.97" target="#b166">[167]</ref>.</s><s coords="10,461.53,397.10,94.63,8.97;10,305.14,409.05,251.07,8.97;10,305.14,421.01,251.05,8.97;10,305.14,432.96,121.12,8.97">Reward shaping tackled the problem of sparse and delayed rewards, giving artificial positive rewards for picking up items and negative rewards for using ammunition and losing health.</s><s coords="10,428.73,432.96,127.48,8.97;10,305.14,444.91,251.03,8.97;10,305.14,456.87,75.69,8.97">Curriculum learning attempts to speed up learning by training on a set of progressively harder environments <ref type="bibr" coords="10,361.74,456.87,15.27,8.97" target="#b10">[11]</ref>.</s><s coords="10,383.59,456.87,172.61,8.97;10,305.14,468.83,251.06,8.97;10,305.14,480.79,251.07,8.97;10,305.14,492.74,251.06,8.97;10,305.14,504.69,176.16,8.97">The second-place entry in the limited track used a modified DRQN network architecture with an additional stream of fully connected layers to learn supervised auxiliary tasks such as enemy detection, with the purpose of speeding up the training of the convolutional layers <ref type="bibr" coords="10,462.21,504.69,15.27,8.97" target="#b78">[79]</ref>.</s><s coords="10,483.87,504.69,72.28,8.97;10,305.14,516.65,251.05,8.97;10,305.14,528.60,251.04,8.97;10,305.14,540.56,56.73,8.97">Position inference and object mapping from pixels and depth buffers using simultaneous localization and mapping (SLAM) also improve DQN in Doom <ref type="bibr" coords="10,342.78,540.56,15.27,8.97" target="#b13">[14]</ref>.</s></p><p><s coords="10,315.10,552.52,241.08,8.97;10,305.14,564.47,251.07,8.97;10,305.14,576.42,130.04,8.97">The winner of the full deathmatch competition implemented a direct future prediction (DFP) approach that was shown to outperform DQN and A3C <ref type="bibr" coords="10,416.09,576.42,15.27,8.97" target="#b27">[28]</ref>.</s><s coords="10,438.03,576.42,118.15,8.97;10,305.14,588.38,251.07,8.97;10,305.14,600.34,251.04,8.97;10,305.14,612.29,251.05,8.97;10,305.14,624.25,148.69,8.97">The architecture used in DFP has three streams: one for the screen pixels, one for lower dimensional measurements describing the agent's current state, and one for describing the agent's goal, which is a linear combination of prioritized measurements.</s><s coords="10,456.10,624.25,100.08,8.97;10,305.14,636.20,251.05,8.97;10,305.14,648.15,251.07,8.97;10,305.14,660.11,101.92,8.97">DFP collects experiences in a memory and is trained with supervised learning techniques to predict the future measurements based on the current state, goal, and selected action.</s><s coords="10,410.05,660.11,146.12,8.97;10,305.14,672.07,251.04,8.97">During training, actions are selected that yield the best-predicted outcome, based on the current goal.</s><s coords="10,305.14,684.03,251.06,8.97;10,305.14,695.98,99.89,8.97">This method can be trained on various goals and generalizes to unseen goals at test time.</s></p><p><s coords="11,47.87,66.09,241.10,8.97;11,37.91,78.04,239.94,8.97">Navigation in 3-D environments is one of the important skills required for FPS games and has been studied extensively.</s><s coords="11,281.77,78.04,7.19,8.97;11,37.91,90.01,251.05,8.97;11,37.91,101.96,251.03,8.97;11,37.91,113.91,250.93,8.97">A CNN long short-term memory (LSTM) network was trained with A3C extended with additional outputs predicting the pixel depths and loop closure, showing significant improvements <ref type="bibr" coords="11,269.76,113.91,15.27,8.97" target="#b94">[95]</ref>.</s></p><p><s coords="11,47.87,125.87,241.08,8.97;11,37.91,137.82,251.05,8.97;11,37.91,149.78,251.07,8.97;11,37.91,161.74,21.86,8.97">The UNREAL algorithm, based on A3C, implements an auxiliary task that trains the network to predict the immediate subsequent future reward from a sequence of consecutive observations.</s><s coords="11,63.01,161.74,225.95,8.97;11,37.91,173.69,251.00,8.97;11,37.91,185.65,203.23,8.97">UNREAL was tested on fruit gathering and exploration tasks in OpenArena and achieved a mean human-normalized score of 87%, where A3C only achieved 53% <ref type="bibr" coords="11,222.06,185.65,15.26,8.97" target="#b62">[63]</ref>.</s></p><p><s coords="11,47.87,197.60,241.11,8.97;11,37.91,209.55,251.06,8.97;11,37.91,221.52,70.60,8.97">The ability to transfer knowledge to new environments can reduce the learning time and can in some cases be crucial for some challenging tasks.</s><s coords="11,110.21,221.52,178.75,8.97;11,37.91,233.47,251.04,8.97;11,37.91,245.42,168.56,8.97">Transfer learning can be achieved by pretraining a network in similar environments with simpler tasks or by using random textures during training <ref type="bibr" coords="11,187.38,245.42,15.27,8.97" target="#b19">[20]</ref>.</s><s coords="11,208.28,245.42,80.69,8.97;11,37.91,257.38,251.04,8.97;11,37.91,269.33,251.07,8.97">The distill and transfer learning (Distral) method trains several worker policies (one for each task) concurrently and shares a distilled policy <ref type="bibr" coords="11,264.90,269.33,20.06,8.97" target="#b148">[149]</ref>.</s><s coords="11,37.91,281.28,251.05,8.97;11,37.91,293.25,221.81,8.97">The worker policies are regularized to stay close to the shared policy, which will be the centroid of the worker policies.</s><s coords="11,261.85,293.25,27.11,8.97;11,37.91,305.20,122.62,8.97">Distral was applied to DeepMind Lab.</s></p><p><s coords="11,47.87,317.15,241.08,8.97;11,37.91,329.11,251.06,8.97;11,37.91,341.06,251.08,8.97">The intrinsic curiosity module, consisting of several neural networks, computes an intrinsic reward each time step based on the agent's inability to predict the outcome of taking actions.</s><s coords="11,37.91,353.02,251.05,8.97;11,37.91,364.98,208.42,8.97">It was shown to learn to navigate in complex Doom and Super Mario levels only relying on intrinsic rewards <ref type="bibr" coords="11,222.26,364.98,20.05,8.97" target="#b109">[110]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="11,37.91,389.91,93.08,8.97">E. Open-World Games</head><p><s coords="11,47.87,406.84,241.08,8.97;11,37.91,418.80,251.05,8.97;11,37.91,430.75,251.05,8.97;11,37.91,442.71,251.05,8.97;11,37.91,454.66,90.09,8.97">The hierarchical deep reinforcement learning network (H-DRLN) architecture implements a lifelong learning framework, which is shown to be able to transfer knowledge between simple tasks in Minecraft such as navigation, item collection, and placement tasks <ref type="bibr" coords="11,103.94,454.66,20.05,8.97" target="#b149">[150]</ref>.</s><s coords="11,130.59,454.66,158.36,8.97;11,37.91,466.62,251.01,8.97;11,37.91,478.58,86.23,8.97">The H-DRLN uses a variation of policy distillation <ref type="bibr" coords="11,83.76,466.62,21.58,8.97" target="#b118">[119]</ref> to retain and encapsulate learned knowledge into a single network.</s></p><p><s coords="11,47.87,490.53,241.10,8.97;11,37.91,502.49,251.04,8.97;11,37.91,514.44,251.02,8.97;11,37.91,526.39,85.43,8.97">Neural turing machines (NTMs) are fully differentiable neural networks coupled with an external memory resource, which can learn to solve simple algorithmic problems such as copying and sorting <ref type="bibr" coords="11,104.25,526.39,15.27,8.97" target="#b39">[40]</ref>.</s><s coords="11,127.20,526.39,161.77,8.97;11,37.91,538.35,251.07,8.97;11,37.91,550.31,251.08,8.97;11,37.91,562.26,251.06,8.97;11,37.91,574.22,68.61,8.97">Two memory-based variations, inspired by NTM, called recurrent memory Q-network (RMQN) and feedback recurrent memory Q-network (FRMQN), were able to solve complex navigation tasks that require memory and active perception <ref type="bibr" coords="11,82.45,574.22,20.05,8.97" target="#b101">[102]</ref>.</s></p><p><s coords="11,47.87,586.17,241.08,8.97;11,37.91,598.12,251.07,8.97;11,37.91,610.08,251.03,8.97;11,37.91,622.04,83.47,8.97">The teacher-student curriculum learning (TSCL) framework incorporates a teacher that prioritizes tasks, wherein the student's performance is either increasing (learning) or decreasing (forgetting) <ref type="bibr" coords="11,102.29,622.04,15.26,8.97" target="#b91">[92]</ref>.</s><s coords="11,124.34,622.04,164.61,8.97;11,37.91,633.99,251.06,8.97;11,37.91,645.95,120.65,8.97">TSCL enabled a policy gradient learning method to solve mazes that were otherwise not possible with a uniform sampling of subtasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="11,37.91,670.88,59.47,8.97">F. RTS Games</head><p><s coords="11,47.87,687.82,241.08,8.97;11,37.91,699.77,251.06,8.97;11,37.91,711.73,73.03,8.97">The previous sections described methods that learn to play games end-to-end, i.e., a neural network is trained to map states directly to actions.</s><s coords="11,113.02,711.73,175.96,8.97;11,37.91,723.68,251.06,8.97;11,37.91,735.64,251.05,8.97;11,300.92,66.10,19.64,8.97">RTS games, however, offer much more complex environments, in which players have to control multiple agents simultaneously in real time on a partially observable map.</s><s coords="11,324.18,66.10,227.78,8.97;11,300.92,78.05,212.39,8.97">Additionally, RTS games have no in-game scoring, and thus, the reward is determined by who wins the game.</s><s coords="11,515.59,78.05,36.40,8.97;11,300.92,90.01,251.05,8.97;11,300.92,101.97,251.02,8.97;11,300.92,113.92,76.83,8.97">For these reasons, learning to play RTS games end-to-end may be infeasible for the foreseeable future, and instead, subproblems have been studied so far.</s></p><p><s coords="11,310.89,125.21,241.09,9.63;11,300.92,137.83,251.03,8.97;11,300.92,149.78,251.06,8.97;11,300.92,161.75,42.18,8.97">For the simplistic RTS platform, μRTS, a CNN was trained as a state evaluator using supervised learning on a generated dataset and used in combination with Monte Carlo tree search <ref type="bibr" coords="11,300.92,161.75,10.58,8.97" target="#b3">[4]</ref>, <ref type="bibr" coords="11,319.03,161.75,20.05,8.97" target="#b135">[136]</ref>.</s><s coords="11,347.11,161.75,204.89,8.97;11,300.92,173.70,116.28,8.97">This approach performed significantly better than previous evaluation methods.</s></p><p><s coords="11,310.89,185.66,241.08,8.97;11,300.92,197.61,180.12,8.97">StarCraft has been a popular game platform for AI research, but so far only with a few DL approaches.</s><s coords="11,485.16,197.61,66.82,8.97;11,300.92,209.56,251.06,8.97;11,300.92,221.52,251.04,8.97">DL methods for StarCraft have focused on micromanagement (unit control) or build-order planning and has ignored other aspects of the game.</s><s coords="11,300.92,233.48,251.04,8.97;11,300.92,245.43,251.05,8.97;11,300.92,257.39,251.07,8.97;11,300.92,269.34,75.12,8.97">The problem of delayed rewards in StarCraft can be circumvented in combat scenarios; here, rewards can be shaped as the difference between damage inflicted and damage incurred <ref type="bibr" coords="11,532.91,257.39,15.27,8.97" target="#b31">[32]</ref>, <ref type="bibr" coords="11,300.92,269.34,15.26,8.97" target="#b32">[33]</ref>, <ref type="bibr" coords="11,323.95,269.34,20.05,8.97" target="#b110">[111]</ref>, <ref type="bibr" coords="11,351.98,269.34,20.05,8.97" target="#b153">[154]</ref>.</s><s coords="11,379.99,269.34,171.96,8.97;11,300.92,281.29,251.03,8.97">States and actions are often described locally relative to units, which is extracted from the game engine.</s><s coords="11,300.92,293.25,251.07,8.97;11,300.92,305.21,251.03,8.97;11,300.92,317.16,181.90,8.97">If agents are trained individually, it is difficult to know which agents contributed to the global reward <ref type="bibr" coords="11,460.15,305.21,15.26,8.97" target="#b18">[19]</ref>, a problem known as the multiagent credit assignment problem.</s><s coords="11,485.96,317.16,66.02,8.97;11,300.92,329.12,251.07,8.97;11,300.92,341.07,251.03,8.97;11,300.92,353.02,178.86,8.97">One approach is to train a generic network, which controls each unit separately and search in policy space using zero-order optimization based on the reward accrued in each episode <ref type="bibr" coords="11,455.73,353.02,20.05,8.97" target="#b153">[154]</ref>.</s><s coords="11,482.35,353.02,69.61,8.97;11,300.92,364.99,239.60,8.97">This strategy was able to learn successful policies for armies of up to 15 units.</s><s coords="11,310.89,376.94,241.07,8.97;11,300.92,388.89,251.05,8.97;11,300.92,400.85,214.36,8.97">Independent Q-learning (IQL) simplifies the multiagent RL problem by controlling units individually while treating other agents as if they were part of the environment <ref type="bibr" coords="11,491.23,400.85,20.05,8.97" target="#b146">[147]</ref>.</s><s coords="11,518.42,400.85,33.56,8.97;11,300.92,412.80,251.03,8.97">This enables Q-learning to scale well to a large number of agents.</s><s coords="11,300.92,424.76,251.06,8.97;11,300.92,436.72,251.05,8.97;11,300.92,448.67,157.11,8.97">However, when combining IQL with recent techniques such as experience replay, agents tend to optimize their policies based on experiences with obsolete policies.</s><s coords="11,462.01,448.67,89.97,8.97;11,300.92,460.63,251.03,8.97;11,300.92,472.58,251.05,8.97;11,300.92,484.53,251.07,8.97;11,300.92,496.49,90.47,8.97">This problem is overcome by applying fingerprints to experiences and by applying an importance-weighted loss function that naturally decays obsolete data, which has shown improvements for some small combat scenarios <ref type="bibr" coords="11,372.31,496.49,15.27,8.97" target="#b32">[33]</ref>.</s></p><p><s coords="11,310.89,508.45,241.11,8.97;11,300.92,520.40,251.06,8.97;11,300.92,532.36,251.04,8.97;11,300.92,544.31,145.18,8.97">The multiagent bidirectionally-coordinated network (BiC-Net) implements a vectorized actor-critic framework based on a bidirectional RNN, with one dimension for every agent, and outputs a sequence of actions <ref type="bibr" coords="11,422.05,544.31,20.05,8.97" target="#b110">[111]</ref>.</s><s coords="11,448.97,544.31,102.98,8.97;11,300.92,556.26,251.05,8.97;11,300.92,568.23,136.44,8.97">This network architecture is unique to the other approaches, as it can handle an arbitrary number of units of different types.</s></p><p><s coords="11,310.89,580.18,241.08,8.97;11,300.92,592.13,251.04,8.97;11,300.92,604.09,251.04,8.97;11,300.92,616.04,251.06,8.97;11,300.92,628.00,19.08,8.97">Counterfactual multiagent (COMA) policy gradients is an actor-critic method with a centralized critic and decentralized actors that address the multiagent credit assignment problem with a counterfactual baseline computed by the critic network <ref type="bibr" coords="11,300.92,628.00,15.27,8.97" target="#b31">[32]</ref>.</s><s coords="11,322.63,628.00,229.33,8.97;11,300.92,639.96,251.04,8.97;11,300.92,651.91,18.54,8.97">COMA achieves state-of-the-art results, for decentralized methods, in small combat scenarios with up to ten units on each side.</s></p><p><s coords="11,310.89,663.87,241.09,8.97;11,300.92,675.82,251.07,8.97;11,300.92,687.77,81.23,8.97">DL has also been applied to build-order planning in StarCraft using macro-based supervised learning approach to imitate human strategies <ref type="bibr" coords="11,363.07,687.77,15.27,8.97" target="#b67">[68]</ref>.</s><s coords="11,385.92,687.77,166.07,8.97;11,300.92,699.73,251.03,8.97;11,300.92,711.69,162.57,8.97">The trained network was integrated as a module used in an existing bot capable of playing the full game with an otherwise hand-crafted behavior.</s><s coords="11,465.97,711.69,85.98,8.97;11,300.92,723.62,251.03,8.97;11,300.92,735.58,251.05,8.97;12,42.12,66.09,251.05,8.97;12,42.12,78.04,251.06,8.97;12,42.12,90.01,24.06,8.97">Another macro-based approach, here using RL instead of SL, called convolutional neural-network-fitted Q-learning (CNNFQ), was trained with double DQN for build-order planning in StarCraft II and was able to win against medium-level scripted bots on small maps <ref type="bibr" coords="12,42.12,90.01,20.05,8.97" target="#b147">[148]</ref>.</s><s coords="12,70.63,90.01,222.55,8.97;12,42.12,101.96,251.05,8.97;12,42.12,113.91,231.30,8.97">A macro action-based RL method that uses PPO for build-order planning and high-level attack planning was able to outperform the built-in bot in StarCraft II at level <ref type="bibr" coords="12,237.36,113.91,32.05,8.97">10 [141]</ref>.</s><s coords="12,275.47,113.91,17.71,8.97;12,42.12,125.87,251.04,8.97;12,42.12,137.82,217.52,8.97">This is particularly impressive as the level-10 bot cheats by having full vision of the map and faster resource harvesting.</s><s coords="12,263.13,137.82,30.03,8.97;12,42.12,149.78,251.04,8.97;12,42.12,161.74,231.11,8.97">The results were obtained using 1920 parallel actors on 3840 CPUs across 80 machines and only for one matchup on one map.</s><s coords="12,275.46,161.74,17.70,8.97;12,42.12,173.69,251.06,8.97;12,42.12,185.65,199.39,8.97">This system won a few games against platinum-level human players but lost all games against diamond-level players.</s><s coords="12,244.93,185.65,48.24,8.97;12,42.12,197.60,251.06,8.97;12,42.12,209.55,168.80,8.97">The authors report that the learned policy "lacks strategy diversity in order to consistently beat human players" <ref type="bibr" coords="12,186.85,209.55,20.05,8.97" target="#b140">[141]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,42.12,236.47,94.48,8.97">G. Team Sports Games</head><p><s coords="12,52.08,253.41,198.26,8.97">DDPG was applied to RoboCup 2-D HFO <ref type="bibr" coords="12,231.26,253.41,15.26,8.97" target="#b50">[51]</ref>.</s><s coords="12,254.05,253.41,39.11,8.97;12,42.12,265.36,251.06,8.97;12,42.12,277.32,251.02,8.97;12,42.12,289.28,251.07,8.97;12,42.12,301.23,41.22,8.97">The actor network used two output streams: one for the selection of discrete action types (dash, turn, tackle, and kick) and one for each action type's 1-2 continuously valued parameters (power and direction).</s><s coords="12,87.00,301.23,206.17,8.97;12,42.12,313.18,251.06,8.97;12,42.12,325.14,213.18,8.97">The inverting gradients bounding approach downscales the gradients as the output approaches its boundaries and inverts the gradients if the parameter exceeds them.</s><s coords="12,259.02,325.14,34.16,8.97;12,42.12,337.09,251.07,8.97;12,42.12,349.05,64.67,8.97">This approach outperformed both SARSA and the best agent in the 2012 RoboCup.</s><s coords="12,110.90,349.05,182.26,8.97;12,42.12,361.01,251.04,8.97;12,42.12,372.96,251.07,8.97;12,42.12,384.92,99.32,8.97">DDPG was also applied to HFO by mixing on-policy updates with one-step Q-learning updates <ref type="bibr" coords="12,258.33,361.01,16.59,8.97" target="#b52">[53]</ref> and outperformed a hand-coded agent with expert knowledge with one player on each team.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,42.12,411.83,75.74,8.97">H. Physics Games</head><p><s coords="12,52.08,428.77,241.10,8.97;12,42.12,440.72,251.06,8.97;12,42.12,452.68,133.92,8.97">As video games are usually a reflection or simplification of the real world, it can be fruitful to learn an intuition about the physical laws in an environment.</s><s coords="12,179.32,452.68,113.87,8.97;12,42.12,464.63,251.04,8.97;12,42.12,476.59,251.05,8.97;12,42.12,488.55,102.06,8.97">A predictive neural network using an object-centered approach (also called fixations) learned to run simulations of a billiards game after being trained on random interactions <ref type="bibr" coords="12,125.10,488.55,15.26,8.97" target="#b35">[36]</ref>.</s><s coords="12,147.23,488.55,145.94,8.97;12,42.12,500.50,151.83,8.97">This predictive model could then be used for planning actions in the game.</s></p><p><s coords="12,52.08,512.45,241.08,8.97;12,42.12,524.41,251.04,8.97;12,42.12,536.37,251.07,8.97;12,42.12,548.33,251.04,8.97;12,42.12,560.28,222.56,8.97">A similar predictive approach was tested in a 3-D game-like environment, using the Unreal Engine, where ResNet-34 <ref type="bibr" coords="12,276.57,524.41,16.59,8.97" target="#b54">[55]</ref> (a deep residual network used for image classification) was extended and trained to predict the visual outcome of blocks that were stacked such that they would usually fall <ref type="bibr" coords="12,245.60,560.28,15.27,8.97" target="#b85">[86]</ref>.</s><s coords="12,267.17,560.28,26.01,8.97;12,42.12,572.23,251.06,8.97;12,42.12,584.19,204.19,8.97">Residual networks implement shortcut connections that skip layers, which can improve learning in very deep networks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,42.12,611.10,100.53,8.97">I. Text Adventure Games</head><p><s coords="12,52.08,628.04,241.10,8.97;12,42.12,639.99,207.93,8.97">Text adventure games, in which both states and actions are presented as text only, are a special video game genre.</s><s coords="12,251.68,639.99,41.35,8.97;12,42.12,651.95,251.03,8.97;12,42.12,663.91,251.03,8.97;12,42.12,675.86,251.01,8.97;12,42.12,687.82,251.05,8.97">A network architecture called LSTM-DQN <ref type="bibr" coords="12,169.89,651.95,21.57,8.97" target="#b100">[101]</ref> was designed specifically to play these games and is implemented using LSTM networks that convert text from the world state into a vector representation, which estimates Q-values for all possible state-action pairs.</s><s coords="12,42.12,699.77,251.04,8.97;12,42.12,711.73,238.56,8.97">LSTM-DQN was able to complete between 96% and 100% of the quests on average in two different text adventure games.</s></p><p><s coords="12,52.08,723.68,241.08,8.97;12,42.12,735.64,251.05,8.97;12,42.12,747.60,131.68,8.97">To be able to improve on these results, researchers have moved toward learning language models and word embeddings to augment the neural network.</s><s coords="12,178.20,747.60,114.94,8.97;12,305.13,66.10,251.03,8.97;12,305.13,78.05,114.13,8.97">An approach that combines RL with explicit language understanding is deep reinforcement relevance net (DRRN) <ref type="bibr" coords="12,400.17,78.05,15.27,8.97" target="#b53">[54]</ref>.</s><s coords="12,422.90,78.05,133.28,8.97;12,305.13,90.01,113.70,8.97">This approach has two networks that learn word embeddings.</s><s coords="12,421.47,90.01,134.69,8.97;12,305.13,101.97,172.82,8.97">One embeds the state description, and the other embeds the action description.</s><s coords="12,479.98,101.97,76.20,8.97;12,305.13,113.92,251.04,8.97;12,305.13,125.88,251.05,8.97;12,305.13,137.83,40.10,8.97">Relevance between the two embedding vectors is calculated with an interaction function such as the inner product of the vectors or a bilinear operation.</s><s coords="12,348.68,137.83,207.51,8.97;12,305.13,149.78,230.92,8.97">The relevance is then used as the Q-value, and the whole process is trained end-to-end with deep Q-learning.</s><s coords="12,538.50,149.78,17.71,8.97;12,305.13,161.75,251.04,8.97;12,305.13,173.70,251.03,8.97;12,305.13,185.66,27.88,8.97">This approach allows the network to generalize to phrases not seen during training, which is an improvement for very large text games.</s><s coords="12,335.42,185.66,220.78,8.97;12,305.13,197.61,196.38,8.97">The approach was tested on the text games Saving John and Machine of Death, both choice-based games.</s></p><p><s coords="12,315.10,209.56,241.02,8.97;12,305.13,221.52,240.65,8.97">Taking language modeling further, Fulda et al. explicitly modeled language affordances to assist in action selection <ref type="bibr" coords="12,526.69,221.52,15.27,8.97" target="#b36">[37]</ref>.</s><s coords="12,549.02,221.52,7.19,8.97;12,305.13,233.48,251.06,8.97;12,305.13,245.43,251.04,8.97;12,305.13,256.72,251.06,9.63;12,305.13,268.67,251.06,9.63">A word embedding is first learned from a Wikipedia Corpus via unsupervised learning <ref type="bibr" coords="12,398.25,245.43,15.26,8.97" target="#b93">[94]</ref>, and this embedding is then used to calculate analogies such as song is to sing as bike is to x, where x can then be calculated in the embedding space <ref type="bibr" coords="12,537.11,269.34,15.26,8.97" target="#b93">[94]</ref>.</s><s coords="12,305.13,281.29,251.02,8.97;12,305.13,293.25,129.26,8.97">The authors build a dictionary of verbs, noun pairs, and another one of object manipulation pairs.</s><s coords="12,436.34,293.25,119.82,8.97;12,305.13,305.21,251.04,8.97;12,305.13,317.16,17.98,8.97">Using the learned affordances, the model can suggest a small set of actions for a state description.</s><s coords="12,326.95,317.16,229.21,8.97;12,305.13,329.12,73.54,8.97">Policies were learned with Q-learning and tested on 50 Z-machine games.</s></p><p><s coords="12,315.10,341.07,241.09,8.97;12,305.13,353.02,251.04,8.97;12,305.13,364.99,24.61,8.97">The Golovin Agent focuses exclusively on language models <ref type="bibr" coords="12,305.13,353.02,16.59,8.97" target="#b74">[75]</ref> that are pretrained from a corpus of books in the fantasy genre.</s><s coords="12,331.63,364.99,224.55,8.97;12,305.13,376.94,78.02,8.97">Using word embeddings, the agent can replace synonyms with known words.</s><s coords="12,386.53,376.94,169.64,8.97;12,305.13,388.89,211.13,8.97">Golovin is built of five command generators: general, movement, battle, gather, and inventory.</s><s coords="12,518.26,388.89,37.94,8.97;12,305.13,400.85,251.03,8.97;12,305.13,412.80,251.04,8.97;12,305.13,424.76,62.82,8.97">These are generated by analyzing the state description, using the language models to calculate and sample from a number of features for each command.</s><s coords="12,370.77,424.76,185.44,8.97;12,305.13,436.72,91.85,8.97">Golovin uses no RL and scores comparable to the affordance method.</s></p><p><s coords="12,315.10,448.67,241.08,8.97;12,305.13,460.63,58.20,8.97">Most recently, Zahavy et al. have proposed another DQN method <ref type="bibr" coords="12,339.27,460.63,20.05,8.97" target="#b172">[173]</ref>.</s><s coords="12,367.59,460.63,188.58,8.97;12,305.13,472.58,187.62,8.97">This method uses a type of attention mechanism called action elimination network (AEN).</s><s coords="12,495.26,472.58,60.96,8.97;12,305.13,484.53,153.20,8.97">In parser-based games, the action space is very large.</s><s coords="12,461.71,484.53,94.49,8.97;12,305.13,496.49,251.05,8.97;12,305.13,508.45,91.16,8.97">The AEN learns, while playing, to predict which actions that will have no effect for a given state description.</s><s coords="12,398.51,508.45,157.68,8.97;12,305.13,520.40,251.04,8.97;12,305.13,532.36,206.88,8.97">The AEN is then used to eliminate most of the available actions for a given state and after which the remaining actions are evaluated with the Q-network.</s><s coords="12,514.19,532.36,42.01,8.97;12,305.13,544.31,251.03,8.97;12,305.13,556.26,203.76,8.97">The whole process is trained end-to-end and achieves similar performance to DQN with a manually constrained actions space.</s><s coords="12,511.25,556.26,44.95,8.97;12,305.13,568.23,251.05,8.97;12,305.13,580.18,174.75,8.97">Despite the progress made for text adventure games, current techniques are still far from matching human performance.</s></p><p><s coords="12,315.10,592.12,241.08,8.97;12,305.14,604.08,222.45,8.97">Outside of text adventure games, natural language processing has been used for other text-based games as well.</s><s coords="12,531.27,604.08,24.91,8.97;12,305.14,616.03,251.06,8.97;12,305.14,627.99,251.06,8.97;12,305.14,639.94,251.05,8.97;12,305.14,651.89,77.52,8.97">To facilitate communication, a deep distributed recurrent Q-network (DDRQN) architecture was used to train several agents to learn a communication protocol to solve the multiagent Hats and Switch riddles <ref type="bibr" coords="12,363.58,651.89,15.26,8.97" target="#b33">[34]</ref>.</s><s coords="12,385.12,651.89,171.06,8.97;12,305.14,663.86,251.06,8.97;12,305.14,675.81,251.03,8.97;12,305.14,687.76,100.59,8.97">One of the novel modifications in DDRQN is that agents use shared network weights that are conditioned on their unique ID, which enables faster learning while retaining diversity between agents.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="12,341.11,718.69,179.11,8.97">V. HISTORICAL OVERVIEW OF DL IN GAMES</head><p><s coords="12,315.10,735.63,241.10,8.97;12,305.14,747.59,109.29,8.97">The previous section discussed DL methods in games according to the game type.</s><s coords="12,418.87,747.59,137.31,8.97;13,37.91,66.09,251.04,8.97;13,37.91,78.04,251.05,8.97;13,37.91,90.01,143.05,8.97">This section instead looks at the development of these methods in terms of how they influenced each other, giving a historical overview of the DL methods that are reviewed in the previous section.</s><s coords="13,183.01,90.01,105.94,8.97;13,37.91,101.96,251.06,8.97;13,37.91,113.91,251.08,8.97;13,37.91,125.87,128.32,8.97">Many of these methods are inspired from or directly build upon previous methods, while some are applied to different game genres and others are tailored to specific types of games.</s></p><p><s coords="13,47.87,137.82,241.09,8.97;13,37.91,149.78,251.03,8.97;13,37.91,161.74,183.09,8.97">Fig. <ref type="figure" coords="13,66.75,137.82,4.98,8.97" target="#fig_1">4</ref> shows an influence diagram with the reviewed methods and their relations to earlier methods (the current section can be read as a long caption to that figure).</s><s coords="13,224.47,161.74,64.48,8.97;13,37.91,173.69,208.34,8.97">Each method in the diagram is colored to show the game benchmark.</s><s coords="13,248.53,173.69,40.42,8.97;13,37.91,185.65,251.05,8.97;13,37.91,197.60,251.06,8.97;13,37.91,209.55,96.50,8.97">DQN <ref type="bibr" coords="13,272.36,173.69,16.59,8.97" target="#b96">[97]</ref> was very influential as an algorithm that uses gradient-based DL for pixel-based video game playing and was originally applied to the Atari benchmark.</s><s coords="13,137.56,209.55,151.39,8.97;13,37.91,221.52,251.05,8.97;13,37.91,233.47,60.17,8.97">Note that earlier approaches exist but with less success such as <ref type="bibr" coords="13,146.45,221.52,20.05,8.97" target="#b108">[109]</ref>, and successful gradient-free methods <ref type="bibr" coords="13,74.02,233.47,20.05,8.97" target="#b114">[115]</ref>.</s><s coords="13,100.44,233.47,188.54,8.97;13,37.91,245.42,251.05,8.97;13,37.91,257.38,21.86,8.97">Double DQN <ref type="bibr" coords="13,156.05,233.47,21.57,8.97" target="#b154">[155]</ref> and dueling DQN <ref type="bibr" coords="13,252.88,233.47,21.57,8.97" target="#b160">[161]</ref> are early extensions that use multiple networks to improve estimations.</s><s coords="13,62.96,257.38,180.76,8.97">DRQN <ref type="bibr" coords="13,94.37,257.38,16.60,8.97" target="#b50">[51]</ref> uses an RNN as the Q-network.</s><s coords="13,246.92,257.38,42.05,8.97;13,37.91,269.33,251.04,8.97;13,37.91,281.28,103.81,8.97">Prioritized DQN <ref type="bibr" coords="13,61.71,269.33,21.57,8.97" target="#b122">[123]</ref> is another early extension, and it adds improved experience replay sampling.</s><s coords="13,144.81,281.28,144.13,8.97;13,37.91,293.25,251.06,8.97">Bootstrapped DQN <ref type="bibr" coords="13,225.68,281.28,21.57,8.97" target="#b105">[106]</ref> builds off of double DQN with a different improved sampling strategy.</s><s coords="13,37.91,305.20,251.07,8.97;13,37.91,317.15,251.04,8.97;13,37.91,329.11,251.04,8.97;13,37.91,341.06,251.05,8.97;13,37.91,353.02,251.05,8.97;13,37.91,364.98,99.33,8.97">Further DQN enhancements used for Atari include: the C51 algorithm <ref type="bibr" coords="13,71.29,317.15,10.58,8.97" target="#b8">[9]</ref>, which is based on DQN but changes the Q function; Noisy-Nets which make the networks stochastic to aid with exploration <ref type="bibr" coords="13,77.69,341.06,15.49,8.97" target="#b34">[35]</ref>; DQfD which also learns from examples <ref type="bibr" coords="13,269.60,341.06,15.49,8.97" target="#b56">[57]</ref>; and Rainbow, which combines many of these state-of-the-art techniques together <ref type="bibr" coords="13,118.15,364.98,15.26,8.97" target="#b55">[56]</ref>.</s></p><p><s coords="13,47.87,376.93,241.09,8.97;13,37.91,388.88,251.04,8.97;13,37.91,400.84,181.32,8.97">Gorila was the first asynchronous method based on DQN <ref type="bibr" coords="13,37.91,388.88,21.57,8.97" target="#b99">[100]</ref> and was followed by A3C <ref type="bibr" coords="13,165.43,388.88,15.26,8.97" target="#b95">[96]</ref>, which uses multiple asynchronous agents for an actor-critic approach.</s><s coords="13,222.43,400.84,66.54,8.97;13,37.91,412.79,251.07,8.97;13,37.91,424.76,251.07,8.97;13,37.91,436.71,95.15,8.97">This was further extended at the end of 2016 with UNREAL <ref type="bibr" coords="13,226.35,412.79,15.27,8.97" target="#b62">[63]</ref>, which incorporates work done with auxiliary learning to handle sparse feedback environments.</s><s coords="13,135.98,436.71,152.96,8.97;13,37.91,448.66,211.11,8.97">Since then, there has been a lot of additional extensions on A3C <ref type="bibr" coords="13,149.72,448.66,15.27,8.97" target="#b34">[35]</ref>, <ref type="bibr" coords="13,171.47,448.66,20.05,8.97" target="#b119">[120]</ref>, <ref type="bibr" coords="13,198.21,448.66,20.05,8.97" target="#b159">[160]</ref>, <ref type="bibr" coords="13,224.95,448.66,20.05,8.97" target="#b165">[166]</ref>.</s><s coords="13,251.69,448.66,37.26,8.97;13,37.91,460.62,251.04,8.97;13,37.91,472.57,145.42,8.97">IMPALA has taken it further with focusing on a single trained agent that can play all of the Atari games <ref type="bibr" coords="13,164.25,472.57,15.26,8.97" target="#b29">[30]</ref>.</s><s coords="13,186.05,472.57,102.91,8.97;13,37.91,484.52,250.92,8.97;13,37.91,496.49,75.24,8.97">In 2018, the move toward large-scale distributed learning has continued and advanced with Ape-X <ref type="bibr" coords="13,67.51,496.49,15.27,8.97" target="#b60">[61]</ref>, <ref type="bibr" coords="13,89.08,496.49,20.05,8.97" target="#b111">[112]</ref>.</s></p><p><s coords="13,47.87,508.44,241.09,8.97;13,37.91,520.39,52.33,8.97">Evolutionary techniques are also seeing a Renaissance for video games.</s><s coords="13,92.55,520.39,196.40,8.97;13,37.91,532.35,102.57,8.97">First, Salimans et al. showed that ESs could compete with deep RL <ref type="bibr" coords="13,116.42,532.35,20.05,8.97" target="#b120">[121]</ref>.</s><s coords="13,143.65,532.35,145.33,8.97;13,37.91,544.30,251.03,8.97;13,37.91,556.26,251.05,8.97;13,37.91,568.22,19.08,8.97">Then, two more papers came out of Uber AI: one showing that derivative-free evolutionary algorithms can compete with deep RL <ref type="bibr" coords="13,173.01,556.26,20.05,8.97" target="#b138">[139]</ref>, and an extension to ES <ref type="bibr" coords="13,37.91,568.22,15.27,8.97" target="#b23">[24]</ref>.</s><s coords="13,59.61,568.22,229.35,8.97;13,37.91,580.17,124.09,8.97">These benefit from easy parallelization and possibly have some advantage in exploration.</s></p><p><s coords="13,47.87,592.12,241.09,8.97;13,37.91,604.08,105.63,8.97">Another approach used on Atari around the time DQN was introduced is TRPO <ref type="bibr" coords="13,124.45,604.08,15.27,8.97" target="#b76">[77]</ref>.</s><s coords="13,147.48,604.08,141.46,8.97;13,37.91,616.03,150.59,8.97">This updates a surrogate objective that is updated from the environment.</s><s coords="13,191.22,616.03,97.75,8.97;13,37.91,628.00,251.06,8.97;13,37.91,639.95,225.26,8.97">Later, in 2017, PPO was introduced as a more robust and simpler surrogate optimization scheme that also draws from innovations in A3C <ref type="bibr" coords="13,239.11,639.95,20.05,8.97" target="#b128">[129]</ref>.</s><s coords="13,266.26,639.95,22.69,8.97;13,37.91,651.90,251.06,8.97;13,37.91,663.86,251.06,8.97;13,37.91,675.81,187.03,8.97">Some extensions are specifically for Montezuma's revenge, which is a game within the ALE benchmark, but it is particularly difficult due to sparse rewards and hidden information.</s><s coords="13,227.90,675.81,61.07,8.97;13,37.91,687.76,251.06,8.97;13,37.91,699.73,194.65,8.97">The algorithms that do best on Montezuma do so by extending DQN with intrinsic motivation <ref type="bibr" coords="13,97.03,699.73,11.62,8.97" target="#b7">[8]</ref> and hierarchical learning <ref type="bibr" coords="13,213.47,699.73,15.27,8.97" target="#b76">[77]</ref>.</s><s coords="13,235.25,699.73,53.71,8.97;13,37.91,711.68,251.05,8.97;13,37.91,723.63,251.06,8.97;13,37.91,735.59,80.40,8.97">Ms. Pac-Man was also singled out from Atari, where the reward function was learned in separate parts to make the agent more robust to new environments <ref type="bibr" coords="13,94.24,735.59,20.05,8.97" target="#b155">[156]</ref>.</s><s coords="13,310.89,66.10,206.93,8.97">Doom is another benchmark that is new as of 2016.</s><s coords="13,520.53,66.10,31.48,8.97;13,300.92,78.05,251.05,8.97;13,300.92,90.01,118.68,8.97">Most of the work for this game has been extending methods designed for Atari to handle richer data.</s><s coords="13,421.61,90.01,130.34,8.97;13,300.92,101.97,185.88,8.97">A3C + curriculum learning <ref type="bibr" coords="13,530.38,90.01,21.57,8.97" target="#b166">[167]</ref> proposes using curriculum learning with A3C.</s><s coords="13,489.51,101.97,62.48,8.97;13,300.92,113.92,251.03,8.97;13,300.92,125.88,62.94,8.97">DRQN + auxiliary learning <ref type="bibr" coords="13,353.98,113.92,16.59,8.97" target="#b78">[79]</ref> extends DRQN by adding additional rewards during training.</s><s coords="13,367.33,125.88,184.66,8.97;13,300.92,137.83,177.28,8.97">DQN + SLAM <ref type="bibr" coords="13,432.59,125.88,16.60,8.97" target="#b13">[14]</ref> combines techniques for mapping unknown environments with DQN.</s></p><p><s coords="13,310.89,149.78,241.10,8.97;13,300.92,161.75,41.22,8.97">DFP <ref type="bibr" coords="13,331.87,149.78,16.60,8.97" target="#b27">[28]</ref> is the only approach that is not extending an Atari technique.</s><s coords="13,345.44,161.75,206.54,8.97;13,300.92,173.70,251.07,8.97;13,300.92,185.66,251.05,8.97;13,300.92,197.61,24.01,8.97">Like UCT To Classification <ref type="bibr" coords="13,461.12,161.75,16.60,8.97" target="#b41">[42]</ref> for Atari, Objectcentric Prediction <ref type="bibr" coords="13,374.82,173.70,16.60,8.97" target="#b35">[36]</ref> for Billiard, and Direct Perception <ref type="bibr" coords="13,535.39,173.70,16.60,8.97" target="#b20">[21]</ref> for Racing, DFP uses supervised learning to learn about the game.</s><s coords="13,328.10,197.61,223.89,8.97;13,300.92,209.56,251.05,8.97;13,300.92,221.52,122.18,8.97">All of these, except UCT To Classification, learn to directly predict some future state of the game and make a prediction from this information.</s><s coords="13,426.86,221.52,125.12,8.97;13,300.92,233.48,140.58,8.97">None of these works, all from different years, refer to each other.</s><s coords="13,444.67,233.48,107.28,8.97;13,300.92,245.43,251.03,8.97;13,300.92,257.39,117.20,8.97">Besides Direct Perception, the only unique work for racing is DDPG <ref type="bibr" coords="13,472.32,245.43,15.27,8.97" target="#b87">[88]</ref>, which extends DQN for continuous controls.</s><s coords="13,420.02,257.39,131.95,8.97;13,300.92,269.34,122.56,8.97">This technique has been extended for RoboCup Soccer <ref type="bibr" coords="13,385.32,269.34,16.59,8.97" target="#b51">[52]</ref>  <ref type="bibr" coords="13,404.41,269.34,15.26,8.97" target="#b52">[53]</ref>.</s></p><p><s coords="13,310.89,281.29,241.09,8.97;13,300.92,293.25,145.56,8.97">Work on StarCraft micromanagement (unit control) is based on Q-learning started in late 2016.</s><s coords="13,450.78,293.25,101.19,8.97;13,300.92,305.21,251.06,8.97;13,300.92,317.16,52.43,8.97">IQL <ref type="bibr" coords="13,471.63,293.25,16.60,8.97" target="#b32">[33]</ref> extends DQN-Prioritized DQN by treating all other agents as part of the environment.</s><s coords="13,357.50,317.16,194.47,8.97;13,300.92,329.12,251.04,8.97">COMA <ref type="bibr" coords="13,391.51,317.16,16.60,8.97" target="#b31">[32]</ref> extends IQL by calculating counterfactual rewards, the marginal contribution each agent added.</s><s coords="13,300.92,341.07,251.05,8.97;13,300.92,353.02,129.48,8.97">BiCNet <ref type="bibr" coords="13,334.32,341.07,21.57,8.97" target="#b110">[111]</ref> and zero-order optimization <ref type="bibr" coords="13,473.95,341.07,21.58,8.97" target="#b153">[154]</ref> are RL based but are not derived from DQN.</s><s coords="13,434.24,353.02,117.75,8.97;13,300.92,364.99,84.22,8.97">Another popular approach is hierarchical learning.</s><s coords="13,387.24,364.99,164.75,8.97;13,300.92,376.94,251.07,8.97;13,300.92,388.89,176.19,8.97">In 2017, it was tried with replay data <ref type="bibr" coords="13,532.90,364.99,15.27,8.97" target="#b67">[68]</ref>, and in 2018, state-of-the-art results were achieved by using it with two different RL methods <ref type="bibr" coords="13,426.49,388.89,20.05,8.97" target="#b140">[141]</ref>, <ref type="bibr" coords="13,453.05,388.89,20.06,8.97" target="#b147">[148]</ref>.</s><s coords="13,310.89,400.85,241.08,8.97;13,300.92,412.80,24.06,8.97">Some work published in 2016 extends DQN to play Minecraft <ref type="bibr" coords="13,300.92,412.80,20.05,8.97" target="#b149">[150]</ref>.</s><s coords="13,327.90,412.80,224.08,8.97;13,300.92,424.76,251.06,8.97;13,300.92,436.72,48.95,8.97">At around the same time, techniques were developed to make DQN context-aware and modular to handle the large state space <ref type="bibr" coords="13,325.81,436.72,20.05,8.97" target="#b101">[102]</ref>.</s><s coords="13,352.64,436.72,199.35,8.97;13,300.92,448.67,91.15,8.97">Recently, curriculum learning has been applied to Minecraft as well <ref type="bibr" coords="13,372.99,448.67,15.27,8.97" target="#b91">[92]</ref>.</s></p><p><s coords="13,310.89,460.63,241.10,8.97">DQN was applied to text adventure games in 2015 <ref type="bibr" coords="13,527.93,460.63,20.05,8.97" target="#b100">[101]</ref>.</s><s coords="13,300.92,472.58,251.05,8.97;13,300.92,484.53,251.03,8.97">Soon after, it was modified to have a language-specific architecture and use the state-action pair relevance as the Q-value <ref type="bibr" coords="13,532.88,484.53,15.26,8.97" target="#b53">[54]</ref>.</s><s coords="13,300.92,496.49,251.04,8.97;13,300.92,508.45,78.38,8.97">Most of the work on these games has been focused on explicit language modeling.</s><s coords="13,381.63,508.45,170.37,8.97;13,300.92,520.40,251.04,8.97;13,300.92,532.36,234.00,8.97">Golovin agent and affordance-based action selection both use neural networks to learn language models, which provide the actions for the agents to play <ref type="bibr" coords="13,494.09,532.36,15.27,8.97" target="#b36">[37]</ref>, <ref type="bibr" coords="13,515.84,532.36,15.27,8.97" target="#b74">[75]</ref>.</s><s coords="13,537.59,532.36,14.38,8.97;13,300.92,544.31,251.08,8.97">Recently, in 2018, DQN was used again paired with an AEN <ref type="bibr" coords="13,527.93,544.31,20.06,8.97" target="#b172">[173]</ref>.</s></p><p><s coords="13,310.89,556.26,241.08,8.97;13,300.92,568.23,251.06,8.97;13,300.92,580.18,193.87,8.97">Combining extensions from previous algorithms have proven to be a promising direction for DL applied to video games, with Atari being the most popular benchmark for RL.</s><s coords="13,497.39,580.18,54.60,8.97;13,300.92,592.13,251.03,8.97;13,300.92,604.09,233.22,8.97">Another clear trend, which is apparent in Table <ref type="table" coords="13,434.57,592.13,6.08,8.97" target="#tab_0">II</ref>, is the focus on parallelization: distributing the work among multiple CPUs and GPUs.</s><s coords="13,535.71,604.09,16.25,8.97;13,300.92,616.04,251.06,8.97;13,300.92,628.00,251.05,8.97;13,300.92,639.96,101.70,8.97">Parallelization is most common with actor-critic methods, such as A2C and A3C, and evolutionary approaches, such as Deep GA <ref type="bibr" coords="13,300.92,639.96,21.57,8.97" target="#b138">[139]</ref> and ESs <ref type="bibr" coords="13,357.71,639.96,15.27,8.97" target="#b23">[24]</ref>, <ref type="bibr" coords="13,378.56,639.96,20.05,8.97" target="#b120">[121]</ref>.</s><s coords="13,404.40,639.96,147.54,8.97;13,300.92,651.91,251.06,8.97;13,300.92,663.87,236.66,8.97">Hierarchical RL, intrinsic motivation, and transfer learning are promising new directions to explore to master currently unsolved problems in video game playing.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,378.05,694.79,96.77,8.98">VI. OPEN CHALLENGES</head><p><s coords="13,310.89,711.73,241.10,8.97;13,300.92,723.68,251.05,8.97;13,300.92,735.63,48.09,8.97">While DL has shown remarkable results in video game playing, a multitude of important open challenges remain, which we review here.</s><s coords="13,350.89,735.63,201.08,8.97;14,42.12,688.60,251.07,8.97;14,42.12,700.56,251.06,8.97;14,42.12,712.52,20.20,8.97">Indeed, looking back at the current state of research from a decade or two in the future, it is likely that we will see the current research as early steps in a broad and important research field.</s><s coords="14,65.51,712.52,227.65,8.97;14,42.12,724.47,251.06,8.97;14,305.13,688.60,251.03,8.97;14,305.13,700.56,174.72,8.97">This section is divided into four broad categories (agent model properties, game industry, learning models of games, and computational resources) with different game-playing challenges that remain open for DL techniques.</s><s coords="14,482.75,700.56,73.43,8.97;14,305.13,712.52,251.06,8.97;14,305.13,724.47,177.97,8.97">We mention a few potential approaches for some of the challenges, while the best way forward for others is currently not clear.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="15,37.91,66.09,109.23,8.97;15,47.87,84.03,131.31,8.97">A. Agent Model Properties 1) General Video Game Playing:</head><p><s coords="15,184.23,84.03,104.68,8.97;15,37.91,95.98,251.03,8.97;15,37.91,107.93,251.06,8.97;15,37.91,119.89,251.06,8.97;15,37.91,131.85,134.46,8.97">Being able to solve a single problem does not make you intelligent; nobody would say that Deep Blue or AlphaGo <ref type="bibr" coords="15,136.75,107.93,21.58,8.97" target="#b132">[133]</ref> possess general intelligence, as they cannot even play Checkers (without retraining), much less make coffee or tie their shoelaces.</s><s coords="15,174.63,131.85,114.36,8.97;15,37.91,143.80,251.04,8.97;15,37.91,155.76,78.53,8.97">To learn generally intelligent behavior, you need to train on not just a single task, but many different tasks <ref type="bibr" coords="15,97.35,155.76,15.27,8.97" target="#b82">[83]</ref>.</s><s coords="15,119.17,155.76,169.79,8.97;15,37.91,167.71,251.03,8.97;15,37.91,179.66,251.05,8.97;15,37.91,191.63,103.48,8.97">Video games have been suggested as ideal environments for learning general intelligence, partly because there are so many video games that share common interface and reward conventions <ref type="bibr" coords="15,117.33,191.63,20.05,8.97" target="#b123">[124]</ref>.</s><s coords="15,143.66,191.63,145.31,8.97;15,37.91,203.58,251.05,8.97;15,37.91,215.53,187.41,8.97">Yet, the vast majority of work on DL in video games focuses on learning to play a single game or even performing a single task in a single game.</s></p><p><s coords="15,47.87,228.48,241.09,8.97;15,37.91,240.44,251.05,8.97;15,37.91,252.39,251.03,8.97;15,37.91,264.35,170.89,8.97">While deep RL-based approaches can learn to play a variety of different Atari games, it is still a significant challenge to develop algorithms that can learn to play any kind of game (e.g., Atari games, DOOM, and StarCraft).</s><s coords="15,211.26,264.35,77.68,8.97;15,37.91,276.31,251.04,8.97;15,37.91,288.26,186.89,8.97">Current approaches still require significant effort to design the network architecture and reward function to a specific type of game.</s></p><p><s coords="15,47.87,301.21,241.08,8.97;15,37.91,313.17,251.07,8.97;15,37.91,325.12,251.06,8.97;15,37.91,337.08,251.08,8.97;15,37.91,349.04,67.70,8.97">Progress on the problem of playing multiple games includes progressive neural networks <ref type="bibr" coords="15,149.76,313.17,20.06,8.97" target="#b119">[120]</ref>, which allow new games to be learned (without forgetting previously learned ones) and solved quicker by exploiting previously learned features through lateral connections.</s><s coords="15,108.34,349.04,180.63,8.97;15,37.91,360.99,40.75,8.97">However, they require a separate network for each task.</s><s coords="15,82.62,360.99,206.36,8.97;15,37.91,372.94,251.07,8.97;15,37.91,384.90,251.04,8.97;15,37.91,396.85,117.33,8.97">Elastic weight consolidation <ref type="bibr" coords="15,202.41,360.99,16.60,8.97" target="#b73">[74]</ref> can learn multiple Atari games sequentially and avoids catastrophic forgetting by protecting weights from being modified that are important for previously learned games.</s><s coords="15,157.49,396.85,131.47,8.97;15,37.91,408.81,251.06,8.97;15,37.91,420.77,251.06,8.97;15,37.91,432.72,134.12,8.97">In PathNet, an evolutionary algorithm is used to select which parts of a neural network are used for learning new tasks, demonstrating some transfer learning performance on ALE games <ref type="bibr" coords="15,152.95,432.72,15.26,8.97" target="#b30">[31]</ref>.</s></p><p><s coords="15,47.87,445.67,241.08,8.97;15,37.91,457.63,251.04,8.97;15,37.91,469.58,251.05,8.97;15,37.91,481.54,127.88,8.97">In the future, it will be important to extend these methods to learn to play multiple games, even if those games are very different-most current approaches focus on different (known) games in the ALE framework.</s><s coords="15,170.15,481.54,118.82,8.97;15,37.91,493.50,251.03,8.97;15,37.91,505.45,79.73,8.97">One suitable avenue for this kind of research is the new learning track of the GVGAI competition <ref type="bibr" coords="15,71.41,505.45,15.27,8.97" target="#b77">[78]</ref>, <ref type="bibr" coords="15,93.57,505.45,20.05,8.97" target="#b115">[116]</ref>.</s><s coords="15,120.72,505.45,168.22,8.97;15,37.91,517.40,77.97,8.97">GVGAI has a potentially unlimited set of games, unlike ALE.</s><s coords="15,117.60,517.40,171.35,8.97;15,37.91,529.36,251.04,8.97;15,37.91,541.32,251.04,8.97;15,37.91,553.27,155.40,8.97">Recent work in GVGAI showed that modelfree deep RL overfitted not just to the individual game, but even to the individual level; this was countered by continuously generating new levels during training <ref type="bibr" coords="15,174.22,553.27,15.27,8.97" target="#b68">[69]</ref>.</s></p><p><s coords="15,47.87,566.22,241.08,8.97;15,37.91,578.18,133.19,8.97">It is possible that significant advances on the multigame problem will come from outside DL.</s><s coords="15,174.55,578.18,114.38,8.97;15,37.91,590.13,251.04,8.97;15,37.91,602.08,126.29,8.97">In particular, the recent tangled graph representation, a form of genetic programming, has shown promise in this task <ref type="bibr" coords="15,145.11,602.08,15.27,8.97" target="#b71">[72]</ref>.</s><s coords="15,166.43,602.08,122.54,8.97;15,37.91,614.05,251.06,8.97;15,37.91,626.00,133.12,8.97">The recent IMPALA algorithm tries to tackle multigame learning through massive scaling, with somewhat promising results <ref type="bibr" coords="15,151.95,626.00,15.27,8.97" target="#b29">[30]</ref>.</s></p><p><s coords="15,47.87,638.95,241.07,8.97;15,37.91,650.91,251.05,8.97;15,37.91,662.86,251.07,8.97;15,37.91,674.81,47.28,8.97">2) Overcoming Sparse, Delayed, or Deceptive Rewards: Games such as Montezuma's Revenge that are characterized by sparse rewards still pose a challenge for most of the deep RL approaches.</s><s coords="15,89.54,674.81,199.44,8.97;15,37.91,686.78,251.06,8.97;15,37.91,698.73,251.09,8.97;15,37.91,710.68,103.13,8.97">While recent advances that combine DQN with intrinsic motivation <ref type="bibr" coords="15,121.66,686.78,11.62,8.97" target="#b7">[8]</ref> or expert demonstrations <ref type="bibr" coords="15,244.08,686.78,15.27,8.97" target="#b56">[57]</ref>, <ref type="bibr" coords="15,267.39,686.78,21.58,8.97" target="#b111">[112]</ref> can help, games with sparse rewards are still a challenge for current deep RL methods.</s><s coords="15,143.42,710.68,145.53,8.97;15,37.91,722.64,251.04,8.97;15,37.91,734.59,170.55,8.97">There is a long history of research in intrinsically motivated RL <ref type="bibr" coords="15,147.87,722.64,15.27,8.97" target="#b21">[22]</ref>, <ref type="bibr" coords="15,170.38,722.64,20.06,8.97" target="#b124">[125]</ref>, as well as hierarchical RL, which might be useful here <ref type="bibr" coords="15,167.63,734.59,10.58,8.97" target="#b4">[5]</ref>, <ref type="bibr" coords="15,184.40,734.59,20.05,8.97" target="#b162">[163]</ref>.</s><s coords="15,211.14,734.59,77.82,8.97;15,300.92,66.10,251.04,8.97;15,300.92,78.05,251.07,8.97;15,300.92,90.02,89.76,8.97">The Project Malmo environment, based on Minecraft, provides an excellent venue for creating tasks with very sparse rewards, where agents need to set their own goals.</s><s coords="15,393.76,90.02,158.21,8.97;15,300.92,101.97,251.03,8.97;15,300.92,113.92,251.06,8.97;15,300.92,125.88,251.07,8.97;15,300.92,137.83,40.94,8.97">Derivative-free and gradient-free methods, such as ESs and genetic algorithms, explore the parameter space by sampling locally and are promising for these games, especially when combined with novelty search as in <ref type="bibr" coords="15,535.40,125.88,16.59,8.97" target="#b23">[24]</ref> and <ref type="bibr" coords="15,317.80,137.83,20.05,8.97" target="#b138">[139]</ref>.</s></p><p><s coords="15,310.89,149.79,241.09,8.97;15,300.92,161.75,240.84,8.97">3) Learning With Multiple Agents: Current deep RL approaches are mostly concerned with training a single agent.</s><s coords="15,544.81,161.75,7.19,8.97;15,300.92,173.70,251.05,8.97;15,300.92,185.66,251.03,8.97;15,300.92,197.61,238.73,8.97">A few exceptions exist, where multiple agents have to cooperate <ref type="bibr" coords="15,300.92,185.66,15.26,8.97" target="#b31">[32]</ref>, <ref type="bibr" coords="15,322.91,185.66,15.27,8.97" target="#b32">[33]</ref>, <ref type="bibr" coords="15,344.91,185.66,15.27,8.97" target="#b84">[85]</ref>, <ref type="bibr" coords="15,366.90,185.66,20.05,8.97" target="#b110">[111]</ref>, <ref type="bibr" coords="15,393.88,185.66,20.05,8.97" target="#b153">[154]</ref>, but it remains an open challenge how these can scale to more agents in various situations.</s><s coords="15,543.69,197.61,8.30,8.97;15,300.92,209.56,251.04,8.97;15,300.92,221.53,184.02,8.97">In many current video games such as StarCraft or GTA V, many agents interact with each other and the player.</s><s coords="15,487.64,221.53,64.34,8.97;15,300.92,233.48,251.05,8.97;15,300.92,245.43,251.04,8.97;15,300.92,257.39,230.12,8.97">To scale multiagent learning in video games to the same level of performance as current single agent approaches will likely require new methods that can effectively train multiple agents at the same time.</s></p><p><s coords="15,310.89,269.34,241.09,8.97;15,300.92,281.30,251.08,8.97;15,300.92,293.26,251.02,8.97;15,300.92,305.21,251.08,8.97;15,300.92,317.17,129.21,8.97">4) Lifetime Adaptation: While nonplayer characters (NPCs) can be trained to play a variety of games well (see Section IV), current machine learning techniques still struggle when it comes to agents that should be able to adapt during their lifetime, i.e., while the game is being played.</s><s coords="15,433.28,317.17,118.67,8.97;15,300.92,329.12,251.06,8.97;15,300.92,341.07,225.99,8.97">For example, a human player can quickly change its behavior when realizing that the player is always ambushed at the same position in an FPS map.</s><s coords="15,529.55,341.07,22.42,8.97;15,300.92,353.03,251.05,8.97;15,300.92,364.99,251.06,8.97;15,300.92,376.94,232.30,8.97">However, most of the current DL techniques would require expensive retraining to adapt to these situations and other unforeseen situations that they have not encountered during training.</s><s coords="15,536.48,376.94,15.49,8.97;15,300.92,388.90,251.06,8.97;15,300.92,400.85,251.05,8.97">The amount of data provided by the real-time behavior of a single human are nowhere near that required by common DL methods.</s><s coords="15,300.92,412.80,251.04,8.97;15,300.92,424.77,217.61,8.97">This challenge is related to the wider problem of few-shot learning, transfer learning, and general video game playing.</s><s coords="15,520.98,424.77,30.99,8.97;15,300.92,436.72,251.05,8.97;15,300.92,448.67,25.74,8.97">Solving it will be important to create more believable and human-like NPCs.</s></p><p><s coords="15,310.89,460.63,241.08,8.97;15,300.92,472.58,251.05,8.97;15,300.92,484.54,61.86,8.97">5) Human-Like Game Playing: Lifetime learning is just one of the differences that current NPCs lack in comparison to human players.</s><s coords="15,366.30,484.54,185.68,8.97;15,300.92,496.50,251.04,8.97;15,300.92,508.45,169.26,8.97">Most approaches are concerned with creating agents that play a particular game as well as possible, often only taking into account the score reached.</s><s coords="15,472.46,508.45,79.53,8.97;15,300.92,520.41,251.08,8.97;15,300.92,532.36,172.45,8.97">However, if humans are expected to play against or cooperate with AI-based bots in video games, other factors come into play.</s><s coords="15,476.53,532.36,75.43,8.97;15,300.92,544.31,251.05,8.97;15,300.92,556.27,251.06,8.97;15,300.92,568.23,216.75,8.97">Instead of creating a bot that plays perfectly, in this context, it becomes more important that the bot is believable and is fun to play against, with similar idiosyncrasies we expect from a human player.</s></p><p><s coords="15,310.88,580.18,241.11,8.97;15,300.92,592.14,251.03,8.97;15,300.92,604.09,251.05,8.97;15,300.92,616.05,140.51,8.97">Human-like game playing is an active area of research with two different competitions focused on human-like behavior, namely, the 2k BotPrize <ref type="bibr" coords="15,399.39,604.09,15.26,8.97" target="#b57">[58]</ref>, <ref type="bibr" coords="15,421.19,604.09,16.60,8.97" target="#b58">[59]</ref> and the Turing Test track of the Mario AI Championship <ref type="bibr" coords="15,417.36,616.05,20.05,8.97" target="#b130">[131]</ref>.</s><s coords="15,444.40,616.05,107.57,8.97;15,300.92,628.01,251.03,8.97;15,300.92,639.96,251.03,8.97;15,300.92,651.92,182.02,8.97">Most entries in these competitions are based on various nonneural network techniques, while some used evolutionary training of deep neural networks to generate human-like behavior <ref type="bibr" coords="15,432.31,651.92,20.05,8.97" target="#b104">[105]</ref>, <ref type="bibr" coords="15,458.88,651.92,20.05,8.97" target="#b126">[127]</ref>.</s></p><p><s coords="15,310.88,663.85,241.09,8.98;15,300.92,675.81,251.06,8.97;15,300.92,687.77,251.06,8.97">6) Adjustable Performance Levels: Almost all current research on DL for game playing aims at creating agents that can play the game as well as possible, maybe even "beating" it.</s><s coords="15,300.92,699.72,251.06,8.97;15,300.92,711.67,251.06,8.97;15,300.92,723.63,251.04,8.97;15,300.92,735.59,241.09,8.97">However, for purposes of game testing, creating tutorials, and demonstrating games-in all those places where it would be important to have human-like game play-it could be important to be able to create agents with a particular skill level.</s><s coords="15,545.34,735.59,6.63,8.97;16,42.12,66.09,251.06,8.97;16,42.12,78.04,205.48,8.97">If your agent plays better than any human player, then it is not a good model of what a human would do in the game.</s><s coords="16,249.89,78.04,43.28,8.97;16,42.12,90.01,251.05,8.97;16,42.12,101.96,251.06,8.97;16,42.12,113.91,41.94,8.97">At its most basic, this could entail training an agent that plays the game very well and then find a way of decreasing the performance of that agent.</s><s coords="16,87.02,113.91,206.16,8.97;16,42.12,125.87,251.06,8.97;16,42.12,137.82,251.04,8.97;16,42.12,149.78,113.88,8.97">However, it would be more useful to be able to adjust the performance level in a more fine-grained way, so as to, for example, separately control the reaction speed or long-term planning ability of an agent.</s><s coords="16,158.98,149.78,134.19,8.97;16,42.12,161.74,251.03,8.97;16,42.12,173.69,251.04,8.97;16,42.12,185.65,131.99,8.97">Even more useful would be to be able to ban certain capacities of playstyles of a trained agent, so as to test whether, for example, a given level could be solved without certain actions or tactics.</s></p><p><s coords="16,52.08,197.60,241.11,8.97;16,42.12,209.55,251.07,8.97;16,42.12,221.52,90.23,8.97">One path to realizing this is the concept of procedural personas, where the preferences of an agent are encoded as a set of utility weights <ref type="bibr" coords="16,113.26,221.52,15.27,8.97" target="#b59">[60]</ref>.</s><s coords="16,135.04,221.52,158.15,8.97;16,42.12,233.47,251.05,8.97;16,42.12,245.42,151.48,8.97">However, this concept has not been implemented using DL, and it is still unclear how to realize the planning depth control in this context.</s></p><p><s coords="16,52.08,257.38,241.07,8.97;16,42.12,269.33,251.07,8.97;16,42.12,281.28,251.04,8.97;16,42.12,293.25,160.35,8.97">7) Dealing With Extremely Large Decision Spaces: Whereas the average branching factor hovers around 30 for Chess and 300 for Go, a game like StarCraft has a branching factor that is orders of magnitudes larger.</s><s coords="16,204.39,293.25,88.77,8.97;16,42.12,305.20,251.06,8.97;16,42.12,317.15,251.05,8.97;16,42.12,329.11,251.03,8.97;16,42.12,341.06,136.77,8.97">While recent advances in evolutionary planning have allowed real-time and long-term planning in games with larger branching factors to <ref type="bibr" coords="16,251.72,317.15,15.27,8.97" target="#b65">[66]</ref>, <ref type="bibr" coords="16,274.08,317.15,15.27,8.97" target="#b66">[67]</ref>, and <ref type="bibr" coords="16,58.52,329.11,20.05,8.97" target="#b158">[159]</ref>, how we can scale deep RL to such levels of complexity is an important open challenge.</s><s coords="16,181.20,341.06,112.00,8.97;16,42.12,353.02,251.07,8.97;16,42.12,364.98,37.89,8.97">Learning heuristics with DL in these games to enhance search algorithms is also a promising direction.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="16,42.12,395.91,73.54,8.97">B. Game Industry</head><p><s coords="16,52.08,412.85,241.09,8.97;16,42.12,424.80,251.03,8.97;16,42.12,436.75,251.03,8.97;16,42.12,448.71,251.06,8.97;16,42.12,460.67,97.95,8.97">1) Adoption in the Game Industry: Many of the recent advances in DL have been accelerated because of the increased interest by a variety of different companies such as Facebook, Google/Alphabet, Microsoft, and Amazon, which heavily invest in its development.</s><s coords="16,143.90,460.67,149.29,8.97;16,42.12,472.62,183.67,8.97">However, the game industry has not embraced these advances to the same extent.</s><s coords="16,229.33,472.62,63.85,8.97;16,42.12,484.58,251.05,8.97;16,42.12,496.53,194.48,8.97">This sometimes surprises commentators outside of the game industry, as games are seen as making heavy use of AI techniques.</s><s coords="16,239.83,496.53,53.35,8.97;16,42.12,508.49,251.05,8.97;16,42.12,520.44,251.04,8.97;16,42.12,532.40,115.95,8.97">However, the type of AI that is most commonly used in the games industry focuses more on hand-authoring of expressive NPC behaviors rather than machine learning.</s><s coords="16,160.32,532.40,132.84,8.97;16,42.12,544.36,251.07,8.97;16,42.12,556.31,251.04,8.97;16,42.12,568.26,251.06,8.97;16,42.12,580.22,233.34,8.97">An often-cited reason for the lack of adoption of neural networks (and similar methods) within this industry is that such methods are inherently difficult to control, which could result in unwanted NPC behaviors (e.g., an NPC could decide to kill a key actor that is relevant to the story).</s><s coords="16,277.70,580.22,15.49,8.97;16,42.12,592.17,251.05,8.97;16,42.12,604.13,251.05,8.97">Additionally, training deep network models require a certain level of expertise, and the pool of experts in this area is still limited.</s><s coords="16,42.12,616.09,251.08,8.97;16,42.12,628.04,120.77,8.97">It is important to address these challenges to encourage a wide adoption in the game industry.</s></p><p><s coords="16,52.08,639.99,241.10,8.97;16,42.12,651.95,251.05,8.97;16,42.12,663.91,190.33,8.97">Additionally, while most DL approaches focus exclusively on playing games as well as possible, this goal might not be the most important for the game industry <ref type="bibr" coords="16,208.38,663.91,20.05,8.97" target="#b170">[171]</ref>.</s><s coords="16,235.02,663.91,58.12,8.97;16,42.12,675.86,251.07,8.97;16,42.12,687.82,75.79,8.97">Here, the level of fun or engagement the player experiences while playing is a crucial component.</s><s coords="16,119.87,687.82,173.31,8.97;16,42.12,699.77,251.02,8.97;16,42.12,711.73,251.05,8.97">One use of DL for game playing in the game production process is for game testing, where artificial agents test that levels are solvable or that the difficulty is appropriate.</s><s coords="16,42.12,723.68,251.07,8.97;16,42.12,735.64,251.06,8.97;16,305.13,66.10,251.03,8.97;16,305.13,78.05,96.94,8.97">DL might see its most prominent use in the games industry not for playing games, but for generating game content <ref type="bibr" coords="16,246.66,735.64,21.57,8.97" target="#b129">[130]</ref> based on training on existing content <ref type="bibr" coords="16,436.99,66.10,20.05,8.97" target="#b139">[140]</ref>, <ref type="bibr" coords="16,464.43,66.10,20.05,8.97" target="#b157">[158]</ref>, or for modeling player experience <ref type="bibr" coords="16,378.02,78.05,20.05,8.97" target="#b168">[169]</ref>.</s></p><p><s coords="16,315.10,90.01,241.06,8.97;16,305.13,101.97,251.08,8.97;16,305.13,113.92,251.06,8.97;16,305.13,125.88,52.40,8.97">Within the game industry, several of the large development and technology companies, including Electronic Arts, Ubisoft, and Unity, have recently started in-house research arms focusing partly on DL.</s><s coords="16,359.26,125.88,196.93,8.97;16,305.13,137.83,251.05,8.97;16,305.13,149.78,74.42,8.97">It remains to be seen whether these techniques will also be embraced by the development arms of these companies or their customers.</s></p><p><s coords="16,315.10,161.75,241.09,8.97;16,305.13,173.70,250.95,8.97;16,305.13,185.66,123.13,8.97">2) Interactive Tools for Game Development: Related to the previous challenge, there is currently a lack of tools for designers to easily train NPC behaviors.</s><s coords="16,431.66,185.66,124.53,8.97;16,305.13,197.61,251.07,8.97;16,305.13,209.56,118.80,8.97">While many open-source tools to training deep networks exist now, most of them require a significant level of expertise.</s><s coords="16,427.86,209.56,128.35,8.97;16,305.13,221.52,251.02,8.97;16,305.13,233.48,251.05,8.97;16,305.13,245.43,251.05,8.97;16,305.13,257.39,58.51,8.97">A tool that allows designers to easily specify desired NPC behaviors (and undesired ones) while assuring a certain level of control over the final trained outcomes would greatly accelerate the uptake of these new methods in the game industry.</s></p><p><s coords="16,315.10,269.34,241.10,8.97;16,305.13,281.29,47.20,8.97">Learning from human preferences is one promising direction in this area.</s><s coords="16,355.35,281.29,200.84,8.97;16,305.13,293.25,251.07,8.97;16,305.13,305.21,251.05,8.97;16,305.13,317.16,24.06,8.97">This approach has been extensively studied in the context of NE <ref type="bibr" coords="16,366.36,293.25,20.06,8.97" target="#b114">[115]</ref>, and also in the context of video games, allowing nonexpert users to breed behaviors for Super Mario <ref type="bibr" coords="16,305.13,317.16,20.05,8.97" target="#b134">[135]</ref>.</s><s coords="16,332.76,317.16,223.41,8.97;16,305.13,329.12,251.07,8.97;16,305.13,341.07,251.06,8.97;16,305.13,353.03,54.11,8.97">Recently, a similar preference-based approach was applied to deep RL method <ref type="bibr" coords="16,411.58,329.12,15.27,8.97" target="#b22">[23]</ref>, allowing agents to learn Atari games based on a combination of human preference learning and deep RL.</s><s coords="16,362.09,353.03,194.11,8.97;16,305.13,364.99,251.04,8.97;16,305.13,376.94,251.05,8.97;16,305.13,388.90,69.46,8.97">Recently, the game company King published results using imitation learning to learn policies for playtesting of Candy Crush levels, showing a promising direction for new design tools <ref type="bibr" coords="16,355.50,388.90,15.27,8.97" target="#b40">[41]</ref>.</s></p><p><s coords="16,315.10,400.85,241.08,8.97;16,305.13,412.80,213.91,8.97">3) Creating New Types of Video Games: DL could potentially offer a way to create completely new games.</s><s coords="16,523.24,412.80,32.95,8.97;16,305.13,424.76,251.05,8.97;16,305.13,436.72,251.06,8.97;16,305.13,448.67,251.04,8.97;16,305.13,460.63,13.00,8.97">Most of today's game designs stem from a time when no advanced AI methods were available or the hardware too limited to utilize them, meaning that games have been designed to not need AI.</s><s coords="16,322.42,460.63,233.74,8.97;16,305.14,472.58,80.09,8.97">Designing new games around AI can help to break out of these limitations.</s><s coords="16,388.34,472.58,167.84,8.97;16,305.14,484.54,251.04,8.97;16,305.14,496.49,251.04,8.97;16,305.14,508.45,87.34,8.97">While evolutionary algorithms and NE in particular <ref type="bibr" coords="16,346.85,484.54,21.57,8.97" target="#b114">[115]</ref> have allowed the creation of completely new types of games, DL based on gradient descent has not been explored in this context.</s><s coords="16,395.19,508.45,160.99,8.97;16,305.14,520.41,251.04,8.97;16,305.14,532.36,70.57,8.97">NE is a core mechanic in games such as NERO <ref type="bibr" coords="16,334.51,520.41,20.05,8.97" target="#b136">[137]</ref>, Galactic Arms Race <ref type="bibr" coords="16,444.47,520.41,15.26,8.97" target="#b47">[48]</ref>, Petalz <ref type="bibr" coords="16,493.23,520.41,20.05,8.97" target="#b113">[114]</ref>, and Evo-Commander <ref type="bibr" coords="16,356.63,532.36,15.26,8.97" target="#b63">[64]</ref>.</s><s coords="16,377.94,532.36,178.23,8.97;16,305.14,544.31,251.03,8.97;16,305.14,556.27,251.07,8.97;16,305.14,568.23,167.22,8.97">One challenge with gradient-based optimization is that the structures are often limited to having mathematical smoothness (i.e., differentiability), making it challenging to create interesting and unexpected outputs.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="16,305.14,599.16,123.34,8.97">C. Learning Models of Games</head><p><s coords="16,315.10,616.10,241.07,8.97;16,305.14,628.05,251.08,8.97;16,305.14,640.00,201.08,8.97">Much work on DL for game-playing takes a model-free endto-end learning approach, where a neural network is trained to produce actions given state observations as input.</s><s coords="16,509.46,640.00,46.74,8.97;16,305.14,651.96,251.05,8.97;16,305.14,663.91,251.06,8.97;16,305.14,675.87,194.65,8.97">However, it is well known that a good and fast forward model makes game playing much easier, as it makes it possible to use planning methods based on tree search or evolution <ref type="bibr" coords="16,475.72,675.87,20.05,8.97" target="#b170">[171]</ref>.</s><s coords="16,502.40,675.87,53.78,8.97;16,305.14,687.83,251.05,8.97;16,305.14,699.78,251.08,8.97;16,305.14,711.74,105.68,8.97">Therefore, an important open challenge in this field is to develop methods that can learn a forward model of the game, making it possible to reason about its dynamics.</s></p><p><s coords="16,315.10,723.69,241.09,8.97;16,305.14,735.65,251.07,8.97;17,37.91,66.09,63.50,8.97">The hope is that approaches that learn the rules of the game can generalize better to different game variations and show more robust learning.</s><s coords="17,105.09,66.09,183.88,8.97;17,37.91,78.04,251.05,8.97;17,37.91,90.01,155.30,8.97">Promising work in this area includes the approach by Guzdial et al. <ref type="bibr" coords="17,133.25,78.04,16.59,8.97" target="#b43">[44]</ref> that learns a simple game engine of Super Mario Bros. from gameplay data.</s><s coords="17,195.07,90.01,93.89,8.97;17,37.91,101.96,250.96,8.97;17,37.91,113.91,251.03,8.97;17,37.91,125.87,209.53,8.97">Kansky et al. <ref type="bibr" coords="17,248.37,90.01,16.59,8.97" target="#b69">[70]</ref> introduce the idea of Schema Networks that follow an object-oriented approach and are trained to predict future object attributes and rewards based on the current attributes and actions.</s><s coords="17,250.78,125.87,38.18,8.97;17,37.91,137.82,251.03,8.97;17,37.91,149.78,251.06,8.97;17,37.91,161.74,219.64,8.97">A trained schema network thus provides a probabilistic model that can be used for planning and is able to perform zero-shot transfer to variations of Breakout similar to those used in training.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="17,37.91,186.15,117.99,8.97">D. Computational Resources</head><p><s coords="17,47.87,203.09,241.08,8.97;17,37.91,215.04,251.05,8.97;17,37.91,227.00,34.00,8.97">With more advanced computational models and a larger number of agents in open worlds, computational speed becomes a concern.</s><s coords="17,74.59,227.00,214.33,8.97;17,37.91,238.96,251.05,8.97;17,37.91,250.91,208.13,8.97">Methods that aim to make the networks computationally more efficient by either compressing networks <ref type="bibr" coords="17,238.62,238.96,16.60,8.97" target="#b61">[62]</ref> or pruning networks after training <ref type="bibr" coords="17,144.14,250.91,15.27,8.97" target="#b42">[43]</ref>, <ref type="bibr" coords="17,165.19,250.91,16.60,8.97" target="#b46">[47]</ref> could be useful.</s><s coords="17,248.01,250.91,40.95,8.97;17,37.91,262.87,251.05,8.97;17,37.91,274.82,170.87,8.97">Of course, improvements in processing power in general or for neural networks specifically will also be important.</s><s coords="17,212.59,274.82,76.37,8.97;17,37.91,286.78,251.07,8.97;17,37.91,298.74,251.07,8.97;17,37.91,310.69,124.50,8.97">Currently, it is not feasible to train networks in real time to adapt to changes in the game or to fit players' playing styles, something which could be useful in the design process.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="17,126.20,335.11,74.47,8.97">VII. CONCLUSION</head><p><s coords="17,47.87,352.05,241.09,8.97;17,37.91,364.00,251.05,8.97;17,37.91,375.95,251.05,8.97;17,37.91,387.91,27.88,8.97">This paper reviewed DL methods applied to game playing in video games of various genres including arcade, racing, FPSs, open-world, RTS, team sports, physics, and text adventure games.</s><s coords="17,68.28,387.91,220.68,8.97;17,37.91,399.87,251.06,8.97;17,37.91,411.82,142.81,8.97">Most of the reviewed work is within end-to-end modelfree deep RL, where a CNN learns to play directly from raw pixels by interacting with the game.</s><s coords="17,183.28,411.82,105.69,8.97;17,37.91,423.78,251.06,8.97;17,37.91,435.73,48.00,8.97">Recent work demonstrates that derivative-free ESs and genetic algorithms are competitive alternatives.</s><s coords="17,87.86,435.73,201.12,8.97;17,37.91,447.68,251.05,8.97;17,37.91,459.65,204.79,8.97">Some of the reviewed work apply supervised learning to imitate behaviors from game logs, while others are based on methods that learn a model of the environment.</s><s coords="17,245.72,459.65,43.25,8.97;17,37.91,471.60,251.05,8.97;17,37.91,483.55,251.07,8.97;17,37.91,495.51,164.15,8.97">For simple games, such as most arcade games, the reviewed methods can achieve above human-level performance, while there are many open challenges in more complex games.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,37.91,160.09,329.00,7.17;5,42.35,66.77,504.14,61.34"><head>Fig. 2 .</head><label>2</label><figDesc><div><p><s coords="5,37.91,160.09,329.00,7.17">Fig. 2. Screenshots of selected games and frameworks used as research platforms for research in DL.</s></p></div></figDesc><graphic coords="5,42.35,66.77,504.14,61.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="14,42.12,631.99,514.09,7.17;14,42.12,640.95,514.09,7.17;14,42.12,649.92,514.10,7.17;14,42.12,658.88,245.91,7.17"><head>Fig. 4 .</head><label>4</label><figDesc><div><p><s coords="14,42.12,631.99,232.20,7.17">Fig. 4. Influence diagram of the DL techniques discussed in this paper.</s><s coords="14,276.30,631.99,237.48,7.17">Each node is an algorithm while the color represents the game benchmark.</s><s coords="14,515.74,631.99,40.47,7.17;14,42.12,640.95,261.73,7.17">The distance from the center represents the date that the original paper was published on arXiv.</s><s coords="14,305.91,640.95,156.48,7.17">The arrows represent how techniques are related.</s><s coords="14,464.46,640.95,91.75,7.17;14,42.12,649.92,136.71,7.17">Each node points to all other nodes that used or modified that technique.</s><s coords="14,180.77,649.92,276.64,7.17">Arrows pointing to a particular algorithm show which algorithms influenced its design.</s><s coords="14,459.36,649.92,96.86,7.17;14,42.12,658.88,245.91,7.17">Influences are not transitive: if algorithm a influenced b and b influenced c, a did not necessarily influence c.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,305.14,65.65,251.09,213.18"><head>TABLE I HUMAN</head><label>I</label><figDesc><div><p><s coords="6,345.02,74.61,193.54,7.17;6,350.80,83.57,159.72,7.17;6,372.60,92.55,116.12,7.17;6,305.14,236.69,251.07,6.28;6,305.14,245.66,188.98,6.28">-NORMALIZED SCORES REPORTED WITH VARIOUS DEEP RL ALGORITHMS IN ALE ON 57 ATARI GAMES USING THE 30 no-ops EVALUATION METRIC References in the first column refer to the paper that included the results, while the last column references the paper that first introduced the specific technique.</s><s coords="6,495.72,245.66,60.48,6.28;6,305.14,254.62,251.07,6.28">Note that the reported scores use various amounts of training time and resources, thus not entirely comparable.</s><s coords="6,305.14,263.59,184.85,6.28">Successors typically use more resources and less wall-clock time.</s><s coords="6,491.96,263.59,64.26,6.28;6,305.14,272.55,168.95,6.28">*Hyperparameters was tuned for every game leading to higher scores for UNREAL.</s></p></div></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Univ Politecnica de Madrid. Downloaded on February 07,2023 at 23:18:34 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">http://bwapi.github.io/ Authorized licensed use limited to: Univ Politecnica de Madrid. Downloaded on February 07,2023 at</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_2">:18:34 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">http://vizdoom.cs.put.edu.pl/competition-cig-2016 Authorized licensed use limited to: Univ Politecnica de Madrid. Downloaded on February 07,2023 at 23:18:34 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="17,123.17,519.93,80.51,8.97">ACKNOWLEDGMENT</head><p><s coords="17,47.87,536.86,241.10,8.97;17,37.91,548.82,251.05,8.97;17,37.91,560.78,251.06,8.97;17,37.91,572.73,221.64,8.97">The authors would like to thank the numerous colleagues who took the time to comment on drafts of this paper, including C. Tessler, D. Pérez-Liébana, E. Caballero, H. Daumé, III, J. Busk, K. Arulkumaran, M. Heywood, M. G. Bellemare, M.-P.</s><s coords="17,262.12,572.73,26.83,8.97;17,37.91,584.68,251.05,8.97;17,37.91,596.64,173.78,8.97">Huget, M. Preuss, N. de Freitas, N. A. Barriga, O. Delalleau, P. Stone, S. Ontañón, T. Matiisen, Y. Fu, and Y. Hou.</s></p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,207.40,618.43,81.57,7.17;1,37.91,627.41,251.08,7.17;1,37.91,636.37,98.72,7.17">The work of N. Justesen was supported by the Elite Research travel grant from The Danish Ministry for Higher Education and Science.</s></p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="17,60.15,638.36,228.82,7.17;17,60.16,647.33,228.81,7.17;17,60.16,656.30,45.84,7.17" xml:id="b0">
	<analytic>
		<title level="a" type="main">Autoencoder-augmented neuroevolution for visual doom playing</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Alvernaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2017.8080408</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Alvernaz and J. Togelius, &quot;Autoencoder-augmented neuroevolution for visual doom playing,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 1-8.</note>
</biblStruct>

<biblStruct coords="17,60.15,665.26,228.81,7.17;17,60.16,674.22,228.81,7.17;17,60.16,683.20,137.16,7.17" xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning: A Brief Survey</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
			<idno type="ORCID">0000-0003-0459-892X</idno>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">Anthony</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="DOI">10.1109/msp.2017.2743240</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<title level="j" type="abbrev">IEEE Signal Process. Mag.</title>
		<idno type="ISSN">1053-5888</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017-11">Nov. 2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, &quot;Deep reinforcement learning: A brief survey,&quot; IEEE Signal Process. Mag., vol. 34, no. 6, pp. 26-38, Nov. 2017.</note>
</biblStruct>

<biblStruct coords="17,60.16,692.16,228.83,7.17;17,60.16,701.12,228.83,7.17;17,60.16,710.09,49.82,7.17" xml:id="b2">
	<analytic>
		<title level="a" type="main">Overview of RoboCup-98</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tambe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kitano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Kraetzschmar</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-48422-1_1</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
				<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Asada, M. M. Veloso, M. Tambe, I. Noda, H. Kitano, and G. K. Kraetzschmar, &quot;Overview of RoboCup-98,&quot; AI Mag., vol. 21, no. 1, pp. 9-19, 2000.</note>
</biblStruct>

<biblStruct coords="17,60.15,719.06,228.81,7.17;17,60.16,728.03,228.83,7.17;17,60.16,736.99,188.18,7.17" xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining Strategic Learning with Tactical Search in Real-Time Strategy Games</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Barriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Stanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Buro</surname></persName>
		</author>
		<idno type="DOI">10.1609/aiide.v13i1.12922</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<title level="j" type="abbrev">AIIDE</title>
		<idno type="ISSN">2326-909X</idno>
		<idno type="ISSNe">2334-0924</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2017">2017</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">N. A. Barriga, M. Stanescu, and M. Buro, &quot;Combining strategic learning and tactical search in real-time strategy games,&quot; in Proc. 13th AAAI Conf. Artif. Intell. Interact. Digit. Entertainment, 2017, pp. 9-15.</note>
</biblStruct>

<biblStruct coords="17,323.16,67.45,228.82,7.17;17,323.16,76.42,228.83,7.17;17,323.16,85.39,61.79,7.17" xml:id="b4">
	<analytic>
		<title level="a" type="main">Recent advances in hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1025696116075</idno>
	</analytic>
	<monogr>
		<title level="j">Discrete Event Dynamic Systems</title>
		<idno type="ISSN">0924-6703</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="379" />
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. G. Barto and S. Mahadevan, &quot;Recent advances in hierarchical reinforcement learning,&quot; Discrete Event Dyn. Syst., vol. 13, no. 4, pp. 341-379, 2003.</note>
</biblStruct>

<biblStruct coords="17,323.16,94.35,192.09,7.17" xml:id="b5">
	<monogr>
		<title level="m" type="main">Review of Freyberg et al 2016 A lab in the field....</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<idno type="DOI">10.5194/hess-2016-585-rc1</idno>
		<idno type="arXiv">arXiv:1612.03801</idno>
		<imprint>
			<date type="published" when="2016-11-23">2016</date>
			<publisher>Copernicus GmbH</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Beattie et al., &quot;DeepMind lab,&quot; 2016, arXiv:1612.03801.</note>
</biblStruct>

<biblStruct coords="17,323.16,103.32,228.82,7.17;17,323.16,112.28,228.82,7.17;17,323.16,121.25,177.16,7.17" xml:id="b6">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.3912</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<title level="j" type="abbrev">jair</title>
		<idno type="ISSNe">1076-9757</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2015">2015</date>
			<publisher>AI Access Foundation</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, &quot;The arcade learn- ing environment: An evaluation platform for general agents,&quot; in Proc. 24th Int. Joint Conf. Artif. Intell., 2015, pp. 4148-4152.</note>
</biblStruct>

<biblStruct coords="17,323.16,130.22,228.83,7.17;17,323.16,139.18,228.82,7.17;17,323.16,148.15,205.51,7.17" xml:id="b7">
	<analytic>
		<title level="a" type="main">Increasing the Action Gap: New Operators for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>G. Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v30i1.10303</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1471" to="1479" />
			<date type="published" when="2016-02-21">2016</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, &quot;Unifying count-based exploration and intrinsic motivation,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 1471-1479.</note>
</biblStruct>

<biblStruct coords="17,323.16,157.12,228.84,7.17;17,323.16,166.08,228.84,7.17;17,323.16,175.05,41.87,7.17" xml:id="b8">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. G. Bellemare, W. Dabney, and R. Munos, &quot;A distributional perspec- tive on reinforcement learning,&quot; in Proc. Int. Conf. Mach. Learn., 2017, pp. 449-458.</note>
</biblStruct>

<biblStruct coords="17,323.15,184.01,228.82,7.17;17,323.16,192.98,228.83,7.17;17,323.16,201.95,144.54,7.17" xml:id="b9">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.3912</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<title level="j" type="abbrev">jair</title>
		<idno type="ISSNe">1076-9757</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013-06-14">2013</date>
			<publisher>AI Access Foundation</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, &quot;The arcade learning environment: An evaluation platform for general agents,&quot; J. Artif. Intell. Res., vol. 47, pp. 253-279, 2013.</note>
</biblStruct>

<biblStruct coords="17,323.16,210.91,228.85,7.17;17,323.16,219.88,214.50,7.17" xml:id="b10">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
				<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-06-14">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Bengio, J. Louradour, R. Collobert, and J. Weston, &quot;Curriculum learn- ing,&quot; in Proc. 26th Annu. Int. Conf. Mach. Learn., 2009, pp. 41-48.</note>
</biblStruct>

<biblStruct coords="17,323.16,228.85,228.84,7.17;17,323.16,237.81,228.83,7.17;17,323.16,246.78,228.84,7.17;17,323.16,255.75,52.48,7.17" xml:id="b11">
	<analytic>
		<title level="a" type="main">Hyperopt: a Python library for model selection and hyperparameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1088/1749-4699/8/1/014008</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<title level="j" type="abbrev">Comput. Sci. Disc.</title>
		<idno type="ISSNe">1749-4699</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">014008</biblScope>
			<date type="published" when="2013">2013</date>
			<publisher>IOP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Bergstra, D. Yamins, and D. D. Cox, &quot;Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vi- sion architectures,&quot; in Proc. 30th Int. Conf. Int. Conf. Mach. Learn., 2013, pp. I-115-I-123.</note>
</biblStruct>

<biblStruct coords="17,323.16,264.71,228.83,7.17;17,323.16,273.68,228.82,7.17;17,323.16,282.64,88.58,7.17" xml:id="b12">
	<analytic>
		<title level="a" type="main">Suitability of V1 Energy Models for Object Classification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco_a_00084</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="774" to="790" />
			<date type="published" when="2011-03">2011</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. S. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, &quot;Algorithms for hyper-parameter optimization,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2011, pp. 2546-2554.</note>
</biblStruct>

<biblStruct coords="17,323.15,291.61,228.82,7.17;17,323.16,300.58,228.84,7.17;17,323.16,309.54,113.04,7.17" xml:id="b13">
	<monogr>
		<title level="m" type="main">Playing doom with SLAM-augmented deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bhatti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00380</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Bhatti, A. Desmaison, O. Miksik, N. Nardelli, N. Siddharth, and P. H. Torr, &quot;Playing doom with SLAM-augmented deep reinforcement learning,&quot; 2016, arXiv:1612.00380.</note>
</biblStruct>

<biblStruct coords="17,323.16,318.51,228.82,7.17;17,323.16,327.48,155.01,7.17" xml:id="b14">
	<monogr>
		<title level="m" type="main">Playing SNES in the retro learning environment</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bhonker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rozenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02205</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Bhonker, S. Rozenberg, and I. Hubara, &quot;Playing SNES in the retro learning environment,&quot; 2017, arXiv:1611.02205.</note>
</biblStruct>

<biblStruct coords="17,323.16,336.44,228.83,7.17;17,323.16,345.41,228.83,7.17;17,323.16,354.37,128.63,7.17" xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep apprenticeship learning for playing video games</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bogdanovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Markovikj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshops 29th AAAI Conf</title>
				<meeting>Workshops 29th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Bogdanovic, D. Markovikj, M. Denil, and N. De Freitas, &quot;Deep apprenticeship learning for playing video games,&quot; in Proc. Workshops 29th AAAI Conf. Artif. Intell., Jan. 2015.</note>
</biblStruct>

<biblStruct coords="17,323.16,363.34,200.94,7.17" xml:id="b16">
	<monogr>
		<title level="m" type="main">OpenAI Gym</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Brockman et al., &quot;OpenAI Gym,&quot; 2016, arXiv:1606.01540.</note>
</biblStruct>

<biblStruct coords="17,323.16,372.31,228.81,7.17;17,323.16,381.27,228.84,7.17" xml:id="b17">
	<analytic>
		<title level="a" type="main">A Survey of Monte Carlo Tree Search Methods</title>
		<author>
			<persName><forename type="first">Cameron</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyridon</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Colton</surname></persName>
		</author>
		<idno type="DOI">10.1109/tciaig.2012.2186810</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<title level="j" type="abbrev">IEEE Trans. Comput. Intell. AI Games</title>
		<idno type="ISSN">1943-068X</idno>
		<idno type="ISSNe">1943-0698</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012-03">Mar. 2012</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">C. B. Browne et al., &quot;A survey of Monte Carlo tree search methods,&quot; IEEE Trans. Comput. Intell. AI Games, vol. 4, no. 1, pp. 1-43, Mar. 2012.</note>
</biblStruct>

<biblStruct coords="17,323.16,390.23,228.84,7.17;17,323.16,399.21,228.83,7.17;17,323.16,408.17,109.47,7.17" xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Global Network Topology Using Local Information for Multi-Agent Coordination</title>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<idno type="DOI">10.2514/6.2021-1768.vid</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<publisher>American Institute of Aeronautics and Astronautics (AIAA)</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y.-H. Chang, T. Ho, and L. P. Kaelbling, &quot;All learning is local: Multi- agent learning in global reward games,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2003, pp. 807-814.</note>
</biblStruct>

<biblStruct coords="17,323.15,417.14,228.84,7.17;17,323.16,426.11,228.83,7.17;17,323.16,435.07,186.13,7.17" xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer deep reinforcement learning in 3D environments: An empirical study</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Sathyendra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. S. Chaplot, G. Lample, K. M. Sathyendra, and R. Salakhutdinov, &quot;Transfer deep reinforcement learning in 3D environments: An empirical study,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2016.</note>
</biblStruct>

<biblStruct coords="17,323.16,444.04,228.83,7.17;17,323.16,453.00,228.83,7.17;17,323.16,461.97,146.78,7.17" xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving</title>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.312</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. Chen, A. Seff, A. Kornhauser, and J. Xiao, &quot;DeepDriving: Learning affordance for direct perception in autonomous driving,&quot; in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 2722-2730.</note>
</biblStruct>

<biblStruct coords="17,323.16,470.94,228.84,7.17;17,323.16,479.90,228.82,7.17;17,323.16,488.86,69.77,7.17" xml:id="b21">
	<analytic>
		<title level="a" type="main">Intrinsically motivated reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chentanez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1281" to="1288" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Chentanez, A. G. Barto, and S. P. Singh, &quot;Intrinsically motivated reinforcement learning,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2005, pp. 1281-1288.</note>
</biblStruct>

<biblStruct coords="17,323.16,497.84,228.83,7.17;17,323.16,506.81,228.83,7.17;17,323.16,515.77,173.55,7.17" xml:id="b22">
	<analytic>
		<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2017.2655663</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw. Learning Syst.</title>
		<idno type="ISSN">2162-237X</idno>
		<idno type="ISSNe">2162-2388</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="772" to="772" />
			<date type="published" when="2017-03">2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, &quot;Deep reinforcement learning from human preferences,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 4302-4310.</note>
</biblStruct>

<biblStruct coords="17,323.16,524.73,228.82,7.17;17,323.16,533.71,228.82,7.17;17,323.16,542.67,228.84,7.17;17,323.16,551.63,154.19,7.17" xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf</title>
				<meeting>Int. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5032" to="5043" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. Conti, V. Madhavan, F. P. Such, J. Lehman, K. Stanley, and J. Clune, &quot;Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2018, pp. 5032-5043.</note>
</biblStruct>

<biblStruct coords="17,323.16,560.61,228.82,7.17;17,323.16,569.57,107.24,7.17" xml:id="b24">
	<analytic>
		<title level="a" type="main">TextWorld: A Learning Environment for Text-Based Games</title>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ákos</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kybartas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tavian</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><surname>El Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Adada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-24337-1_3</idno>
		<idno type="arXiv">arXiv:1806.11532</idno>
	</analytic>
	<monogr>
		<title level="m">Communications in Computer and Information Science</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
	<note type="raw_reference">M.-A. Côté et al., &quot;TextWorld: A learning environment for text-based games,&quot; 2018, arXiv:1806.11532.</note>
</biblStruct>

<biblStruct coords="17,323.16,578.54,228.83,7.17;17,323.16,587.50,112.15,7.17" xml:id="b25">
	<analytic>
		<title level="a" type="main">Playing Atari with Six Neurons (Extended Abstract)</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/651</idno>
		<idno type="arXiv">arXiv:1806.01363</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Cuccu, J. Togelius, and P. Cudre-Mauroux, &quot;Playing Atari with six neurons,&quot; 2018, arXiv:1806.01363.</note>
</biblStruct>

<biblStruct coords="17,323.16,596.47,228.83,7.17;17,323.16,605.44,228.83,7.17;17,323.16,614.40,91.13,7.17" xml:id="b26">
	<analytic>
		<title level="a" type="main">Model-Free reinforcement learning with continuous action in practice</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="DOI">10.1109/acc.2012.6315022</idno>
	</analytic>
	<monogr>
		<title level="m">2012 American Control Conference (ACC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-06">2012</date>
			<biblScope unit="page" from="2177" to="2182" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Degris, P. M. Pilarski, and R. S. Sutton, &quot;Model-free reinforcement learning with continuous action in practice,&quot; in Proc. Amer. Control Conf., 2012, pp. 2177-2182.</note>
</biblStruct>

<biblStruct coords="17,323.16,623.36,228.84,7.17;17,323.16,632.34,228.81,7.17;17,323.16,641.30,228.82,7.17;17,323.16,650.26,83.46,7.17" xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to act by predicting the future</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Dosovitskiy and V. Koltun, &quot;Learning to act by predicting the fu- ture,&quot; in Proc. Int. Conf. Learn. Represent., 2017, [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.</note>
</biblStruct>

<biblStruct coords="17,323.16,659.23,228.85,7.17;17,323.16,668.20,228.82,7.17;17,323.16,677.17,165.76,7.17" xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continuous control</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Int. Conf. Mach. Learn</title>
				<meeting>33rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, &quot;Bench- marking deep reinforcement learning for continuous control,&quot; in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 1329-1338.</note>
</biblStruct>

<biblStruct coords="17,323.16,686.13,228.82,7.17;17,323.16,695.09,228.84,7.17;17,323.16,704.07,154.00,7.17" xml:id="b29">
	<analytic>
		<title level="a" type="main">IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">inProc. 35th Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="1407" to="1416" />
			<date type="published" when="2018">Jul. 10-15, 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Espeholt et al., &quot;IMPALA: Scalable distributed deep-RL with im- portance weighted actor-learner architectures,&quot; inProc. 35th Int. Conf. Mach. Learn., Jul. 10-15, 2018, pp. 1407-1416.</note>
</biblStruct>

<biblStruct coords="17,323.16,714.03,228.83,7.17;17,323.16,722.99,157.24,7.17" xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient Descent Algorithms</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/3905.003.0011</idno>
		<idno type="arXiv">arXiv:1701.08734</idno>
	</analytic>
	<monogr>
		<title level="m">An Introduction to Neural Networks</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Fernando et al., &quot;PathNet: Evolution channels gradient descent in super neural networks,&quot; 2017, arXiv:1701.08734.</note>
</biblStruct>

<biblStruct coords="18,64.36,67.45,228.84,7.17;18,64.37,76.42,228.83,7.17;18,64.37,85.39,111.04,7.17" xml:id="b31">
	<analytic>
		<title level="a" type="main">Counterfactual Multi-Agent Policy Gradients</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v32i1.11794</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2974" to="2982" />
			<date type="published" when="2018-04-29">2018</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, &quot;Counterfactual multi-agent policy gradients,&quot; in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 2974-2982.</note>
</biblStruct>

<biblStruct coords="18,64.36,94.35,228.81,7.17;18,64.37,103.32,228.82,7.17;18,64.37,112.28,37.88,7.17" xml:id="b32">
	<analytic>
		<title level="a" type="main">Stabilising experience replay for deep multi-agent reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1146" to="1155" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. N. Foerster et al., &quot;Stabilising experience replay for deep multi-agent reinforcement learning,&quot; in Proc. Int. Conf. Mach. Learn., 2017, pp. 1146-1155.</note>
</biblStruct>

<biblStruct coords="18,64.36,121.25,228.83,7.17;18,64.37,130.22,228.84,7.17;18,64.37,139.18,116.05,7.17" xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to communicate to solve riddles with deep distributed recurrent qnetworks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02672</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, &quot;Learn- ing to communicate to solve riddles with deep distributed recurrent q- networks,&quot; 2016, arXiv:1602.02672.</note>
</biblStruct>

<biblStruct coords="18,64.36,148.15,228.81,7.17;18,64.37,157.12,228.80,7.17;18,64.37,166.08,228.81,7.17;18,64.37,175.05,83.46,7.17" xml:id="b34">
	<analytic>
		<title level="a" type="main">Noisy networks for exploration</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Fortunato et al., &quot;Noisy networks for exploration,&quot; in Proc. Int. Conf. Learn. Represent., 2018. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.</note>
</biblStruct>

<biblStruct coords="18,64.36,184.01,228.83,7.17;18,64.37,192.98,228.83,7.17;18,64.37,201.95,228.80,7.17;18,64.37,210.91,228.81,7.17;18,64.37,219.88,83.46,7.17" xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning visual predictive models of physics for playing billiards</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Fragkiadaki, P. Agrawal, S. Levine, and J. Malik, &quot;Learn- ing visual predictive models of physics for playing billiards,&quot; in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.</note>
</biblStruct>

<biblStruct coords="18,64.36,228.85,228.82,7.17;18,64.37,237.81,228.82,7.17;18,64.37,246.78,148.14,7.17" xml:id="b36">
	<analytic>
		<title level="a" type="main">What Can You Do with a Rock? Affordance Extraction via Word Embeddings</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wingate</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="1039" to="1045" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Fulda, D. Ricks, B. Murdoch, and D. Wingate, &quot;What can you do with a rock? Affordance extraction via word embeddings,&quot; in Proc. 26th Int. Joint Conf. Artif. Intell., 2017, pp. 1039-1045.</note>
</biblStruct>

<biblStruct coords="18,64.36,255.75,228.77,7.17;18,64.37,264.71,199.91,7.17" xml:id="b37">
	<analytic>
		<title level="a" type="main">Machine learning in digital games: a survey</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Galway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darryl</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-009-9112-y</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<title level="j" type="abbrev">Artif Intell Rev</title>
		<idno type="ISSN">0269-2821</idno>
		<idno type="ISSNe">1573-7462</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="161" />
			<date type="published" when="2008-04">2008</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Galway, D. Charles, and M. Black, &quot;Machine learning in digital games: A survey,&quot; Artif. Intell. Rev., vol. 29, no. 2, pp. 123-161, 2008.</note>
</biblStruct>

<biblStruct coords="18,64.36,273.68,228.83,7.17;18,64.37,282.64,92.33,7.17" xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016.</note>
</biblStruct>

<biblStruct coords="18,64.36,291.61,228.84,7.17;18,64.37,300.58,56.04,7.17" xml:id="b39">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Graves, G. Wayne, and I. Danihelka, &quot;Neural turing machines,&quot; 2014, arXiv:1410.5401.</note>
</biblStruct>

<biblStruct coords="18,64.36,309.54,228.81,7.17;18,64.37,318.51,179.73,7.17" xml:id="b40">
	<analytic>
		<title level="a" type="main">Human-Like Playtesting with Deep Learning</title>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">Freyr</forename><surname>Gudmundsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Eisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Poromaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nodet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Purmonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartlomiej</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Meurling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lele</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2018.8490442</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-08">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Gudmundsson et al., &quot;Human-like playtesting with deep learning,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2018, pp. 1-8.</note>
</biblStruct>

<biblStruct coords="18,64.36,327.48,228.84,7.17;18,64.37,336.44,228.83,7.17;18,64.37,345.41,228.83,7.17;18,64.37,354.37,49.83,7.17" xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-Agent Assisted Shortest Path Planning using Monte Carlo Tree Search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.2514/6.2023-2655.vid</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf</title>
				<meeting>Int. Conf. Neural Inf</meeting>
		<imprint>
			<publisher>American Institute of Aeronautics and Astronautics (AIAA)</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3338" to="3346" />
		</imprint>
	</monogr>
	<note type="raw_reference">X. Guo, S. Singh, H. Lee, R. L. Lewis, and X. Wang, &quot;Deep learn- ing for real-time Atari game play using offline Monte-Carlo tree search planning,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2014, pp. 3338-3346.</note>
</biblStruct>

<biblStruct coords="18,64.37,363.34,228.83,7.17;18,64.37,372.31,228.83,7.17;18,64.37,381.27,49.83,7.17" xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient DNNs</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1379" to="1387" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Guo, A. Yao, and Y. Chen, &quot;Dynamic network surgery for ef- ficient DNNs,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 1379-1387.</note>
</biblStruct>

<biblStruct coords="18,64.36,390.24,228.82,7.17;18,64.37,399.21,188.30,7.17" xml:id="b43">
	<analytic>
		<title level="a" type="main">Game Engine Learning from Video</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Riedl</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="3707" to="3713" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Guzdial, B. Li, and M. O. Riedl, &quot;Game engine learning from video,&quot; in Proc. Int. Joint Conf. Artif. Intell., 2017, pp. 3707-3713.</note>
</biblStruct>

<biblStruct coords="18,64.36,408.17,228.83,7.17;18,64.37,417.14,228.82,7.17;18,64.37,426.11,228.80,7.17;18,64.37,435.07,228.79,7.17;18,64.37,444.04,69.76,7.17" xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2450" to="2462" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ha and J. Schmidhuber, &quot;Recurrent world models facilitate policy evolution,&quot; in Advances in Neural Information Processing Systems 31, S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa- Bianchi and R. Garnett, Eds. New York, NY, USA: Curran Associates, 2018, pp. 2450-2462.</note>
</biblStruct>

<biblStruct coords="18,64.36,453.00,228.82,7.17;18,64.37,461.97,105.84,7.17" xml:id="b45">
	<analytic>
		<title level="a" type="main">Double q-learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">V</forename><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf</title>
				<meeting>Int. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. V. Hasselt, &quot;Double q-learning,&quot; in Proc. Int. Conf. Neural Inf. Pro- cess. Syst., 2010, pp. 2613-2621.</note>
</biblStruct>

<biblStruct coords="18,64.36,470.94,228.82,7.17;18,64.37,479.90,228.83,7.17;18,64.37,488.87,41.86,7.17" xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimal Brain Surgeon and general network pruning</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Wolff</surname></persName>
		</author>
		<idno type="DOI">10.1109/icnn.1993.298572</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="164" to="164" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. Hassibi et al., &quot;Second order derivatives for network pruning: Opti- mal brain surgeon,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 1993, pp. 164-164.</note>
</biblStruct>

<biblStruct coords="18,64.36,497.84,228.81,7.17;18,64.37,506.80,228.82,7.17;18,64.37,515.77,176.96,7.17" xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic Content Generation in the &lt;i&gt;Galactic Arms Race&lt;/i&gt; Video Game</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Hastings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.1109/tciaig.2009.2038365</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<title level="j" type="abbrev">IEEE Trans. Comput. Intell. AI Games</title>
		<idno type="ISSN">1943-068X</idno>
		<idno type="ISSNe">1943-0698</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="263" />
			<date type="published" when="2009-12">Dec. 2009</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">E. J. Hastings, R. K. Guha, and K. O. Stanley, &quot;Automatic content generation in the galactic arms race video game,&quot; IEEE Trans. Comput. Intell. AI Games, vol. 1, no. 4, pp. 245-263, Dec. 2009.</note>
</biblStruct>

<biblStruct coords="18,64.36,524.73,228.83,7.17;18,64.37,533.70,228.82,7.17;18,64.37,542.67,176.96,7.17" xml:id="b48">
	<analytic>
		<title level="a" type="main">A Neuroevolution Approach to General Atari Game Playing</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.1109/tciaig.2013.2294713</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<title level="j" type="abbrev">IEEE Trans. Comput. Intell. AI Games</title>
		<idno type="ISSN">1943-068X</idno>
		<idno type="ISSNe">1943-0698</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="366" />
			<date type="published" when="2014-12">Dec. 2014</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Hausknecht, J. Lehman, R. Miikkulainen, and P. Stone, &quot;A neuroevo- lution approach to general Atari game playing,&quot; IEEE Trans. Comput. Intell. AI Games, vol. 6, no. 4, pp. 355-366, Dec. 2014.</note>
</biblStruct>

<biblStruct coords="18,64.36,551.63,228.82,7.17;18,64.37,560.60,228.84,7.17;18,64.37,569.57,228.82,7.17;18,64.37,578.53,17.94,7.17" xml:id="b49">
	<analytic>
		<title level="a" type="main">Half field offense: An environment for multiagent learning and ad hoc teamwork</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mupparaju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kalyanakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAMAS Adaptive Learn. Agents Workshop</title>
				<meeting>AAMAS Adaptive Learn. Agents Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Hausknecht, P. Mupparaju, S. Subramanian, S. Kalyanakrishnan, and P. Stone, &quot;Half field offense: An environment for multiagent learning and ad hoc teamwork,&quot; in Proc. AAMAS Adaptive Learn. Agents Workshop, 2016.</note>
</biblStruct>

<biblStruct coords="18,64.36,587.50,228.83,7.17;18,64.37,596.47,228.83,7.17;18,64.37,605.43,117.29,7.17" xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep recurrent q-learning for partially observable MDPs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Fall Symp. Sequential Decis. Making Intell. Agents</title>
				<meeting>AAAI Fall Symp. Sequential Decis. Making Intell. Agents</meeting>
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Hausknecht and P. Stone, &quot;Deep recurrent q-learning for partially observable MDPs,&quot; in Proc. AAAI Fall Symp. Sequential Decis. Making Intell. Agents, Nov. 2015, pp. 29-37.</note>
</biblStruct>

<biblStruct coords="18,64.36,614.40,228.83,7.17;18,64.37,623.36,228.81,7.17;18,64.37,632.34,228.77,7.17;18,64.37,641.30,74.15,7.17" xml:id="b51">
	<analytic>
		<title level="a" type="main">UAS Conflict Resolution in Continuous Action Space Using Deep Reinforcement Learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.2514/6.2020-2909.vid</idno>
		<ptr target="http://www.cs.utexas.edu/users/ai-lab/?hausknecht:iclr16" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<publisher>American Institute of Aeronautics and Astronautics (AIAA)</publisher>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Hausknecht and P. Stone, &quot;Deep reinforcement learning in parameterized action space,&quot; in Proc. Int. Conf. Learn. Repre- sent., May 2016. [Online]. Available: http://www.cs.utexas.edu/users/ai- lab/?hausknecht:iclr16.</note>
</biblStruct>

<biblStruct coords="18,64.36,650.27,228.82,7.17;18,64.37,659.23,228.80,7.17;18,64.37,668.20,114.63,7.17" xml:id="b52">
	<analytic>
		<title level="a" type="main">On-policy vs. off-policy updates for deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI Workshop: Deep Reinforcement Learn</title>
				<meeting>IJCAI Workshop: Deep Reinforcement Learn</meeting>
		<imprint>
			<publisher>Frontiers Challenges</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Hausknecht and P. Stone, &quot;On-policy vs. off-policy updates for deep reinforcement learning,&quot; in Proc. IJCAI Workshop: Deep Reinforcement Learn.: Frontiers Challenges, 2016.</note>
</biblStruct>

<biblStruct coords="18,64.36,677.17,228.84,7.17;18,64.37,686.13,228.83,7.17;18,64.37,695.09,71.81,7.17" xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with a Natural Language Action Space</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1153</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1621" to="1630" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. He et al., &quot;Deep reinforcement learning with a natural language action space,&quot; in Proc. 54th Annu. Meeting Assoc. Comput. Linguistics, 2016, vol. 1, pp. 1621-1630.</note>
</biblStruct>

<biblStruct coords="18,64.36,704.07,228.82,7.17;18,64.37,713.03,228.83,7.17;18,64.37,722.00,41.86,7.17" xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. He, X. Zhang, S. Ren, and J. Sun, &quot;Deep residual learning for image recognition,&quot; in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770-778.</note>
</biblStruct>

<biblStruct coords="18,327.37,67.45,228.82,7.17;18,327.38,76.42,133.91,7.17" xml:id="b55">
	<analytic>
		<title level="a" type="main">Rainbow: Combining Improvements in Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v32i1.11796</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018-04-29">2018</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Hessel et al., &quot;Rainbow: Combining improvements in deep rein- forcement learning,&quot; in Proc. AAAI, 2018.</note>
</biblStruct>

<biblStruct coords="18,327.37,85.39,228.84,7.17;18,327.38,94.35,149.68,7.17" xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep Q-learning From Demonstrations</title>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Sendonaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrunas</forename><surname>Gruslys</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v32i1.11757</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3223" to="3230" />
			<date type="published" when="2018-04-29">2018</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Hester et al., &quot;Deep q-learning from demonstrations,&quot; in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 3223-3230.</note>
</biblStruct>

<biblStruct coords="18,327.37,103.32,228.83,7.17;18,327.38,112.28,159.20,7.17" xml:id="b57">
	<analytic>
		<title level="a" type="main">A new design for a Turing Test for Bots</title>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Hingston</surname></persName>
		</author>
		<idno type="DOI">10.1109/itw.2010.5593336</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games</title>
				<meeting>the 2010 IEEE Conference on Computational Intelligence and Games</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-08">2010</date>
			<biblScope unit="page" from="345" to="350" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Hingston, &quot;A new design for a turing test for Bots,&quot; in Proc. IEEE Symp. Comput. Intell. Games, 2010, pp. 345-350.</note>
</biblStruct>

<biblStruct coords="18,327.37,121.25,228.82,7.17;18,327.38,130.22,103.48,7.17" xml:id="b58">
	<monogr>
		<title level="m" type="main">Believable Bots</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hingston</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-32323-2</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Berlin Heidelberg</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Hingston, Believable Bots: Can Computers Play Like People? New York, NY, USA: Springer, 2012.</note>
</biblStruct>

<biblStruct coords="18,327.38,139.18,228.81,7.17;18,327.38,148.15,228.82,7.17;18,327.38,157.12,228.81,7.17;18,327.38,166.08,184.65,7.17" xml:id="b59">
	<analytic>
		<title level="a" type="main">Evolving personas for player decision modeling</title>
		<author>
			<persName><forename type="first">Christoffer</forename><surname>Holmgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Liapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2014.6932911</idno>
		<ptr target="http://www.fdg2014.org/papers/fdg2014\_poster\_05.pdf" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computational Intelligence and Games</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-08">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Holmgård, A. Liapis, J. Togelius, and G. N. Yannakakis, &quot;Gen- erative agents for player decision modeling in games,&quot; in Proc. 9th Int. Conf. Found. Digit. Games, 2014. [Online]. Available: http://www.fdg2014.org/papers/fdg2014\_poster\_05.pdf</note>
</biblStruct>

<biblStruct coords="18,327.37,175.05,228.82,7.17;18,327.38,184.01,228.81,7.17;18,327.38,192.98,228.82,7.17;18,327.38,201.95,81.47,7.17" xml:id="b60">
	<analytic>
		<title level="a" type="main">Distributed prioritized experience replay</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Horgan et al., &quot;Distributed prioritized experience replay,&quot; in Proc. Int. Conf. Learn. Represent., 2018. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model</note>
</biblStruct>

<biblStruct coords="18,327.37,210.91,228.83,7.17;18,327.38,219.88,228.83,7.17;18,327.38,228.31,183.17,7.71" xml:id="b61">
	<monogr>
		<title level="m" type="main">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5 MB model size</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, &quot;SqueezeNet: AlexNet-level accuracy with 50x fewer parame- ters and &lt; 0.5 MB model size,&quot; 2016, arXiv:1602.07360.</note>
</biblStruct>

<biblStruct coords="18,327.37,237.81,228.84,7.17;18,327.38,246.78,228.81,7.17;18,327.38,255.75,228.82,7.17;18,327.38,264.71,83.46,7.17" xml:id="b62">
	<analytic>
		<title level="a" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Jaderberg et al., &quot;Reinforcement learning with unsupervised auxil- iary tasks,&quot; in Proc. Int. Conf. Learn. Represent., 2017. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model.</note>
</biblStruct>

<biblStruct coords="18,327.37,273.68,228.85,7.17;18,327.38,282.64,228.82,7.17;18,327.38,291.61,204.44,7.17" xml:id="b63">
	<analytic>
		<title level="a" type="main">EvoCommander: A Novel Game Based on Evolving and Switching Between Artificial Brains</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jallov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<idno type="DOI">10.1109/tciaig.2016.2535416</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<title level="j" type="abbrev">IEEE Trans. Comput. Intell. AI Games</title>
		<idno type="ISSN">1943-068X</idno>
		<idno type="ISSNe">1943-0698</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="191" />
			<date type="published" when="2017-06">Jun. 2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Jallov, S. Risi, and J. Togelius, &quot;EvoCommander: A novel game based on evolving and switching between artificial brains,&quot; IEEE Trans. Comput. Intell. AI Games, vol. 9, no. 2, pp. 181-191, Jun. 2017.</note>
</biblStruct>

<biblStruct coords="18,327.37,300.58,228.83,7.17;18,327.38,309.54,228.84,7.17;18,327.38,318.51,111.05,7.17" xml:id="b64">
	<analytic>
		<title level="a" type="main">The Malmo platform for artificial intelligence experimentation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bignell</surname></persName>
		</author>
		<idno type="DOI">10.24225/kjai.2016.4.1.</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
				<meeting>Int. Joint Conf</meeting>
		<imprint>
			<publisher>Korean Artificial Intelligence</publisher>
			<date type="published" when="2016-03-15">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="4246" to="4247" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Johnson, K. Hofmann, T. Hutton, and D. Bignell, &quot;The Malmo plat- form for artificial intelligence experimentation,&quot; in Proc. Int. Joint Conf. Artif. Intell., 2016, pp. 4246-4247.</note>
</biblStruct>

<biblStruct coords="18,327.37,327.48,228.85,7.17;18,327.38,336.44,228.84,7.17;18,327.38,345.41,41.87,7.17" xml:id="b65">
	<analytic>
		<title level="a" type="main">Online Evolution for Multi-action Adversarial Games</title>
		<author>
			<persName><forename type="first">Niels</forename><surname>Justesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Mahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-31204-0_38</idno>
	</analytic>
	<monogr>
		<title level="m">Applications of Evolutionary Computation</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="590" to="603" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Justesen, T. Mahlmann, and J. Togelius, &quot;Online evolution for multi- action adversarial games,&quot; in Proc. Eur. Conf. Appl. Evol. Comput., 2016, pp. 590-603.</note>
</biblStruct>

<biblStruct coords="18,327.37,354.37,228.84,7.17;18,327.38,363.34,228.83,7.17;18,327.38,372.31,61.79,7.17" xml:id="b66">
	<analytic>
		<title level="a" type="main">Continual online evolutionary planning for in-game build order adaptation in StarCraft</title>
		<author>
			<persName><forename type="first">Niels</forename><surname>Justesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3071178.3071210</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Justesen and S. Risi, &quot;Continual online evolution for in-game build order adaptation in StarCraft,&quot; in Proc. Genetic Evol. Comput. Conf., 2017, pp. 187-194.</note>
</biblStruct>

<biblStruct coords="18,327.37,381.27,228.85,7.17;18,327.38,390.24,228.81,7.17;18,327.38,399.21,61.79,7.17" xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning macromanagement in starcraft from replays using deep learning</title>
		<author>
			<persName><forename type="first">Niels</forename><surname>Justesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2017.8080430</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Justesen and S. Risi, &quot;Learning macromanagement in StarCraft from replays using deep learning,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 162-169.</note>
</biblStruct>

<biblStruct coords="18,327.37,408.17,228.81,7.17;18,327.38,417.14,228.83,7.17;18,327.38,426.11,228.82,7.18;18,327.38,435.08,76.54,7.17" xml:id="b68">
	<analytic>
		<title level="a" type="main">Illuminating generalization in deep reinforcement learning through procedural level generation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Justesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Torrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Risi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS Workshop Deep Reinforcement Learn</title>
				<meeting>NeurIPS Workshop Deep Reinforcement Learn</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius, and S. Risi, &quot;Illuminating generalization in deep reinforcement learning through procedural level generation.&quot; in Proc. NeurIPS Workshop Deep Rein- forcement Learn., 2018.</note>
</biblStruct>

<biblStruct coords="18,327.37,444.04,228.85,7.17;18,327.38,453.00,228.84,7.17;18,327.38,461.98,49.84,7.17" xml:id="b69">
	<analytic>
		<title level="a" type="main">Schema networks: Zero-shot transfer with a generative causal model of intuitive physics</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1809" to="1818" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Kansky et al., &quot;Schema networks: Zero-shot transfer with a generative causal model of intuitive physics,&quot; in Proc. Int. Conf. Mach. Learn., 2017, pp. 1809-1818.</note>
</biblStruct>

<biblStruct coords="18,327.37,470.94,228.84,7.17;18,327.38,479.90,183.44,7.17" xml:id="b70">
	<monogr>
		<title level="m" type="main">Beating Atari with natural language guided reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sosa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05539</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Kaplan, C. Sauer, and A. Sosa, &quot;Beating Atari with natural language guided reinforcement learning,&quot; 2017, arXiv:1704.05539.</note>
</biblStruct>

<biblStruct coords="18,327.37,488.87,228.86,7.17;18,327.38,497.84,228.83,7.17;18,327.38,506.81,83.15,7.17" xml:id="b71">
	<analytic>
		<title level="a" type="main">Multi-task learning in Atari video games with emergent tangled program graphs</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><forename type="middle">I</forename><surname>Heywood</surname></persName>
		</author>
		<idno type="DOI">10.1145/3071178.3071303</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Kelly and M. I. Heywood, &quot;Multi-task learning in Atari video games with emergent tangled program graphs,&quot; in Proc. Genetic Evol. Comput. Conf., 2017, pp. 195-202.</note>
</biblStruct>

<biblStruct coords="18,327.37,515.77,228.81,7.17;18,327.38,524.73,228.83,7.17;18,327.38,533.71,221.02,7.17" xml:id="b72">
	<analytic>
		<title level="a" type="main">ViZDoom: A Doom-based AI research platform for visual reinforcement learning</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Jaskowski</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2016.7860433</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-09">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaśkowski, &quot;ViZ- Doom: A doom-based AI research platform for visual reinforcement learning,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2016, pp. 1-8.</note>
</biblStruct>

<biblStruct coords="18,327.37,542.67,228.84,7.17;18,327.38,551.63,223.13,7.17" xml:id="b73">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1611835114</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<title level="j" type="abbrev">Proc. Natl. Acad. Sci. U.S.A.</title>
		<idno type="ISSN">0027-8424</idno>
		<idno type="ISSNe">1091-6490</idno>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017-03-14">2017</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Kirkpatrick et al., &quot;Overcoming catastrophic forgetting in neural networks,&quot; Proc. Nat. Acad. Sci. USA, vol. 114, pp. 3521-3526, 2017.</note>
</biblStruct>

<biblStruct coords="18,327.37,560.61,228.83,7.17;18,327.38,569.57,228.82,7.17;18,327.38,578.53,87.91,7.17" xml:id="b74">
	<analytic>
		<title level="a" type="main">Text-based adventures of the golovin AI agent</title>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Kostka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslaw</forename><surname>Kwiecieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Rychlikowski</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2017.8080433</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. Kostka, J. Kwiecieli, J. Kowalski, and P. Rychlikowski, &quot;Text-based adventures of the Golovin AI agent,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 181-188.</note>
</biblStruct>

<biblStruct coords="18,327.37,587.50,228.82,7.17;18,327.38,596.47,228.82,7.17;18,327.38,605.44,202.71,7.17" xml:id="b75">
	<analytic>
		<title level="a" type="main">Evolving large-scale neural networks for vision-based reinforcement learning</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2463372.2463509</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th annual conference on Genetic and evolutionary computation</title>
				<meeting>the 15th annual conference on Genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-07-06">2013</date>
			<biblScope unit="page" from="1061" to="1068" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Koutník, G. Cuccu, J. Schmidhuber, and F. Gomez, &quot;Evolving large- scale neural networks for vision-based reinforcement learning,&quot; in Proc. 15th Annu. Conf. Genetic Evol. Comput., 2013, pp. 1061-1068.</note>
</biblStruct>

<biblStruct coords="18,327.37,614.40,228.83,7.17;18,327.38,623.36,228.83,7.17;18,327.38,632.34,228.84,7.17;18,327.38,641.30,49.84,7.17" xml:id="b76">
	<analytic>
		<title level="a" type="main">Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3675" to="3683" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, &quot;Hierar- chical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 3675-3683.</note>
</biblStruct>

<biblStruct coords="18,327.37,650.26,228.75,7.17;18,327.38,659.23,228.83,7.17;18,327.38,668.20,69.77,7.17" xml:id="b77">
	<analytic>
		<title level="a" type="main">General Video Game AI: Learning from screen capture</title>
		<author>
			<persName><forename type="first">Kamolwan</forename><surname>Kunanusont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Perez-Liebana</surname></persName>
		</author>
		<idno type="DOI">10.1109/cec.2017.7969556</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Congress on Evolutionary Computation (CEC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-06">2017</date>
			<biblScope unit="page" from="2078" to="2085" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Kunanusont, S. M. Lucas, and D. Perez-Liebana, &quot;General video game AI: Learning from screen capture,&quot; in Proc. IEEE Congr. Evol. Comput., 2017, pp. 2078-2085.</note>
</biblStruct>

<biblStruct coords="18,327.37,677.16,228.85,7.17;18,327.38,686.13,228.02,7.17" xml:id="b78">
	<analytic>
		<title level="a" type="main">Playing FPS Games with Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><forename type="middle">Singh</forename><surname>Chaplot</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v31i1.10827</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2140" to="2146" />
			<date type="published" when="2017-02-13">2017</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Lample and D. S. Chaplot, &quot;Playing FPS games with deep reinforce- ment learning,&quot; in Proc. AAAI Conf. Artif. Intell., 2017, pp. 2140-2146.</note>
</biblStruct>

<biblStruct coords="18,327.38,695.09,228.81,7.17;18,327.38,704.07,168.52,7.17" xml:id="b79">
	<analytic>
		<title level="a" type="main">Modèles connexionnistes de l&apos;apprentissage</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Françoise</forename><surname>Fogelman-Soulié</surname></persName>
		</author>
		<idno type="DOI">10.3406/intel.1987.1804</idno>
	</analytic>
	<monogr>
		<title level="j">Intellectica. Revue de l&apos;Association pour la Recherche Cognitive</title>
		<title level="j" type="abbrev">intel</title>
		<idno type="ISSN">0769-4113</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="143" />
			<date type="published" when="1987">1987</date>
			<publisher>PERSEE Program</publisher>
		</imprint>
		<respStmt>
			<orgName>Pierre Marie Curie Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note type="raw_reference">Y. Le Cun, &quot;Modèles connexionnistes de l&apos;apprentissage,&quot; Ph.D. disser- tation, Pierre Marie Curie Univ., Paris, France, 1987.</note>
</biblStruct>

<biblStruct coords="18,327.37,713.03,228.84,7.17;18,327.38,721.99,93.67,7.17" xml:id="b80">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<title level="j" type="abbrev">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<idno type="ISSNe">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05-27">2015</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. LeCun, Y. Bengio, and G. Hinton, &quot;Deep learning,&quot; Nature, vol. 521, no. 7553, pp. 436-444, 2015.</note>
</biblStruct>

<biblStruct coords="19,60.16,67.45,228.82,7.17;19,60.16,76.42,202.20,7.17" xml:id="b81">
	<analytic>
		<title level="a" type="main">Backpropagation Applied to Handwritten Zip Code Recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.4.541</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12">1989</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. LeCun et al., &quot;Backpropagation applied to handwritten zip code recognition,&quot; Neural Comput., vol. 1, no. 4, pp. 541-551, 1989.</note>
</biblStruct>

<biblStruct coords="19,60.15,85.39,228.84,7.17;19,60.16,94.35,197.76,7.17" xml:id="b82">
	<analytic>
		<title level="a" type="main">Universal Intelligence: A Definition of Machine Intelligence</title>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-007-9079-x</idno>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<title level="j" type="abbrev">Minds &amp; Machines</title>
		<idno type="ISSN">0924-6495</idno>
		<idno type="ISSNe">1572-8641</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="444" />
			<date type="published" when="2007-11-10">2007</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Legg and M. Hutter, &quot;Universal intelligence: A definition of machine intelligence,&quot; Minds Mach., vol. 17, no. 4, pp. 391-444, 2007.</note>
</biblStruct>

<biblStruct coords="19,60.15,103.32,228.83,7.17;19,60.16,112.28,228.81,7.17;19,60.16,121.25,61.78,7.17" xml:id="b83">
	<analytic>
		<title level="a" type="main">Beyond Open-endedness: Quantifying Impressiveness</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.7551/978-0-262-31050-5-ch011</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Life 13</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Lehman and K. O. Stanley, &quot;Exploiting open-endedness to solve prob- lems through the search for novelty,&quot; in Proc. 11th Int. Conf. Artif. Life, 2008, pp. 329-336.</note>
</biblStruct>

<biblStruct coords="19,60.15,130.22,228.81,7.17;19,60.16,139.18,228.81,7.17;19,60.16,148.15,199.43,7.17" xml:id="b84">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning in sequential social dilemmas</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Marecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Conf. Auton. Agents Multiagent Syst</title>
				<meeting>16th Conf. Auton. Agents Multiagent Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel, &quot;Multi- agent reinforcement learning in sequential social dilemmas,&quot; in Proc. 16th Conf. Auton. Agents Multiagent Syst., 2017, pp. 464-473.</note>
</biblStruct>

<biblStruct coords="19,60.15,157.12,228.85,7.17;19,60.16,166.08,228.83,7.17" xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning physical intuition of block towers by example</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Lerer, S. Gross, and R. Fergus, &quot;Learning physical intuition of block towers by example,&quot; in Proc. Int. Conf. Mach. Learn., 2016, pp. 430-438.</note>
</biblStruct>

<biblStruct coords="19,60.15,175.05,228.83,7.17;19,60.16,184.01,60.03,7.17" xml:id="b86">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning in urban computing</title>
		<author>
			<persName coords=""><forename type="first">Yexin</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.14711/thesis-991012936267103412</idno>
		<idno type="arXiv">arXiv:1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The Hong Kong University of Science and Technology Library</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Li, &quot;Deep reinforcement learning: An overview,&quot; 2017, arXiv:1701.07274.</note>
</biblStruct>

<biblStruct coords="19,60.15,192.98,228.84,7.17;19,60.16,201.95,155.20,7.17" xml:id="b87">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. P. Lillicrap et al., &quot;Continuous control with deep reinforcement learn- ing,&quot; in Proc. Int. Conf. Learn. Represent., 2016.</note>
</biblStruct>

<biblStruct coords="19,60.15,210.91,228.81,7.17;19,60.16,219.88,228.83,7.17;19,60.16,228.85,73.58,7.17" xml:id="b88">
	<analytic>
		<title level="a" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, School Comput. Sci</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">L.-J. Lin, &quot;Reinforcement learning for robots using neural networks,&quot; Ph.D. dissertation, School Comput. Sci., Carnegie Mellon Univ., Pitts- burgh, PA, USA, 1993.</note>
</biblStruct>

<biblStruct coords="19,60.15,237.81,228.83,7.17;19,60.16,246.78,201.02,7.17" xml:id="b89">
	<analytic>
		<title level="a" type="main">Evolutionary computation and games</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kendall</surname></persName>
		</author>
		<idno type="DOI">10.1109/mci.2006.1597057</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<title level="j" type="abbrev">IEEE Comput. Intell. Mag.</title>
		<idno type="ISSN">1556-603X</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2006-02">Feb. 2006</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. M. Lucas and G. Kendall, &quot;Evolutionary computation and games,&quot; IEEE Comput. Intell. Mag., vol. 1, no. 1, pp. 10-18, Feb. 2006.</note>
</biblStruct>

<biblStruct coords="19,60.15,255.75,228.81,7.17;19,60.16,264.71,228.85,7.17;19,60.16,273.68,228.81,7.17;19,60.16,282.64,87.75,7.17" xml:id="b90">
	<analytic>
		<title level="a" type="main">Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents</title>
		<author>
			<persName><forename type="first">Marlos</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.5699</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<title level="j" type="abbrev">jair</title>
		<idno type="ISSNe">1076-9757</idno>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018-03-19">2018</date>
			<publisher>AI Access Foundation</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling, &quot;Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents,&quot; J. Artif. Intell. Res., vol. 61, pp. 523-562, 2018.</note>
</biblStruct>

<biblStruct coords="19,60.15,291.61,228.82,7.17;19,60.16,300.58,228.57,7.17" xml:id="b91">
	<analytic>
		<title level="a" type="main">Teacher–Student Curriculum Learning</title>
		<author>
			<persName><forename type="first">Tambet</forename><surname>Matiisen</surname></persName>
			<idno type="ORCID">0000-0002-6815-1580</idno>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2019.2934906</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw. Learning Syst.</title>
		<idno type="ISSN">2162-237X</idno>
		<idno type="ISSNe">2162-2388</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3732" to="3740" />
			<date type="published" when="2017">2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Matiisen, A. Oliver, T. Cohen, and J. Schulman, &quot;Teacher-student curriculum learning,&quot; in Proc. Deep Reinforcement Learn. Symp., 2017.</note>
</biblStruct>

<biblStruct coords="19,60.15,309.54,228.80,7.17;19,60.16,318.51,228.83,7.17;19,60.16,327.48,228.82,7.17;19,60.16,336.44,130.20,7.17" xml:id="b92">
	<analytic>
		<title level="a" type="main">Computational intelligence in games</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cornelius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">V</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence: Principles and Practice</title>
				<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Intell. Soc</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="155" to="191" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Miikkulainen, B. D. Bryant, R. Cornelius, I. V. Karpov, K. O. Stanley, and C. H. Yong, &quot;Computational intelligence in games,&quot; in Computa- tional Intelligence: Principles and Practice. Piscataway, NJ, USA: IEEE Comput. Intell. Soc., 2006, pp. 155-191.</note>
</biblStruct>

<biblStruct coords="19,60.15,345.41,228.83,7.17;19,60.16,354.37,199.15,7.17" xml:id="b93">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Mikolov, K. Chen, G. Corrado, and J. Dean, &quot;Efficient estimation of word representations in vector space,&quot; 2013, arXiv:1301.3781.</note>
</biblStruct>

<biblStruct coords="19,60.15,363.34,228.84,7.17;19,60.16,372.31,228.80,7.17;19,60.16,381.27,228.81,7.17;19,60.16,390.24,81.47,7.17" xml:id="b94">
	<analytic>
		<title level="a" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Mirowski et al., &quot;Learning to navigate in complex environ- ments,&quot; in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Avail- able: https://sites.google.com/site/representationlearning2014/program- details/publication-model</note>
</biblStruct>

<biblStruct coords="19,60.16,399.21,228.82,7.17;19,60.16,408.17,176.32,7.17" xml:id="b95">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
	<note type="raw_reference">V. Mnih et al., &quot;Asynchronous methods for deep reinforcement learning,&quot; in Proc. Int. Conf. Mach. Learn., 2016, pp. 1928-1937.</note>
</biblStruct>

<biblStruct coords="19,60.16,417.14,228.82,7.17;19,60.16,426.11,228.83,7.17" xml:id="b96">
	<analytic>
		<title level="a" type="main">Playing Atari with deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst., Deep Learn. Workshop</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst., Deep Learn. Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. Mnih et al., &quot;Playing Atari with deep reinforcement learning,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., Deep Learn. Workshop, 2013.</note>
</biblStruct>

<biblStruct coords="19,60.15,435.07,228.83,7.17;19,60.16,444.04,166.60,7.17" xml:id="b97">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<title level="j" type="abbrev">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<idno type="ISSNe">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02-25">2015</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">V. Mnih et al., &quot;Human-level control through deep reinforcement learn- ing,&quot; Nature, vol. 518, no. 7540, pp. 529-533, 2015.</note>
</biblStruct>

<biblStruct coords="19,60.15,453.00,228.80,7.17;19,60.16,461.97,228.82,7.17;19,60.16,470.94,206.86,7.17" xml:id="b98">
	<analytic>
		<title level="a" type="main">Dagstuhl Manifesto</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muñoz-Avila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Congdon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kendall</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00287-013-0690-3</idno>
	</analytic>
	<monogr>
		<title level="j">Informatik-Spektrum</title>
		<title level="j" type="abbrev">Informatik Spektrum</title>
		<idno type="ISSN">0170-6012</idno>
		<idno type="ISSNe">1432-122X</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2013-02-22">2013</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Muñoz-Avila, C. Bauckhage, M. Bida, C. B. Congdon, and G. Kendall, &quot;Learning and game AI,&quot; in Dagstuhl Follow-Ups, vol. 6. Wadern, Ger- many: Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2013.</note>
</biblStruct>

<biblStruct coords="19,60.15,479.90,228.81,7.17;19,60.16,488.87,113.04,7.17" xml:id="b99">
	<monogr>
		<title level="m" type="main">Massively parallel methods for deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04296</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Nair et al., &quot;Massively parallel methods for deep reinforcement learning,&quot; 2015, arXiv:1507.04296.</note>
</biblStruct>

<biblStruct coords="19,60.15,497.84,228.83,7.17;19,60.16,506.80,228.81,7.17;19,60.16,515.77,210.29,7.17" xml:id="b100">
	<analytic>
		<title level="a" type="main">Language Understanding for Text-based Games using Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Narasimhan, T. D. Kulkarni, and R. Barzilay, &quot;Language understand- ing for textbased games using deep reinforcement learning,&quot; in Proc. Conf. Empirical Methods Natural Lang. Process., 2015, pp. 1-11.</note>
</biblStruct>

<biblStruct coords="19,60.15,524.73,228.82,7.17;19,60.16,533.70,228.83,7.17;19,60.16,542.67,142.77,7.17" xml:id="b101">
	<analytic>
		<title level="a" type="main">Control of memory, active perception, and action in Minecraft</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Int. Conf. Mach. Learn</title>
				<meeting>33rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Oh, V. Chockalingam, S. Singh, and H. Lee, &quot;Control of memory, active perception, and action in Minecraft,&quot; in Proc. 33rd Int. Conf. Mach. Learn., 2016, vol. 48, pp. 2790-2799.</note>
</biblStruct>

<biblStruct coords="19,60.15,551.63,228.83,7.17;19,60.16,560.61,228.83,7.17;19,60.16,569.57,154.17,7.17" xml:id="b102">
	<analytic>
		<title level="a" type="main">How to Beat Atari Games by Using Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/11440.003.0011</idno>
	</analytic>
	<monogr>
		<title level="m">How Smart Machines Think</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, &quot;Action-conditional video prediction using deep networks in Atari games,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2015, pp. 2863-2871.</note>
</biblStruct>

<biblStruct coords="19,60.15,578.54,228.82,7.17;19,60.16,587.50,228.81,7.17;19,60.16,596.47,173.69,7.17" xml:id="b103">
	<analytic>
		<title level="a" type="main">The Combinatorial Multi-Armed Bandit Problem and Its Application to Real-Time Strategy Games</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ontanón</surname></persName>
		</author>
		<idno type="DOI">10.1609/aiide.v9i1.12681</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<title level="j" type="abbrev">AIIDE</title>
		<idno type="ISSN">2326-909X</idno>
		<idno type="ISSNe">2334-0924</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="64" />
			<date type="published" when="2013">2013</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Ontanón, &quot;The combinatorial multi-armed bandit problem and its application to real-time strategy games,&quot; in Proc. 9th AAAI Conf. Artif. Intell. Interact. Digit. Entertainment, 2013, pp. 58-64.</note>
</biblStruct>

<biblStruct coords="19,60.15,605.44,228.75,7.17;19,60.16,614.40,228.82,7.17;19,60.16,623.36,77.72,7.17" xml:id="b104">
	<analytic>
		<title level="a" type="main">Imitating human playing styles in Super Mario Bros</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noor</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.entcom.2012.10.001</idno>
	</analytic>
	<monogr>
		<title level="j">Entertainment Computing</title>
		<title level="j" type="abbrev">Entertainment Computing</title>
		<idno type="ISSN">1875-9521</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2013-04">2013</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Ortega, N. Shaker, J. Togelius, and G. N. Yannakakis, &quot;Imitating human playing styles in Super Mario Bros,&quot; Entertainment Comput., vol. 4, no. 2, pp. 93-104, 2013.</note>
</biblStruct>

<biblStruct coords="19,60.15,632.34,228.83,7.17;19,60.16,641.30,228.81,7.17;19,60.16,650.27,69.76,7.17" xml:id="b105">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped DQN</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
	<note type="raw_reference">I. Osband, C. Blundell, A. Pritzel, and B. Van Roy, &quot;Deep exploration via bootstrapped DQN,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 4026-4034.</note>
</biblStruct>

<biblStruct coords="19,60.15,659.23,228.84,7.17;19,60.16,668.20,228.83,7.17;19,60.16,677.17,116.80,7.17" xml:id="b106">
	<analytic>
		<title level="a" type="main">Countbased exploration with neural density models</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. Conf. Mach. Learn</title>
				<meeting>34th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2721" to="2730" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos, &quot;Count- based exploration with neural density models,&quot; in Proc. 34th Int. Conf. Mach. Learn., 2017, pp. 2721-2730.</note>
</biblStruct>

<biblStruct coords="19,60.15,686.13,228.82,7.17;19,60.15,695.09,228.83,7.17;19,60.15,704.07,228.81,7.17;19,60.15,713.03,210.31,7.17;19,60.15,721.99,81.47,7.17" xml:id="b107">
	<analytic>
		<title level="a" type="main">Actor-mimic: Deep multitask and transfer reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. Parisotto, J. L. Ba, and R. Salakhutdinov, &quot;Actor-mimic: Deep multitask and transfer reinforcement learning,&quot; in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Available: https://sites.google.com/site/representationlearning2014/program- details/publication-model</note>
</biblStruct>

<biblStruct coords="19,323.16,67.45,228.83,7.17;19,323.16,76.42,228.83,7.17;19,323.16,85.39,70.64,7.17" xml:id="b108">
	<analytic>
		<title level="a" type="main">Neurovisual Control in the Quake II Environment</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><forename type="middle">D</forename><surname>Bryant</surname></persName>
		</author>
		<idno type="DOI">10.1109/tciaig.2012.2184109</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<title level="j" type="abbrev">IEEE Trans. Comput. Intell. AI Games</title>
		<idno type="ISSN">1943-068X</idno>
		<idno type="ISSNe">1943-0698</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="2012-03">Mar. 2012</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Parker and B. D. Bryant, &quot;Neurovisual control in the Quake II environment,&quot; IEEE Trans. Comput. Intell. AI Games, vol. 4, no. 1, pp. 44-54, Mar. 2012.</note>
</biblStruct>

<biblStruct coords="19,323.16,94.35,228.84,7.17;19,323.16,103.32,228.84,7.17;19,323.16,112.28,94.78,7.17" xml:id="b109">
	<analytic>
		<title level="a" type="main">Curiosity-Driven Exploration by Self-Supervised Prediction</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2017.70</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="2778" to="2787" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, &quot;Curiosity-driven exploration by self-supervised prediction,&quot; in Proc. Int. Conf. Mach. Learn., 2017, pp. 2778-2787.</note>
</biblStruct>

<biblStruct coords="19,323.16,121.25,228.83,7.17;19,323.16,130.22,187.82,7.17" xml:id="b110">
	<monogr>
		<title level="m" type="main">Multiagent bidirectionally-coordinated nets for learning to play StarCraft combat games</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10069</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Peng et al., &quot;Multiagent bidirectionally-coordinated nets for learning to play StarCraft combat games,&quot; 2017, arXiv:1703.10069.</note>
</biblStruct>

<biblStruct coords="19,323.16,139.18,228.84,7.17;19,323.16,148.15,145.35,7.17" xml:id="b111">
	<monogr>
		<title level="m" type="main">Observe and look further: Achieving consistent performance on Atari</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11593</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Pohlen et al., &quot;Observe and look further: Achieving consistent per- formance on Atari,&quot; 2018, arXiv:1805.11593.</note>
</biblStruct>

<biblStruct coords="19,323.15,157.12,228.85,7.17;19,323.16,166.08,228.82,7.17;19,323.16,175.05,195.69,7.17" xml:id="b112">
	<analytic>
		<title level="a" type="main">DLNE: A hybridization of deep learning and neuroevolution for visual control</title>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Precht</forename><surname>Poulsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Thorhauge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikkel</forename><forename type="middle">Hvilshj</forename><surname>Funch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2017.8080444</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. P. Poulsen, M. Thorhauge, M. Hvilshj, and S. Risi, &quot;DLNE: A hy- bridization of deep learning and neuroevolution for visual control,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2017, pp. 256-263.</note>
</biblStruct>

<biblStruct coords="19,323.16,184.01,228.84,7.17;19,323.16,192.98,228.83,7.17;19,323.16,201.95,228.81,7.17;19,323.16,210.91,123.78,7.17" xml:id="b113">
	<analytic>
		<title level="a" type="main">Combining Search-Based Procedural Content Generation and Social Gaming in the Petalz Video Game</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>D'ambrosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.1609/aiide.v8i1.12502</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<title level="j" type="abbrev">AIIDE</title>
		<idno type="ISSN">2326-909X</idno>
		<idno type="ISSNe">2334-0924</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="68" />
			<date type="published" when="2012">2012</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Risi, J. Lehman, D. B. D&apos;Ambrosio, R. Hall, and K. O. Stanley, &quot;Com- bining search-based procedural content generation and social gaming in the Petalz video game,&quot; in Proc. 8th AAAI Conf. Artif. Intell. Interact. Digit. Entertainment, 2012, pp. 63-68.</note>
</biblStruct>

<biblStruct coords="19,323.16,219.88,228.84,7.17;19,323.16,228.85,228.83,7.17;19,323.16,237.81,70.64,7.17" xml:id="b114">
	<analytic>
		<title level="a" type="main">Neuroevolution in Games: State of the Art and Open Challenges</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<idno type="DOI">10.1109/tciaig.2015.2494596</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<title level="j" type="abbrev">IEEE Trans. Comput. Intell. AI Games</title>
		<idno type="ISSN">1943-068X</idno>
		<idno type="ISSNe">1943-0698</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="41" />
			<date type="published" when="2017-03">Mar. 2017</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Risi and J. Togelius, &quot;Neuroevolution in games: State of the art and open challenges,&quot; IEEE Trans. Comput. Intell. AI Games, vol. 9, no. 1, pp. 25-41, Mar. 2017.</note>
</biblStruct>

<biblStruct coords="19,323.16,246.78,228.80,7.17;19,323.16,255.75,228.83,7.17;19,323.16,264.71,179.74,7.17" xml:id="b115">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for general video game AI</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rodriguez Torrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Perez-Liebana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Intell. Games</title>
				<meeting>IEEE Conf. Comput. Intell. Games</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Rodriguez Torrado, P. Bontrager, J. Togelius, J. Liu, and D. Perez- Liebana, &quot;Deep reinforcement learning for general video game AI,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2018, pp. 1-8.</note>
</biblStruct>

<biblStruct coords="19,323.16,273.68,228.81,7.17;19,323.16,282.64,228.83,7.17;19,323.16,291.61,228.80,7.17;19,323.16,300.58,53.82,7.17" xml:id="b116">
	<analytic>
		<title level="a" type="main">Parallel Distributed Processing</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/5236.001.0001</idno>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
				<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="45" to="76" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. E. Rumelhart et al., &quot;A general framework for parallel distributed processing,&quot; in Parallel Distributed Processing: Explorations in the Mi- crostructure of Cognition, vol. 1. Cambridge, MA, USA: MIT Press, 1986, pp. 45-76.</note>
</biblStruct>

<biblStruct coords="19,323.15,309.54,228.84,7.17;19,323.16,318.51,212.80,7.17" xml:id="b117">
	<analytic>
		<title level="a" type="main">Gaskinetic Theory.By Tamas Gombosi. Cambridge Univ. Press, Cambridge, UK, 1994. 297 pp., $59.95 Cloth/$29.95 paper</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<idno type="DOI">10.1006/icar.1996.5620</idno>
	</analytic>
	<monogr>
		<title level="j">Icarus</title>
		<title level="j" type="abbrev">Icarus</title>
		<idno type="ISSN">0019-1035</idno>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="230" to="231" />
			<date type="published" when="1994">1994</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
		<respStmt>
			<orgName>Univ. Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">G. A. Rummery and M. Niranjan, On-Line Q-Learning Using Connec- tionist Systems, vol. 37. Cambridge, U.K.: Univ. Cambridge, 1994.</note>
</biblStruct>

<biblStruct coords="19,323.15,327.48,228.83,7.17;19,323.16,336.44,228.79,7.17;19,323.16,345.41,200.55,7.17" xml:id="b118">
	<analytic>
		<title level="a" type="main">Figure 1—figure supplement 1. Sequence analysis of IS607 insertion sites.</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<idno type="DOI">10.7554/elife.39611.003</idno>
		<ptr target="https://sites.google.com/site/representationlearning2014/program-details/publication-model" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<publisher>eLife Sciences Publications, Ltd</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. A. Rusu et al., &quot;Policy distillation,&quot; in Proc. Int. Conf. Learn. Represent., 2016. [Online]. Available: https://sites.google.com/site/ representationlearning2014/program-details/publication-model</note>
</biblStruct>

<biblStruct coords="19,323.15,354.37,228.83,7.17;19,323.16,363.34,39.87,7.17" xml:id="b119">
	<monogr>
		<title level="m" type="main">Progressive neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. A. Rusu et al., &quot;Progressive neural networks,&quot; 2016, arXiv: 1606.04671.</note>
</biblStruct>

<biblStruct coords="19,323.15,372.31,228.82,7.17;19,323.16,381.27,228.83,7.17" xml:id="b120">
	<monogr>
		<title level="m" type="main">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Salimans, J. Ho, X. Chen, and I. Sutskever, &quot;Evolution strategies as a scalable alternative to reinforcement learning,&quot; 2017, arXiv:1703.03864.</note>
</biblStruct>

<biblStruct coords="19,323.15,390.23,228.83,7.17;19,323.16,399.21,228.84,7.17;19,323.16,408.17,25.92,7.17" xml:id="b121">
	<analytic>
		<title level="a" type="main">A video game description language for model-based or interactive learning</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2013.6633610</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computational Inteligence in Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-08">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Schaul, &quot;A video game description language for model-based or in- teractive learning,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2013, pp. 1-8.</note>
</biblStruct>

<biblStruct coords="19,323.15,417.14,228.83,7.17;19,323.16,426.11,164.94,7.17" xml:id="b122">
	<analytic>
		<title level="a" type="main">A Deep Reinforcement Learning Approach with Prioritized Experience Replay and Importance Factor for Makespan Minimization in Manufacturing</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="DOI">10.31390/gradschool_dissertations.5830</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<publisher>Louisiana State University Libraries</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Schaul, J. Quan, I. Antonoglou, and D. Silver, &quot;Prioritized experience replay,&quot; in Proc. Int. Conf. Learn. Represent., 2016.</note>
</biblStruct>

<biblStruct coords="19,323.16,435.08,228.83,7.17;19,323.16,444.04,130.06,7.17" xml:id="b123">
	<analytic>
		<title level="a" type="main">A scalable neural network architecture for board games</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2008.5035662</idno>
		<idno type="arXiv">arXiv:1109.1314</idno>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Symposium On Computational Intelligence and Games</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Schaul, J. Togelius, and J. Schmidhuber, &quot;Measuring intelligence through games,&quot; 2011, arXiv:1109.1314.</note>
</biblStruct>

<biblStruct coords="19,323.16,453.00,228.83,7.17;19,323.16,461.98,228.83,7.17;19,323.16,470.94,77.72,7.17" xml:id="b124">
	<analytic>
		<title level="a" type="main">Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)</title>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/tamd.2010.2056368</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<title level="j" type="abbrev">IEEE Trans. Auton. Mental Dev.</title>
		<idno type="ISSN">1943-0604</idno>
		<idno type="ISSNe">1943-0612</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="247" />
			<date type="published" when="1990">1990-2010. Sep. 2010</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Schmidhuber, &quot;Formal theory of creativity, fun, and intrinsic motiva- tion (1990-2010),&quot; IEEE Trans. Auton. Mental Develop., vol. 2, no. 3, pp. 230-247, Sep. 2010.</note>
</biblStruct>

<biblStruct coords="19,323.16,479.91,228.84,7.17;19,323.16,488.87,116.72,7.17" xml:id="b125">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.003</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<title level="j" type="abbrev">Neural Networks</title>
		<idno type="ISSN">0893-6080</idno>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015-01">2015</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Schmidhuber, &quot;Deep learning in neural networks: An overview,&quot; Neu- ral Netw., vol. 61, pp. 85-117, 2015.</note>
</biblStruct>

<biblStruct coords="19,323.16,497.84,228.83,7.17;19,323.16,506.81,228.80,7.17;19,323.16,515.77,203.89,7.17" xml:id="b126">
	<analytic>
		<title level="a" type="main">UT&amp;#x2041;2: Human-like behavior via neuroevolution of combat behavior and replay of human traces</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Schrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><forename type="middle">V</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2011.6032024</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computational Intelligence and Games (CIG&apos;11)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-08">2011</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Schrum, I. V. Karpov, and R. Miikkulainen, &quot;UT 2: Human-like behav- ior via neuroevolution of combat behavior and replay of human traces,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2011, pp. 329-336.</note>
</biblStruct>

<biblStruct coords="19,323.16,524.73,228.83,7.17;19,323.16,533.71,228.84,7.17;19,323.16,542.67,49.84,7.17" xml:id="b127">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, &quot;Trust region policy optimization,&quot; in Proc. Int. Conf. Mach. Learn., 2015, pp. 1889-1897.</note>
</biblStruct>

<biblStruct coords="19,323.16,551.63,228.83,7.17;19,323.16,560.61,201.26,7.17" xml:id="b128">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, &quot;Prox- imal policy optimization algorithms,&quot; 2017, arXiv:1707.06347.</note>
</biblStruct>

<biblStruct coords="19,323.16,569.57,228.84,7.17;19,323.16,578.54,154.64,7.17" xml:id="b129">
	<monogr>
		<title level="m" type="main">Procedural Content Generation in Games</title>
		<author>
			<persName><forename type="first">Noor</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-42716-4</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Shaker, J. Togelius, and M. J. Nelson, Procedural Content Generation in Games. New York, NY, USA: Springer, 2016.</note>
</biblStruct>

<biblStruct coords="19,323.16,587.50,228.84,7.17;19,323.16,596.47,228.82,7.17;19,323.16,605.44,71.96,7.17" xml:id="b130">
	<analytic>
		<title level="a" type="main">The turing test track of the 2012 Mario AI Championship: Entries and evaluation</title>
		<author>
			<persName><forename type="first">Noor</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Likith</forename><surname>Poovanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><forename type="middle">S</forename><surname>Ethiraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><forename type="middle">J</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">G</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">K</forename><surname>Heether</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Gallagher</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2013.6633634</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computational Inteligence in Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-08">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Shaker et al., &quot;The turing test track of the 2012 Mario AI Cham- pionship: Entries and evaluation,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2013, pp. 1-8.</note>
</biblStruct>

<biblStruct coords="19,323.16,614.40,228.83,7.17;19,323.16,623.36,228.83,7.17;19,323.16,632.34,97.89,7.17" xml:id="b131">
	<analytic>
		<title level="a" type="main">Loss is its own reward: Self-supervision for reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mahmoudieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. Shelhamer, P. Mahmoudieh, M. Argus, and T. Darrell, &quot;Loss is its own reward: Self-supervision for reinforcement learning,&quot; in Proc. Int. Conf. Learn. Represent., 2016.</note>
</biblStruct>

<biblStruct coords="19,323.15,641.30,228.85,7.17;19,323.16,650.26,204.22,7.17" xml:id="b132">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature16961</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<title level="j" type="abbrev">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<idno type="ISSNe">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016-01-27">2016</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Silver et al., &quot;Mastering the game of go with deep neural networks and tree search,&quot; Nature, vol. 529, no. 7587, pp. 484-489, 2016.</note>
</biblStruct>

<biblStruct coords="19,323.15,659.23,228.83,7.17;19,323.16,668.20,228.84,7.17;19,323.16,677.17,86.81,7.17" xml:id="b133">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf. Mach. Learn</title>
				<meeting>31st Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, &quot;Deterministic policy gradient algorithms,&quot; in Proc. 31st Int. Conf. Mach. Learn., 2014, pp. 387-395.</note>
</biblStruct>

<biblStruct coords="19,323.16,686.13,228.84,7.17;19,323.16,695.09,228.84,7.17;19,323.16,704.07,122.00,7.17" xml:id="b134">
	<analytic>
		<title level="a" type="main">Breeding a diversity of Super Mario behaviors through interactive evolution</title>
		<author>
			<persName><forename type="first">Patrikk</forename><forename type="middle">D</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeppeh</forename><forename type="middle">M</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2016.7860436</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-09">2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. D. Sørensen, J. M. Olsen, and S. Risi, &quot;Breeding a diversity of super Mario behaviors through interactive evolution,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2016, pp. 1-7.</note>
</biblStruct>

<biblStruct coords="19,323.15,713.03,228.82,7.17;19,323.16,721.99,228.83,7.17;19,323.16,730.97,141.37,7.17" xml:id="b135">
	<analytic>
		<title level="a" type="main">Evaluating real-time strategy game states using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Stanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">A</forename><surname>Barriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Buro</surname></persName>
		</author>
		<idno type="DOI">10.1109/cig.2016.7860439</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computational Intelligence and Games (CIG)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-09">2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Stanescu, N. A. Barriga, A. Hess, and M. Buro, &quot;Evaluating real-time strategy game states using convolutional neural networks,&quot; in Proc. IEEE Conf. Comput. Intell. Games, 2016, pp. 1-7.</note>
</biblStruct>

<biblStruct coords="20,64.36,67.45,228.82,7.17;20,64.37,76.42,228.82,7.17;20,64.37,85.39,98.52,7.17" xml:id="b136">
	<analytic>
		<title level="a" type="main">Real-Time Neuroevolution in the NERO Video Game</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno type="DOI">10.1109/tevc.2005.856210</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<title level="j" type="abbrev">IEEE Trans. Evol. Computat.</title>
		<idno type="ISSN">1089-778X</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2005-12">Dec. 2005</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">K. O. Stanley, B. D. Bryant, and R. Miikkulainen, &quot;Real-time neuroevo- lution in the NERO video game,&quot; IEEE Trans. Evol. Comput., vol. 9, no. 6, pp. 653-668, Dec. 2005.</note>
</biblStruct>

<biblStruct coords="20,64.36,94.35,228.82,7.17;20,64.37,103.32,190.74,7.17" xml:id="b137">
	<analytic>
		<title level="a" type="main">Keepaway Soccer: A Machine Learning Test bed</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45603-1_22</idno>
	</analytic>
	<monogr>
		<title level="m">RoboCup 2001: Robot Soccer World Cup V</title>
				<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Stone and R. S. Sutton, &quot;Keepaway soccer: A machine learning test bed,&quot; in Proc. Robot Soccer World Cup, 2001, pp. 214-223.</note>
</biblStruct>

<biblStruct coords="20,64.36,112.28,228.84,7.17;20,64.37,121.25,228.83,7.17;20,64.37,130.22,228.81,7.17;20,64.37,139.18,194.72,7.17" xml:id="b138">
	<analytic>
		<title level="a" type="main">Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annu. Conf. Genetic Evol. Comput</title>
				<meeting>11th Annu. Conf. Genetic Evol. Comput</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune, &quot;Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning,&quot; in Proc. 11th Annu. Conf. Genetic Evol. Comput., 2017, pp. 145-152.</note>
</biblStruct>

<biblStruct coords="20,64.36,148.15,228.83,7.17;20,64.37,157.12,228.84,7.17;20,64.37,166.08,33.89,7.17" xml:id="b139">
	<analytic>
		<title level="a" type="main">Procedural Content Generation via Machine Learning (PCGML)</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Summerville</surname></persName>
			<idno type="ORCID">0000-0003-1747-0861</idno>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoffer</forename><surname>Holmgard</surname></persName>
			<idno type="ORCID">0000-0002-1917-0099</idno>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">K</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Isaksen</surname></persName>
			<idno type="ORCID">0000-0001-7659-2987</idno>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
			<idno type="ORCID">0000-0003-3128-4598</idno>
		</author>
		<idno type="DOI">10.1109/tg.2018.2846639</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<title level="j" type="abbrev">IEEE Trans. Games</title>
		<idno type="ISSN">2475-1502</idno>
		<idno type="ISSNe">2475-1510</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="270" />
			<date type="published" when="2018-09">Sep. 2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Summerville et al., &quot;Procedural content generation via machine learning (PCGML),&quot; IEEE Trans. Games, vol. 10, no. 3, pp. 257-270, Sep. 2018.</note>
</biblStruct>

<biblStruct coords="20,64.37,175.05,228.81,7.17;20,64.37,184.01,175.66,7.17" xml:id="b140">
	<monogr>
		<title level="m" type="main">TStarBots: Defeating the cheating level builtin AI in StarCraft II in the full game</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07193</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Sun et al., &quot;TStarBots: Defeating the cheating level builtin AI in StarCraft II in the full game,&quot; 2018, arXiv:1809.07193.</note>
</biblStruct>

<biblStruct coords="20,64.36,192.98,228.83,7.17;20,64.37,201.95,153.70,7.17" xml:id="b141">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement Learning: An Introduction</title>
				<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, vol. 1. Cambridge, MA, USA: MIT Press, 1998.</note>
</biblStruct>

<biblStruct coords="20,64.36,210.91,228.82,7.17;20,64.37,219.88,228.81,7.17;20,64.37,228.85,114.55,7.17" xml:id="b142">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. S. Sutton et al., &quot;Policy gradient methods for reinforcement learning with function approximation,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 1999, vol. 99, pp. 1057-1063.</note>
</biblStruct>

<biblStruct coords="20,64.36,237.81,228.84,7.17;20,64.37,246.78,32.11,7.17" xml:id="b143">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sweetser</surname></persName>
		</author>
		<title level="m">Emergence in Games</title>
				<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cengage Learning</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Sweetser, Emergence in Games. Boston, MA, USA: Cengage Learn- ing, 2008.</note>
</biblStruct>

<biblStruct coords="20,64.36,255.75,228.82,7.17;20,64.37,264.71,175.07,7.17" xml:id="b144">
	<monogr>
		<title level="m" type="main">TorchCraft: A library for machine learning research on real-time strategy games</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00625</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Synnaeve et al., &quot;TorchCraft: A library for machine learning research on real-time strategy games,&quot; 2016, arXiv:1611.00625.</note>
</biblStruct>

<biblStruct coords="20,64.36,273.68,228.82,7.17;20,64.37,282.64,228.83,7.17" xml:id="b145">
	<analytic>
		<title level="a" type="main">Multiagent cooperation and competition with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Ardi</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tambet</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorian</forename><surname>Kodelja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristjan</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaan</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Vicente</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0172395</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e0172395</biblScope>
			<date type="published" when="2017-04-05">2017. e0172395</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Tampuu et al., &quot;Multiagent cooperation and competition with deep re- inforcement learning,&quot; PloS One, vol. 12, no. 4, 2017, Art. no. e0172395.</note>
</biblStruct>

<biblStruct coords="20,64.36,291.61,228.85,7.17;20,64.37,300.58,225.47,7.17" xml:id="b146">
	<analytic>
		<title level="a" type="main">Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-1-55860-307-3.50049-6</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings 1993</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Tan, &quot;Multi-agent reinforcement learning: Independent vs. coopera- tive agents,&quot; in Proc. 10th Int. Conf. Mach. Learn., 1993, pp. 330-337.</note>
</biblStruct>

<biblStruct coords="20,64.36,309.54,228.83,7.17;20,64.37,318.51,228.82,7.17;20,64.37,327.48,61.79,7.17" xml:id="b147">
	<analytic>
		<title level="a" type="main">Reinforcement Learning for Build-Order Production in StarCraft II</title>
		<author>
			<persName><forename type="first">Zhentao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/icist.2018.8426160</idno>
	</analytic>
	<monogr>
		<title level="m">2018 Eighth International Conference on Information Science and Technology (ICIST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Tang, D. Zhao, Y. Zhu, and P. Guo, &quot;Reinforcement learning for build- order production in StarCraft II,&quot; in Proc. 8th Int. Conf. Inf. Sci. Technol., 2018, pp. 153-158.</note>
</biblStruct>

<biblStruct coords="20,64.37,336.44,228.81,7.17;20,64.37,345.41,186.38,7.17" xml:id="b148">
	<analytic>
		<title level="a" type="main">Distral: Robust multitask reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4497" to="4507" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Teh et al., &quot;Distral: Robust multitask reinforcement learning,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 4497-4507.</note>
</biblStruct>

<biblStruct coords="20,64.36,354.37,228.82,7.17;20,64.37,363.34,228.81,7.17;20,64.37,372.31,228.82,7.17;20,64.37,381.27,49.83,7.17" xml:id="b149">
	<analytic>
		<title level="a" type="main">A Deep Hierarchical Approach to Lifelong Learning in Minecraft</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Givony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v31i1.10744</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1553" to="1561" />
			<date type="published" when="2017-04-09">Feb. 4-9, 2017</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Tessler, S. Givony, T. Zahavy, D. J. Mankowitz, and S. Mannor, &quot;A deep hierarchical approach to lifelong learning in minecraft,&quot; in Proc. 31st AAAI Conf. Artif. Intell., San Francisco, CA, USA, Feb. 4-9, 2017, pp. 1553-1561.</note>
</biblStruct>

<biblStruct coords="20,64.36,390.24,228.84,7.17;20,64.37,399.21,228.79,7.17;20,64.37,408.17,228.83,7.17;20,64.37,417.14,49.83,7.17" xml:id="b150">
	<analytic>
		<title level="a" type="main">ELF: An extensive, lightweight and flexible research platform for real-time strategy games</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2656" to="2666" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Tian, Q. Gong, W. Shang, Y. Wu, and C. L. Zitnick, &quot;ELF: An ex- tensive, lightweight and flexible research platform for real-time strat- egy games,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 2656-2666.</note>
</biblStruct>

<biblStruct coords="20,64.36,426.11,228.83,7.17;20,64.37,435.07,228.83,7.17;20,64.37,444.04,49.83,7.17" xml:id="b151">
	<analytic>
		<title level="a" type="main">MuJoCo: A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="DOI">10.1109/iros.2012.6386109</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-10">2012</date>
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. Todorov, T. Erez, and Y. Tassa, &quot;MuJoCo: A physics engine for model- based control,&quot; in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2012, pp. 5026-5033.</note>
</biblStruct>

<biblStruct coords="20,64.36,453.00,228.84,7.17;20,64.37,461.97,228.81,7.17;20,64.37,470.94,122.51,7.17" xml:id="b152">
	<analytic>
		<title level="a" type="main">Ontogenetic and phylogenetic reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Künstliche Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="30" to="33" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Togelius, T. Schaul, D. Wierstra, C. Igel, F. Gomez, and J. Schmidhu- ber, &quot;Ontogenetic and phylogenetic reinforcement learning,&quot; Künstliche Intell., vol. 23, no. 3, pp. 30-33, 2009.</note>
</biblStruct>

<biblStruct coords="20,64.36,479.90,228.83,7.17;20,64.37,488.87,228.83,7.17;20,64.37,497.84,131.86,7.17" xml:id="b153">
	<monogr>
		<title level="m" type="main">Episodic exploration for deep deterministic policies: An application to StarCraft micromanagement tasks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02993</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Usunier, G. Synnaeve, Z. Lin, and S. Chintala, &quot;Episodic exploration for deep deterministic policies: An application to StarCraft microman- agement tasks,&quot; 2016, arXiv:1609.02993.</note>
</biblStruct>

<biblStruct coords="20,64.36,506.80,228.82,7.17;20,64.37,515.77,228.83,7.17;20,64.37,524.73,49.83,7.17" xml:id="b154">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Double Q-Learning</title>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v30i1.10295</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2094" to="2100" />
			<date type="published" when="2016-03-02">2016</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Van Hasselt, A. Guez, and D. Silver, &quot;Deep reinforcement learning with double q-learning,&quot; in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 2094-2100.</note>
</biblStruct>

<biblStruct coords="20,64.36,533.70,228.82,7.17;20,64.37,542.67,228.82,7.17;20,64.37,551.64,117.43,7.17" xml:id="b155">
	<analytic>
		<title level="a" type="main">Postponed Updates for Temporal-Difference Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Harm</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="DOI">10.1109/isda.2009.76</idno>
	</analytic>
	<monogr>
		<title level="m">2009 Ninth International Conference on Intelligent Systems Design and Applications</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5396" to="5406" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Van Seijen, R. Laroche, M. Fatemi, and J. Romoff, &quot;Hybrid reward architecture for reinforcement learning,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 5396-5406.</note>
</biblStruct>

<biblStruct coords="20,327.37,67.45,228.84,7.17;20,327.37,76.42,113.04,7.17" xml:id="b156">
	<monogr>
		<title level="m" type="main">Starcraft II: A new challenge for reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04782</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">O. Vinyals et al., &quot;Starcraft II: A new challenge for reinforcement learning,&quot; 2017, arXiv:1708.04782.</note>
</biblStruct>

<biblStruct coords="20,327.37,85.39,228.84,7.17;20,327.37,94.35,228.81,7.17;20,327.37,103.32,228.84,7.17;20,327.37,112.28,41.87,7.17" xml:id="b157">
	<analytic>
		<title level="a" type="main">Evolving mario levels in the latent space of a deep convolutional generative adversarial network</title>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Schrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3205455.3205517</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
				<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-07-02">2018</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
	<note type="raw_reference">V. Volz, J. Schrum, J. Liu, S. M. Lucas, A. Smith, and S. Risi, &quot;Evolv- ing Mario levels in the latent space of a deep convolutional genera- tive adversarial network,&quot; in Proc. Genetic Evol. Comput. Conf., 2018, pp. 221-228.</note>
</biblStruct>

<biblStruct coords="20,327.37,121.26,228.83,7.17;20,327.37,130.22,228.82,7.17;20,327.37,139.18,130.76,7.17" xml:id="b158">
	<analytic>
		<title level="a" type="main">Portfolio Online Evolution in StarCraft</title>
		<author>
			<persName><forename type="first">Che</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoffer</forename><surname>Holmgård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<idno type="DOI">10.1609/aiide.v12i1.12862</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<title level="j" type="abbrev">AIIDE</title>
		<idno type="ISSN">2326-909X</idno>
		<idno type="ISSNe">2334-0924</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="120" />
			<date type="published" when="2016">2016</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Wang, P. Chen, Y. Li, C. Holmgård, and J. Togelius, &quot;Portfolio on- line evolution in StarCraft,&quot; in Proc. 12th Artif. Intell. Interact. Digit. Entertainment Conf., 2016, pp. 114-121.</note>
</biblStruct>

<biblStruct coords="20,327.37,148.15,228.81,7.17;20,327.37,157.12,79.96,7.17" xml:id="b159">
	<monogr>
		<title level="m" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01224</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Wang et al., &quot;Sample efficient actor-critic with experience replay,&quot; 2017, arXiv:1611.01224.</note>
</biblStruct>

<biblStruct coords="20,327.37,166.08,228.84,7.17;20,327.37,175.05,228.80,7.17;20,327.37,184.01,193.08,7.17" xml:id="b160">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Int. Conf. Mach. Learn</title>
				<meeting>33rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and N. de Freitas, &quot;Dueling network architectures for deep reinforcement learning,&quot; in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 1995-2003.</note>
</biblStruct>

<biblStruct coords="20,327.37,192.98,228.83,7.17;20,327.37,201.95,61.79,7.17" xml:id="b161">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00992698</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<title level="j" type="abbrev">Mach Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<idno type="ISSNe">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992-05">1992</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">C. J. Watkins and P. Dayan, &quot;Q-learning,&quot; Mach. Learn., vol. 8, nos. 3/4, pp. 279-292, 1992.</note>
</biblStruct>

<biblStruct coords="20,327.37,210.91,228.83,7.17;20,327.37,219.88,103.70,7.17" xml:id="b162">
	<analytic>
		<title level="a" type="main">HQ-Learning</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Wiering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1177/105971239700600202</idno>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<title level="j" type="abbrev">Adaptive Behavior</title>
		<idno type="ISSN">1059-7123</idno>
		<idno type="ISSNe">1741-2633</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="246" />
			<date type="published" when="1997-09">1997</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Wiering and J. Schmidhuber, &quot;HQ-learning,&quot; Adaptive Behav., vol. 6, no. 2, pp. 219-246, 1997.</note>
</biblStruct>

<biblStruct coords="20,327.37,228.85,228.86,7.17;20,327.37,237.81,228.83,7.17;20,327.37,246.78,61.79,7.17" xml:id="b163">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00992696</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<title level="j" type="abbrev">Mach Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<idno type="ISSNe">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992-05">1992</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. J. Williams, &quot;Simple statistical gradient-following algorithms for connectionist reinforcement learning,&quot; Mach. Learn., vol. 8, nos. 3/4, pp. 229-256, 1992.</note>
</biblStruct>

<biblStruct coords="20,327.37,255.75,228.83,7.17;20,327.37,264.71,228.83,7.17;20,327.37,273.68,61.79,7.17" xml:id="b164">
	<analytic>
		<title level="a" type="main">A Learning Algorithm for Continually Running Fully Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.2.270</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989-06">1989</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. J. Williams and D. Zipser, &quot;A learning algorithm for continually running fully recurrent neural networks,&quot; Neural Comput., vol. 1, no. 2, pp. 270-280, 1989.</note>
</biblStruct>

<biblStruct coords="20,327.37,282.64,228.85,7.17;20,327.37,291.61,228.82,7.17;20,327.37,300.58,228.84,7.17;20,327.37,309.54,49.84,7.17" xml:id="b165">
	<analytic>
		<title level="a" type="main">Scalable trustregion method for deep reinforcement learning using kronecker-factored approximation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process. Syst</title>
				<meeting>Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Wu, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba, &quot;Scalable trust- region method for deep reinforcement learning using kronecker-factored approximation,&quot; in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 5285-5294.</note>
</biblStruct>

<biblStruct coords="20,327.36,318.51,228.84,7.17;20,327.37,327.48,228.81,7.17;20,327.37,336.44,17.94,7.17" xml:id="b166">
	<analytic>
		<title level="a" type="main">Training agent for first-person shooter game with actor-critic curriculum learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
				<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Wu and Y. Tian, &quot;Training agent for first-person shooter game with actor-critic curriculum learning,&quot; in Proc. Int. Conf. Learn. Represent., 2017.</note>
</biblStruct>

<biblStruct coords="20,327.37,345.41,228.82,7.17;20,327.37,354.37,228.84,7.17;20,327.37,363.34,151.12,7.17" xml:id="b167">
	<monogr>
		<title level="m" type="main">The Open Racing Car Simulator, Software</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wymann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Espié</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guionneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dimitrakakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Torcs</surname></persName>
		</author>
		<ptr target="http://torcs.sourceforge.net" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner, TORCS, The Open Racing Car Simulator, Software, 2000. [Online]. Available: http://torcs.sourceforge.net</note>
</biblStruct>

<biblStruct coords="20,327.37,372.31,228.81,7.17;20,327.37,381.27,228.83,7.17;20,327.37,390.24,158.72,7.17" xml:id="b168">
	<analytic>
		<title level="a" type="main">Dagstuhl Manifesto</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Spronck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Loiacono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>André</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00287-013-0690-3</idno>
	</analytic>
	<monogr>
		<title level="j">Informatik-Spektrum</title>
		<title level="j" type="abbrev">Informatik Spektrum</title>
		<idno type="ISSN">0170-6012</idno>
		<idno type="ISSNe">1432-122X</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2013-02-22">2013</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">G. N. Yannakakis, P. Spronck, D. Loiacono, and E. André, &quot;Player modeling,&quot; in Dagstuhl Follow-Ups, vol. 6. Wadern, Germany: Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2013.</note>
</biblStruct>

<biblStruct coords="20,327.37,399.21,228.84,7.17;20,327.37,408.17,228.81,7.17;20,327.37,417.14,120.52,7.17" xml:id="b169">
	<analytic>
		<title level="a" type="main">A Panorama of Artificial and Computational Intelligence in Games</title>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<idno type="DOI">10.1109/tciaig.2014.2339221</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<title level="j" type="abbrev">IEEE Trans. Comput. Intell. AI Games</title>
		<idno type="ISSN">1943-068X</idno>
		<idno type="ISSNe">1943-0698</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="335" />
			<date type="published" when="2015-12">Dec. 2015</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">G. N. Yannakakis and J. Togelius, &quot;A panorama of artificial and compu- tational intelligence in games,&quot; IEEE Trans. Comput. Intell. AI Games, vol. 7, no. 4, pp. 317-335, Dec. 2015.</note>
</biblStruct>

<biblStruct coords="20,327.37,426.11,228.82,7.17;20,327.37,435.07,120.32,7.17" xml:id="b170">
	<monogr>
		<title level="m" type="main">Artificial Intelligence and Games</title>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-63519-4</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">G. N. Yannakakis and J. Togelius, Artificial Intelligence and Games. New York, NY, USA: Springer, 2018.</note>
</biblStruct>

<biblStruct coords="20,327.36,444.04,228.82,7.17;20,327.37,453.00,228.83,7.17;20,327.37,461.97,60.03,7.17" xml:id="b171">
	<analytic>
		<title level="a" type="main">A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.4556</idno>
		<idno type="arXiv">arXiv:1703.09831</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<title level="j" type="abbrev">jair</title>
		<idno type="ISSNe">1076-9757</idno>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="601" to="713" />
			<date type="published" when="2017">2017</date>
			<publisher>AI Access Foundation</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Yu, H. Zhang, and W. Xu, &quot;A deep compositional framework for human-like language acquisition in virtual environment,&quot; 2017, arXiv:1703.09831.</note>
</biblStruct>

<biblStruct coords="20,327.36,470.94,228.84,7.17;20,327.37,479.90,228.84,7.17;20,327.37,488.87,228.82,7.17;20,327.37,497.84,228.81,7.17;20,327.37,506.80,228.79,7.17;20,327.37,515.77,69.77,7.17" xml:id="b172">
	<analytic>
		<title level="a" type="main">Learn what not to learn: Action elimination with deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Haroush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Merlis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3562" to="3573" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor, &quot;Learn what not to learn: Action elimination with deep reinforcement learning,&quot; in Advances in Neural Information Processing Systems 31, S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa- Bianchi and R. Garnett, Eds. New York, NY, USA: Curran Associates, 2018, pp. 3562-3573.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
